{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.parse\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import sys\n",
    "\n",
    "from secret import *\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "# --- Constants ---\n",
    "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
    "hourly_weather_variables = {\n",
    "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
    "    'humidity' : ['relativehumidity_2m'],\n",
    "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
    "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
    "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
    "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
    "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
    "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
    "    'weather' : ['weathercode'],\n",
    "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
    "    'visibility' : ['visibility'],\n",
    "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
    "}\n",
    "daily_weather_variables = {\n",
    "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
    "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
    "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
    "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
    "    'radiation' : ['shortwave_radiation_sum'],\n",
    "    'energy' : ['et0_fao_evapotranspiration'],\n",
    "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
    "}\n",
    "# --- Weather Retrieval Functions ---\n",
    "\n",
    "# Base OpenMeteo API call\n",
    "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    hourly_params = weather_variables.get('hourly', [])\n",
    "    daily_params = weather_variables.get('daily', [])\n",
    "    params = {'latitude' : lat, \n",
    "              'longitude' : lat, \n",
    "              'start_date' : start.strftime('%Y-%m-%d'), \n",
    "              'end_date' : end.strftime('%Y-%m-%d'), \n",
    "              'hourly' : ','.join(hourly_params),\n",
    "              'daily' : ','.join(daily_params),\n",
    "              'timezone' : 'UTC'\n",
    "              }\n",
    "    try:\n",
    "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
    "        data = response.json()\n",
    "        # Daily Data\n",
    "        if 'daily' in data:\n",
    "            daily = pd.DataFrame(data['daily'])\n",
    "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
    "            daily['latitude'] = lat\n",
    "            daily['longitude'] = lon\n",
    "        else:\n",
    "            daily = None\n",
    "        # Hourly Data\n",
    "        if 'hourly' in data:\n",
    "            hourly = pd.DataFrame(data['hourly'])\n",
    "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
    "            hourly['latitude'] = lat\n",
    "            hourly['longitude'] = lon\n",
    "        else:\n",
    "            hourly = None      \n",
    "        \n",
    "        return daily, hourly\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
    "    \n",
    "# Request an array of locations using OpenMeteo API\n",
    "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
    "    daily, hourly = [], []\n",
    "    for location in locations_array:\n",
    "        lat, lon = location['coordinates']\n",
    "        # Get rest of location data (e.g. name, elevation, etc.)\n",
    "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
    "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
    "        if daily_data is not None:\n",
    "            daily_data = daily_data.assign(**location_metadata)\n",
    "            daily.append(daily_data)\n",
    "        if hourly_data is not None:\n",
    "            hourly_data = hourly_data.assign(**location_metadata)\n",
    "            hourly.append(hourly_data)\n",
    "    return pd.concat(daily), pd.concat(hourly)\n",
    "\n",
    "# --- Database Retrieval Functions (SSH) ---\n",
    "def load_connection(user: str,password: str,host: str,database: str, port):\n",
    "    pg_connection_dict = {\n",
    "    'dbname': database,\n",
    "    'user': user,\n",
    "    'password': password,\n",
    "    'port': port,\n",
    "    'host': host\n",
    "    }\n",
    "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
    "    try:\n",
    "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
    "        return connection\n",
    "    except(Exception, EnvironmentError) as e:\n",
    "        raise Exception (\"Error in connection, }\".format(e))\n",
    "        \n",
    "def load_query(connection, query) -> pd.DataFrame:\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query) \n",
    "        record = cursor.fetchall()\n",
    "        # Convert to pandas\n",
    "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
    "        return df_record\n",
    "\n",
    "    except(Exception,EnvironmentError) as e:\n",
    "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
    "    finally:\n",
    "        # Close connection\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = SSHTunnelForwarder(\n",
    "    (REMOTE_HOST, 22),\n",
    "    ssh_username=REMOTE_USER,\n",
    "    ssh_password=REMOTE_PASS,\n",
    "    remote_bind_address=('localhost', DB_PORT)\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
    "FROM scraper.entsoe_generation_production\n",
    "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "GROUP BY \"time\",\"price_area\"\n",
    "ORDER BY \"time\"\n",
    "\"\"\"\n",
    "server.start()\n",
    "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
    "df = load_query(connection, query)\n",
    "server.stop()\n",
    "\n",
    "#print(df.head(10))\n",
    "\n",
    "location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}\n",
    "w_variables = {\n",
    "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
    "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
    "}\n",
    "\n",
    "print(w_variables)\n",
    "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
    "    dt.date(2020,1,1), \n",
    "    dt.date(2022,12,31), \n",
    "    [location_eindhoven],\n",
    "    weather_variables=w_variables)\n",
    "\n",
    "w_hourly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph of the weather data using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20,3))\n",
    "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
    "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
    "ax.set_title('Weather Data for Eindhoven')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Preliminary Data Analysis\n",
    "#### 1.0: Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSO_COUNTRIES = {\n",
    "\"SK\" : \"Slovakia\",\n",
    "\"DK\" : \"Denmark\",\n",
    "\"LT\" : \"Lithuania\",\n",
    "\"SI\" : \"Slovenia\",\n",
    "\"CZ\" : \"Czech Republic\",\n",
    "\"FR\" : \"France\",\n",
    "\"LV\" : \"Latvia\",\n",
    "\"RO\" : \"Romania\",\n",
    "\"NL\" : \"Netherlands\",\n",
    "\"EE\" : \"Estonia\",\n",
    "\"HU\" : \"Hungary\",\n",
    "\"AT\" : \"Austria\",\n",
    "\"FI\" : \"Finland\",\n",
    "\"PL\" : \"Poland\",\n",
    "\"ES\" : \"Spain\",\n",
    "\"PT\" : \"Portugal\",\n",
    "\"NO\" : \"Norway\",\n",
    "\"BG\" : \"Bulgaria\",\n",
    "\"IT\" : \"Italy\",\n",
    "\"HR\" : \"Croatia\",\n",
    "\"SE\" : \"Sweden\",\n",
    "\"GR\" : \"Greece\",\n",
    "\"CH\" : \"Switzerland\",\n",
    "\"BE\" : \"Belgium\",\n",
    "\"DE\" : \"Germany\"\n",
    "}\n",
    "\n",
    "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
    "LOCATIONS = [\n",
    "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
    "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
    "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
    "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
    "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
    "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
    "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
    "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
    "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
    "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
    "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
    "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
    "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
    "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
    "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
    "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
    "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
    "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
    "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
    "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
    "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
    "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
    "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
    "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
    "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
    "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
    "]\n",
    "\n",
    "W_VARIABLES = {\n",
    "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
    "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
    "}\n",
    "\n",
    "START_DATE = dt.date(2000,1,1)\n",
    "END_DATE = dt.date(2023,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.1: Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather\n",
    "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
    "    START_DATE, \n",
    "    END_DATE, \n",
    "    LOCATIONS,\n",
    "    weather_variables=W_VARIABLES)\n",
    "\n",
    "w_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2: Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy\n",
    "\n",
    "# Connect to remote database\n",
    "server = SSHTunnelForwarder(\n",
    "    (REMOTE_HOST, 22),\n",
    "    ssh_username=REMOTE_USER,\n",
    "    ssh_password=REMOTE_PASS,\n",
    "    remote_bind_address=('localhost', DB_PORT)\n",
    ")\n",
    "\n",
    "# Create data retrieval functions\n",
    "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
    "    price_area, \n",
    "    country_code,\n",
    "    AVG(biomass) AS biomass,\n",
    "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
    "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
    "    AVG(fossil_gas) AS fossil_gas,\n",
    "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
    "    AVG(fossil_oil) AS fossil_oil,\n",
    "    AVG(geothermal) AS geothermal,\n",
    "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
    "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
    "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
    "    AVG(nuclear) AS nuclear,\n",
    "    AVG(other) AS other,\n",
    "    AVG(other_renewable) AS other_renewable,\n",
    "    AVG(solar) AS solar,\n",
    "    AVG(waste) AS waste, \n",
    "    AVG(wind_offshore) AS wind_offshore,\n",
    "    AVG(wind_onshore) AS wind_onshore\n",
    "    FROM scraper.entsoe_generation_production\n",
    "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
    "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
    "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
    "    ORDER BY \"timestamp\"\n",
    "    \"\"\"\n",
    "    server.start()\n",
    "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
    "    df = load_query(connection, query)\n",
    "    server.stop()\n",
    "    return df\n",
    "\n",
    "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
    "    price_area, \n",
    "    country_code,\n",
    "    AVG(solar) AS solar,\n",
    "    AVG(wind_offshore) AS wind_offshore,\n",
    "    AVG(wind_onshore) AS wind_onshore,\n",
    "    AVG(forecasted_load) AS forecasted_load,\n",
    "    AVG(actual_aggregated) AS actual_aggregated\n",
    "    FROM scraper.entsoe_da_forecast\n",
    "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
    "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
    "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
    "    ORDER BY \"timestamp\"\n",
    "    \"\"\"\n",
    "    server.start()\n",
    "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
    "    df = load_query(connection, query)\n",
    "    server.stop()\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
    "    price_area, \n",
    "    country_code,\n",
    "    AVG(price) AS price\n",
    "    FROM scraper.entsoe_da_prices\n",
    "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
    "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
    "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
    "    ORDER BY \"timestamp\"\n",
    "    \"\"\"\n",
    "    server.start()\n",
    "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
    "    df = load_query(connection, query)\n",
    "    server.stop()\n",
    "    return df\n",
    "\n",
    "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
    "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
    "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1: Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = w_hourly.copy()\n",
    "# groupby region, city, and country and resample to weekly\n",
    "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
    "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
    "\n",
    "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
    "for i, region in enumerate(df['region'].unique()):\n",
    "    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])\n",
    "    # Smaller line width\n",
    "    for j in range(0, len(axes[i].lines)):\n",
    "        axes[i].lines[j].set_linewidth(1.5)\n",
    "        axes[i].lines[j].set_alpha(0.5)\n",
    "    axes[i].set_title(region)\n",
    "    # Set axis labels to '' to avoid overlapping\n",
    "    axes[i].set_ylabel('Wind Speed (m/s)')\n",
    "    axes[i].set_xlabel('')\n",
    "    # Rotate xticks\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    # Legends outside the plot\n",
    "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
    "    # Create more space between the plots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the distribution of wind speed for each city\n",
    "df = w_hourly.copy()\n",
    "# groupby region, city, and country and resample to weekly\n",
    "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
    "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
    "\n",
    "def move_legend(ax, new_loc, **kws):\n",
    "    old_legend = ax.legend_\n",
    "    handles = old_legend.legendHandles\n",
    "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
    "    _title = old_legend.get_title().get_text()\n",
    "    # Check if title in **kws\n",
    "    if 'title' in kws:\n",
    "        _title = kws['title']\n",
    "        del kws['title']\n",
    "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
    "    \n",
    "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
    "for i, region in enumerate(df['region'].unique()):\n",
    "    \n",
    "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
    "    axes[i].set_title(region)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    # Show xlabel for all plots\n",
    "    # Show the xticks for all plots\n",
    "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
    "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
    "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
    "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
    "    # Xaxis max of 50 m/s\n",
    "    axes[i].set_xlim(0, 50)\n",
    "# Add legend outside the plot\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
    "df = w_hourly.copy()\n",
    "#df = df[df['country'] == 'NL']\n",
    "# groupby region, city, and country and resample to weekly\n",
    "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
    "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
    "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
    "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
    "\n",
    "def move_legend(ax, new_loc, **kws):\n",
    "    old_legend = ax.legend_\n",
    "    handles = old_legend.legendHandles\n",
    "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
    "    _title = old_legend.get_title().get_text()\n",
    "    # Check if title in **kws\n",
    "    if 'title' in kws:\n",
    "        _title = kws['title']\n",
    "        del kws['title']\n",
    "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
    "    \n",
    "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
    "for i, region in enumerate(df['region'].unique()):\n",
    "    \n",
    "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
    "    axes[i].set_title(region)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    # Show xlabel for all plots\n",
    "    # Show the xticks for all plots\n",
    "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
    "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
    "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
    "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
    "# Add legend outside the plot\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2: Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_hourly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mw_hourly\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# groupby region, city, and country and resample to weekly\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w_hourly' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = w_hourly.copy()\n",
    "# groupby region, city, and country and resample to weekly\n",
    "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
    "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
    "\n",
    "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
    "for i, region in enumerate(df['region'].unique()):\n",
    "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])\n",
    "    # Smaller line width\n",
    "    for j in range(0, len(axes[i].lines)):\n",
    "        axes[i].lines[j].set_linewidth(1.5)\n",
    "        axes[i].lines[j].set_alpha(0.5)\n",
    "    axes[i].set_title(region)\n",
    "    # Set axis labels to '' to avoid overlapping\n",
    "    axes[i].set_ylabel('Temperature (C)')\n",
    "    axes[i].set_xlabel('')\n",
    "    # Rotate xticks\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    # Legends outside the plot\n",
    "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
    "    # Create more space between the plots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3: Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points)\n",
      " 1/2:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(25))\n",
      " 1/3:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(25))\n",
      "print(masked_points.columns)\n",
      " 1/4:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(25)[['point_key','point_label','point_type']])\n",
      "print(masked_points.columns)\n",
      " 1/5:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(20)[['point_key','point_label','point_type']])\n",
      "print(masked_points.columns)\n",
      " 1/6:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "print(masked_points.columns)\n",
      " 1/7:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 1/8:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 1/9:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "1/10:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "1/11:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "1/12:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "1/13:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "1/14:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "print(entsog.__version__)\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "1/15:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "print(EntsogPandasClient.__version__)\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "1/16:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "1/17:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 2/1:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 2/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 2/3:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 3/1:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20171201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20180101', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 3/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 3/3:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "data['value'] = -1* data['value'][data['direction_key'] == 'exit']\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_\n",
      ")\n",
      " 3/4:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "data['value'] = -1* data['value'][data['direction_key'] == 'exit']\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      " 3/5:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['value'] = -1* data['value'][data['direction_key'] == 'exit']\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      " 3/6:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['value'] = -1* data[data['direction_key'] == 'exit']['value']\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      " 3/7:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "print(data.index)\n",
      "print(data.head(3))\n",
      "\n",
      "data['value'] = -1* data[data['direction_key'] == 'exit']['value']\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      " 3/8:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['value'] = -1* data[data['direction_key'] == 'exit']['value']\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      " 3/9:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['net_value'] = data['value']\n",
      "data['net_value'] = -1* data[data['direction_key'] == 'exit']['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/10:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['net_value'] = data['value']\n",
      "data['net_value'] = -1* data[data['direction_key'] == 'exit']['net_value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/11:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['net_value'] = data['value']\n",
      "data['net_value'] = -1* data[(data['direction_key'] == 'exit')]['net_value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/12:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['net_value'] = data['value']\n",
      "print((data['direction_key'] == 'exit'))\n",
      "\n",
      "\n",
      "data['net_value'] = -1* data[(data['direction_key'] == 'exit')]['net_value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/13:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data['net_value'] = data['value']\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "print(data[mask])\n",
      "\n",
      "\n",
      "data['net_value'] = -1* data[(data['direction_key'] == 'exit')]['net_value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/14:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "print(data[mask])\n",
      "\n",
      "\n",
      "data['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/15:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "print(data[mask])\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/16:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "print(data[mask])\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_grid('~point_label')\n",
      ")\n",
      "3/17:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "print(data[mask])\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/18:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "print(data[mask])\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/19:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/20:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped, aes(x='period_from', y='value'))\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    #+ facet_wrap('~point_label')\n",
      ")\n",
      "3/21:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    #+ facet_wrap('~point_label')\n",
      ")\n",
      "3/22:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    #+ facet_wrap('~point_label')\n",
      ")\n",
      "3/23:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/24:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/25:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "print(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_wrap('~point_label')\n",
      ")\n",
      "3/26:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/27:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta(breaks=pd.date_range(min(data_grouped['period_from']), max(data_grouped['period_from']), freq='M'))\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/28:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + aes(x='period_from', y='value')\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line()\n",
      "    + scale_x_timedelta()\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/29:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "\n",
      "    + geom_line()\n",
      ")\n",
      "3/30:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "\n",
      "    + geom_line(x = 'period_from', y = 'value', color = 'blue')\n",
      ")\n",
      "3/31:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "\n",
      "    + geom_line(aes(x = 'period_from', y = 'value', color = 'blue'))\n",
      ")\n",
      "3/32:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "\n",
      "    + geom_line(aes(x = 'period_from', y = 'value'))\n",
      ")\n",
      "3/33:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + labs(x='Date', y='Flow', title='Gazprom Flow')\n",
      "    + geom_line(aes(x='period_from', y='value'))\n",
      "    + scale_x_timedelta()\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/34:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value'))\n",
      "    + scale_x_timedelta()\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/35:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "\n",
      "\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value'))\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/36:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = data['period_from'].dt.strftime('%Y-%m-%d')\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value'))\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/37:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value'))\n",
      "    + facet_grid(facets = '. ~point_label')\n",
      ")\n",
      "3/38:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "3/39:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label'))\n",
      ")\n",
      "3/40:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid(data_grouped['point_type'], scales='free')\n",
      ")\n",
      "3/41:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('point_type', scales='free')\n",
      ")\n",
      "3/42:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('point_type', scales='free')\n",
      ")\n",
      "3/43:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/44:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], join = 'inner')\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/45:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], join = 'inner').reset_index()\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/46:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], join = 'inner', axis = 1)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/47:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/48:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)\n",
      "    + geom_line(aes(x='period_from', y='value', color = 'point_label')) \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/49:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, geom_point, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/50:\n",
      "from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, geom_point, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_timedelta(freq = 'M')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/51:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      "3/52:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, axis.test.x\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/53:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/54:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "3/55:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "3/56:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/57:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/58:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/59:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  - data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/60:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/61:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value'].astype(float)\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/62:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/63:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/64:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], , format='%Y-%m-%dT%H:%M:%S:00').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/65:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S:00').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/66:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%Z').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/67:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/68:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/69:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "print(data['period_from'].drop_duplicates())\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/70:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "print(data['period_from'].drop_duplicates())\n",
      "\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/71:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "print(data['period_from'].drop_duplicates())\n",
      "\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S %z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/72:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S %z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/73:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "3/74:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "3/75:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "3/76:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 4/1:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 4/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 5/1:\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 5/2:\n",
      "from .entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 5/3:\n",
      "from ..entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 5/4:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 5/5:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 5/6:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      " 5/7:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      " 5/8:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      " 5/9:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "5/10:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'])\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "5/11:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'])\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "5/12:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + geom_point()\n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "5/13:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(axis_text_x = element_text(angle=45, hjust =1))\n",
      ")\n",
      "5/14:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/15:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/16:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/17:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/18:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/19:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/20:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      "    + entsog_scale_color_brewer()\n",
      ")\n",
      "5/21:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      ")\n",
      "5/22:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      ")\n",
      "5/23:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme, entsog_scale_color_brewer\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      ")\n",
      "5/24:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme()\n",
      ")\n",
      "5/25:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      "5/26:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      "5/27:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      " 6/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 6/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 6/3:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      " 7/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 7/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 7/3:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      " 7/4:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      " 7/5:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', captions = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      " 7/6:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data)\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "print(data_grouped)\n",
      "print(data_grouped.columns)\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      " 7/7:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      ")\n",
      " 7/8:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      " 7/9:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      "7/10:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      "7/11:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + entsog_theme\n",
      ")\n",
      "7/12:\n",
      "from plotnine import *\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "    axis_text_y = element_text(angle = 90, vjust = 2),\n",
      "    axis_text_x = element_text(angle = 45, hjust = 1),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_line = element_line(colour = 'black'),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10),\n",
      "\n",
      ") \n",
      ")\n",
      "7/13:\n",
      "import * from plotnine\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "    axis_text_y = element_text(angle = 90, vjust = 2),\n",
      "    axis_text_x = element_text(angle = 45, hjust = 1),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_line = element_line(colour = 'black'),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10),\n",
      "\n",
      ") \n",
      ")\n",
      "7/14:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "    axis_text_y = element_text(angle = 90, vjust = 2),\n",
      "    axis_text_x = element_text(angle = 45, hjust = 1),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_line = element_line(colour = 'black'),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10),\n",
      "\n",
      ") \n",
      ")\n",
      "7/15:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "    axis_text_y = element_text(angle = 90, vjust = 2),\n",
      "    axis_text_x = element_text(angle = 45, hjust = 1),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10)\n",
      "\n",
      ") \n",
      ")\n",
      "7/16:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 90, vjust = 2),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 45, hjust = 1)\n",
      "\n",
      ") \n",
      ")\n",
      "7/17:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('.~point_type', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      ")\n",
      "7/18:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      ")\n",
      "7/19:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      ")\n",
      "7/20:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") + scale_fill_brewer(type=\"qual\", palette=\"Accent\")\n",
      ")\n",
      "7/21:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_fill_brewer(type=\"qual\", palette=\"Accent\")\n",
      ")\n",
      "7/22:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_fill_brewer(type=\"qual\", palette=\"Pastel1\")\n",
      ")\n",
      "7/23:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\")\n",
      ")\n",
      "7/24:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Pastel1\")\n",
      ")\n",
      "7/25:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(palette=\"Accent\")\n",
      ")\n",
      "7/26:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(palette=\"Pastel1\")\n",
      ")\n",
      "7/27:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('point_type~.', scales='free_y')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Type')\n",
      ")\n",
      "7/28:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Type')\n",
      ")\n",
      "7/29:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Type')\n",
      ")\n",
      "7/30:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.value\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y')\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "7/31:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.value\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "7/32:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.value\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "7/33:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    print(country.value)\n",
      "    return country.value\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "7/34:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 8/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 8/2:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 8/3:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      " 8/4:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "from entsog.utils import entsog_theme\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 8/5:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 8/6:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','tso_country','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 'tso_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 8/7:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 8/8:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      " 8/9:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "8/10:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "8/11:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/12:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Gwh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=10, angle = 90, vjust = 2)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/13:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Physical Flow (Gwh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_title = element_text(face='bold'),\n",
      "\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/14:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/15:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller\n",
      "from plotnine import scale_y_continous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels = 'comma')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/16:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels = 'comma')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/17:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(suffix = 'comma')\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/18:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)/ 1_000_000\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/19:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/20:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_wrap, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_wrap('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8)\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/21:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(face=\"bold\")\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/22:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black')\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/23:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "    caption=element_text(\n",
      "        size=8,\n",
      "        margin={'r': -120, 't': -30}\n",
      "    )\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/24:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "from plotnine import caption, theme, element_text, element_rect, element_line, element_blank, element_text\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "    caption=element_text(\n",
      "        size=8,\n",
      "        margin={'r': -120, 't': -30}\n",
      "    )\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/25:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "    caption=element_text(\n",
      "        size=8,\n",
      "        margin={'r': -120, 't': -30}\n",
      "    )\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/26:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/27:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py') +\n",
      "    labs('{:>50}'.format(\"source: JHU...\"))\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "8/28:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + labs('{:>50}'.format(\"source: JHU...\"))\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      " 9/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      " 9/3:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + labs('{:>50}'.format(\"source: JHU...\"))\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/4:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/5:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'point_type']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~point_type', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/6:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'connected_operators']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~connected_operators', scales='free_y', labeller = labeller(rows = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/7:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/8:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit' & data['flow_status'] == 'Confirmed')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      " 9/9:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit' and data['flow_status'] == 'Confirmed')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "9/10:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit' and data['flow_status'] == 'Confirmed')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "9/11:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') and (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "9/12:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit' and data['flow_status'] == 'Confirmed')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "9/13:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = (data['direction_key'] == 'exit' & data['flow_status'] == 'Confirmed')\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "9/14:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "9/15:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "11/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "11/3:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/4:\n",
      "from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    ggplot(data_grouped)+\n",
      "    aes(x='period_from', y='value', color = 'point_label')\n",
      "    + geom_line() \n",
      "    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + scale_x_date(date_break = '1 month')\n",
      "    + scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + theme(\n",
      "    axis_text = element_text(),\n",
      "    axis_ticks = element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=element_blank(),\n",
      "    axis_line=element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=element_blank(),\n",
      "    panel_border=element_blank(),\n",
      "    panel_background=element_blank(),\n",
      "    plot_title=element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = element_text(size = 8, colour = 'black'),\n",
      "    caption=element_text(\n",
      "        size=8,\n",
      "        margin={'r': -120, 't': -30}\n",
      "    )\n",
      "\n",
      ") \n",
      "+ scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/5:\n",
      "import plotnine as p9\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    caption=p9.element_text(\n",
      "        size=8,\n",
      "        margin={'r': -120, 't': -30}\n",
      "    )\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/6:\n",
      "import plotnine as p9\n",
      "from  plotnine import caption\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    caption=p9.element_text(\n",
      "        size=8,\n",
      "        margin={'r': -120, 't': -30}\n",
      "    )\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/7:\n",
      "import plotnine as p9\n",
      "from  plotnine import caption\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/8:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "print(data.head(3))\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/9:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/10:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    caption = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/11:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    caption_text = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/12:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    caption_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/13:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/14:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -30}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/15:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption_position = \"bottom\",\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -30}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/16:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -30}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/17:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -50}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/18:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 0}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/19:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': 0, 't': 0}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/20:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': 50, 't': 0}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/21:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 50}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/22:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 50}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/23:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog.\\nCreated using entsog-py.\\nData from {start} to {end}')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/24:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nCreated using entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}, face = 'italian', family = 'Tahoma')'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/25:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nCreated using entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/26:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/27:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 20}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/28:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -100, 't': 20}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/29:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -60, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/30:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/31:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/32:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data[data['adjacent_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/33:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/34:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_key=p9.element_blank(family = 'Tahoma'),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/35:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_text = p9.element_text(family = 'Tahoma'),\n",
      "    legend_title_text = p9.element_text(family = 'Tahoma', face = 'bold'),\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/36:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_text = p9.element_text(family = 'Tahoma'),\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/37:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(masked_points.column)\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_text = p9.element_text(family = 'Tahoma'),\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "11/38:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(masked_points.columns)\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_text = p9.element_text(family = 'Tahoma'),\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "12/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "12/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "12/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "print(masked_points.columns)\n",
      "print(data[data['t_so_country'] == 'LT'])\n",
      "\n",
      "# Group by point and period_from\n",
      "data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()\n",
      "\n",
      "def col_func(s):\n",
      "    country = lookup_country(s)\n",
      "    return country.label\n",
      "\n",
      "(\n",
      "    p9.ggplot(data_grouped)\n",
      "    + p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "    + p9.geom_line() \n",
      "    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "    + p9.scale_x_date(date_break = '1 month')\n",
      "    + p9.scale_y_continuous(labels= lambda l: [f\"{(v / 1_000_000)} GWh\" for v in l])\n",
      "    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))\n",
      "    + p9.theme(\n",
      "    axis_text = p9.element_text(),\n",
      "    axis_ticks = p9.element_line(),\n",
      "\n",
      "    legend_position=\"bottom\",\n",
      "    legend_direction=\"horizontal\",\n",
      "    legend_title_align=\"center\",\n",
      "    legend_box_spacing=0.4,\n",
      "    legend_text = p9.element_text(family = 'Tahoma'),\n",
      "    legend_key=p9.element_blank(),\n",
      "    axis_line=p9.element_line(size=1, colour=\"black\"),\n",
      "    panel_grid_major=p9.element_line(colour=\"#d3d3d3\"),\n",
      "    panel_grid_minor=p9.element_blank(),\n",
      "    panel_border=p9.element_blank(),\n",
      "    panel_background=p9.element_blank(),\n",
      "    plot_title=p9.element_text(size=15, family=\"Tahoma\", \n",
      "                            face=\"bold\"),\n",
      "    text=p9.element_text(family=\"Tahoma\", size=11),\n",
      "    axis_text_x=p9.element_text(colour=\"black\", size=8, angle = 45, hjust = 1),\n",
      "    axis_text_y=p9.element_text(colour=\"black\", size=8),\n",
      "\n",
      "    strip_background=p9.element_rect(colour=\"#f0f0f0\",fill=\"#f0f0f0\"),\n",
      "    strip_text = p9.element_text(size = 8, colour = 'black'),\n",
      "    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),\n",
      "\n",
      ") \n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "12/4:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type']])\n",
      "12/5:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "12/6:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "12/7:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "13/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "13/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "13/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "data = backup\n",
      "\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/7:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "13/8:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/9:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/10:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot = plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/11:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot = plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/12:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot = plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/13:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label',facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label',facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "(\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "13/14:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "facet_row = 'country'\n",
      "facet_col = None\n",
      "\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label',facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label',facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "(\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "13/15:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "facet_row = 'country'\n",
      "facet_col = None\n",
      "\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label',facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label',facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "(\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "13/16:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/17:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = None\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', 'value', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label','value', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "plot = (\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "#+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/18:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', 'value', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label','value', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "plot = (\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "#+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/19:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', 'value', facet_row, facet_col]).agg(\n",
      "        {'aggregated': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label','value', facet_row, facet_col]).agg(\n",
      "        {'aggregated': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "plot = (\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='aggregated', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "#+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/20:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_break = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "plot = (\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_break = date_break)\n",
      "#+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data = data.replace({'': None})\n",
      "data = data.replace({'-': None})\n",
      "\n",
      "data['value'] = data['value'].astype(float)\n",
      "\n",
      "mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))\n",
      "data[mask]['value'] =  -1* data[mask]['value']\n",
      "\n",
      "data['period_from'] = pd.to_datetime(data['period_from'], utc = True)\n",
      "\n",
      "data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "plot_lines(\n",
      "    flow_data = data,\n",
      "    point_data = masked_points\n",
      ")\n",
      "13/21:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_breaks = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_breaks = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "plot = (\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_breaks = date_breaks)\n",
      "#+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "13/22:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "flow_data['value'] = flow_data['value'].astype(float)\n",
      "\n",
      "# Get right flow status, provisional, confirmed etc.\n",
      "flow_data = flow_data[(flow_data['flow_status'] == flow_status)]\n",
      "\n",
      "mask = (flow_data['direction_key'] == 'exit')\n",
      "flow_data[mask]['value'] =  -1* flow_data[mask]['value']\n",
      "\n",
      "# Turn to datetime\n",
      "flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)\n",
      "\n",
      "# Join together\n",
      "merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))\n",
      "\n",
      "merged.rename(\n",
      "    columns = {\n",
      "        't_so_country' : 'country',\n",
      "    },\n",
      "    inplace= True\n",
      ")\n",
      "# Group by point and period_from\n",
      "if aggregation:\n",
      "    date_breaks = '1 month'\n",
      "    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "else:\n",
      "    date_breaks = '1 month'\n",
      "    merged_grouped = merged.groupby(['period_from', 'point_label', facet_row, facet_col]).agg(\n",
      "        {'value': 'sum'}\n",
      "    ).reset_index()\n",
      "\n",
      "(\n",
      "p9.ggplot(merged_grouped)\n",
      "+ p9.aes(x='period_from', y='value', color = 'point_label')\n",
      "+ p9.geom_line() \n",
      "+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\\nLibrary: entsog-py')\n",
      "+ p9.scale_x_date(date_breaks = date_breaks)\n",
      "#+ p9.scale_y_continuous(labels= lambda l: [f\"{(v / UNIT_TRANSFORMATION[unit])} {unit}\" for v in l])\n",
      "#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))\n",
      "+ p9.scale_color_brewer(type=\"qual\", palette=\"Accent\", name = 'Point Label')\n",
      ")\n",
      "13/23:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data)\n",
      "13/24:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data)\n",
      "13/25:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data)\n",
      "13/26:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data)\n",
      "13/27:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data)\n",
      "13/28:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data)\n",
      "13/29:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data, point_date = masked_points)\n",
      "13/30:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_country'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data, point_data = masked_points)\n",
      "14/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "14/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "14/3:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "15/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220401', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "15/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "15/3:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "15/4:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "15/5:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "15/6:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "16/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "16/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "16/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data, point_data = masked_points)\n",
      "16/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data, point_data = masked_points)\n",
      "16/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "plot_lines(flow_data = flow_data, point_data = masked_points)\n",
      "16/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "16/7:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "17/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "17/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "17/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "17/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "18/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "18/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "18/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "19/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "19/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "19/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "20/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220301', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "20/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "20/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points))\n",
      "20/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh'))\n",
      "21/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "21/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "21/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh'))\n",
      "21/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'm3'))\n",
      "22/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "22/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "22/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "22/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "23/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "23/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "23/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "24/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "24/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "24/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "25/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "25/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "25/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "26/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "26/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "26/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "26/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh'))\n",
      "26/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "27/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "27/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "27/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "28/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "28/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "28/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))\n",
      "28/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'connected_operators'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "28/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "29/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "29/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "29/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "29/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "29/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "29/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_point())\n",
      "29/7:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size =1))\n",
      "29/8:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size =0.5))\n",
      "29/9:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size =0.5, alpha = 0.5))\n",
      "29/10:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5))\n",
      "29/11:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5, fill = 'value'))\n",
      "29/12:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5))\n",
      "29/13:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5, fill = 'point_label'))\n",
      "30/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "30/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "31/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "31/4:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220601', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "31/5:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "31/7:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "print(keys)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/8:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    print(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/9:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    print(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/10:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "31/11:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "print(end)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "31/12:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "print(end)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(max(data['period_from']))\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "32/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220601', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "32/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "print(end)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "32/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "33/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220601', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "33/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "print(end)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "33/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "34/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220601', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "34/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "print(end)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "34/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "34/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "35/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220601', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "35/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "keys = ['PL-TSO-0001ITP-00104entry']\n",
      "print(end)\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "35/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))\n",
      "35/4:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "35/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'normal'))\n",
      "35/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'identity'))\n",
      "36/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "36/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "36/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'identity'))\n",
      "36/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "36/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "mask = flow_data['flow_status'] = 'Confirmed'\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data[mask], point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "36/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data[mask], point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "37/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "37/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "37/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "38/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "38/2:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "38/3:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "38/4:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "38/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "39/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "39/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "39/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "40/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "40/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "40/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))\n",
      "42/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "42/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "42/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "43/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "43/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "43/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "44/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "44/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "44/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "45/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "45/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "45/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "45/4:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "45/5:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "45/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "46/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "46/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "46/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "46/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_fill_brewer(palette = 'Set1'))\n",
      "46/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_fill_brewer(type = 'qual',palette = 'Set1'))\n",
      "46/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_lines\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_fill_brewer(type = 'qual',palette = 'Pastel1'))\n",
      "47/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "47/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "47/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "47/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_color_brewer(color = 'black'))\n",
      "48/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "48/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "48/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "49/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "49/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "49/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "50/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "50/2:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "50/3:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "50/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "50/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size = 0.1))\n",
      "51/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "51/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "51/3:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "51/4:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('202204201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "51/5:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('202204201', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "51/6:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220420', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "51/7:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "51/8:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220220', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "51/9:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "51/10:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "52/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220220', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "52/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "52/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))\n",
      "52/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))\n",
      "52/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '3D'))\n",
      "52/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "52/7:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220425', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "52/8:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "52/9:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "52/10:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'kwh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "52/11:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'kWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "52/12:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "52/13:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'operator'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "52/14:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'operator_key'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "52/15:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'balancing_zone'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "53/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220425', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "53/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "53/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'balancing_zone'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "54/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220425', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "54/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "54/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'balancing_zone'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "54/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'operator_key'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "55/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220425', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "55/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "55/3:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220426', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "55/4:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "55/5:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'operator_label'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "55/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "55/7:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "from entsog.mappings import lookup_country\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "aggregation = None\n",
      "\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))\n",
      "55/8:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "55/9:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20210101', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220501', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "55/10:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "55/11:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "flow_data = data.replace({'': None})\n",
      "\n",
      "#mask = (flow_data['flow_status'] == 'Confirmed')\n",
      "print(max(flow_data['period_from']))\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "56/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20210420', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220510', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "56/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "56/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "56/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "56/5:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220420', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220510', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "56/6:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "56/7:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "56/8:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "56/9:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "56/10:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220401', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220510', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "56/11:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination','nomination'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "56/12:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination','nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "56/13:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220401', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220510', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "56/14:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "56/15:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/1:\n",
      "import sys\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "\n",
      "from entsog import EntsogPandasClient\n",
      "import pandas as pd\n",
      "\n",
      "client = EntsogPandasClient()\n",
      "\n",
      "start = pd.Timestamp('20220401', tz='Europe/Brussels')\n",
      "end = pd.Timestamp('20220510', tz='Europe/Brussels')\n",
      "\n",
      "points = client.query_operator_point_directions()\n",
      "mask = points['connected_operators'].str.contains('Gazprom')\n",
      "masked_points = points[mask]\n",
      "print(masked_points.head(5)[['point_key','point_label','point_type','region']])\n",
      "57/2:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/3:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'point_label'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/4:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/5:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/6:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/7:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/8:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/9:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['nomination'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/10:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = ['nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/11:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'renomination'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/12:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/13:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/14:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'nominations'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/15:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'allocation'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/16:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/17:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'firm_booked'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/18:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/19:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'firm_technical'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/20:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/21:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'country'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/22:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'adjacent_region'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/23:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'gcv','physical_flow','firm_technical'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/24:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "57/25:\n",
      "keys = []\n",
      "for idx, item in masked_points.iterrows():\n",
      "    keys.append(f\"{item['operator_key']}{item['point_key']}{item['direction_key']}\")\n",
      "\n",
      "#keys = ['PL-TSO-0001ITP-00104entry']\n",
      "data = client.query_operational_point_data(start = start, end = end, indicators = [ 'gcv','physical_flow','allocation'], point_directions = keys, period_type = 'day',verbose = False)\n",
      "\n",
      "print(data.head(5))\n",
      "backup = data.copy()\n",
      "57/26:\n",
      "import plotnine as p9\n",
      "import numpy as np\n",
      "\n",
      "from entsog.plot_utils import plot_area\n",
      "\n",
      "facet_row = 'country'\n",
      "facet_col = 'indicator'\n",
      "flow_status = 'Confirmed'\n",
      "\n",
      "print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))\n",
      "59/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "59/2: print(yc)\n",
      "59/3:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "yc['3 M'].plot(figsize=(15,5), lw=1, color=\"blue\", label='3-Month Government Bond')\n",
      "yc['6 M'].plot( lw=0.5, color=\"lightblue\",label='6-Month Government Bond')\n",
      "yc['9 M'].plot( lw=0.5, color=\"darkgreen\",label='9-Month Government Bond')\n",
      "yc['1 Y'].plot( lw=0.5, color=\"deepskyblue\",label='1-Year Government Bond')\n",
      "yc['2 Y'].plot( lw=0.5, color=\"dodgerblue\",label='2-Year Government Bond')\n",
      "yc['3 Y'].plot( lw=0.5, color=\"steelblue\",label='3-Year Government Bond')\n",
      "yc['4 Y'].plot( lw=0.5, color=\"blue\",label='4-Year Government Bond')\n",
      "yc['5 Y'].plot( lw=0.5, color=\"mediumblue\",label='5-Year Government Bond')\n",
      "yc['6 Y'].plot( lw=0.5, color=\"slategrey\", label='6-Year Government Bond')\n",
      "yc['7 Y'].plot( lw=0.5, color=\"gray\",label='7-Year Government Bond')\n",
      "yc['8 Y'].plot( lw=0.5, color=\"red\", grid=True,label='8-Year Government Bond')\n",
      "yc['9 Y'].plot( lw=0.5, color=\"orange\", grid=True,label='9-Year Government Bond')\n",
      "yc['10 Y'].plot( lw=0.5, color=\"green\", grid=True,label='10-Year Government Bond')\n",
      "yc['15 Y'].plot( lw=0.5, color=\"cyan\", grid=True,label='15-Year Government Bond')\n",
      "yc['20 Y'].plot( lw=0.5, color=\"yellow\", grid=True,label='20-Year Government Bond')\n",
      "yc['25 Y'].plot( lw=0.5, color=\"pink\", grid=True,label='25-Year Government Bond')\n",
      "yc['30 Y'].plot( lw=0.5, color=\"black\", grid=True,label='30-Year Government Bond')\n",
      "plt.legend(bbox_to_anchor=(1, 1.02))\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Canadian Government Bond Rate\")\n",
      "plt.title(\"The Canadian Term Structure of Interest Rate\", fontweight='bold')\n",
      "59/4:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "59/5:\n",
      "# Maturity\n",
      "header = []\n",
      "for name in ycn.dtype.names[1:]:\n",
      "    maturity = float(name.split(\" \")[0])\n",
      "    if name.split(\" \")[1] == 'M':\n",
      "        maturity = maturity / 12\n",
      "    header.append(maturity)\n",
      "59/6:\n",
      "# We create three empty lists \n",
      "x_data = []; y_data = []; z_data = []\n",
      "for dt in ycn.Date:\n",
      "    dt_num = dates.date2num(dt)\n",
      "    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])\n",
      "# print ('x_data: ', x_data[1:5])\n",
      "59/7:\n",
      "for row in ycn:\n",
      "    y_data.append(header)\n",
      "    z_data.append(list(row.tolist()[1:]))\n",
      "# print ('y_data: ', y_data[1:5])\n",
      "# print ('z_data: ', z_data[1:5])\n",
      "59/8:\n",
      "x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')\n",
      "# x ==> Dates\n",
      "# y ==> Maturities\n",
      "# z ==> Yields\n",
      "# print ('x:', x) \n",
      "# print ('y: ', y)\n",
      "# print ('z: ', z)\n",
      "59/9:\n",
      "%matplotlib inline\n",
      "fig = plt.figure(figsize=(15, 10))\n",
      "ax = fig.add_subplot(111, projection='3d')\n",
      "z_percent = z*100\n",
      "ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))\n",
      "ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')\n",
      "ax.set_ylabel('Maturity (\\u03C4)')\n",
      "ax.set_zlabel('Yield (Percent)')\n",
      "plt.savefig('my_pgf_plot.jpeg')\n",
      "\n",
      "def format_date(x, pos=None):\n",
      "     return dates.num2date(x).strftime('%Y')\n",
      "ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
      "for tl in ax.w_xaxis.get_ticklabels():\n",
      "    tl.set_ha('right')\n",
      "    tl.set_rotation(40)\n",
      "plt.show()\n",
      "59/10:\n",
      "# Stylized fact 1: The average yield curve over time is increasing and concave\n",
      "average_yc = yc.mean(axis=0) * 100\n",
      "average_yc.plot()\n",
      "59/11:\n",
      "# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, \n",
      "# humped, inverted humped, S-shapes)\n",
      "curve_1 = yc.iloc[1]*100\n",
      "curve_2 = yc.iloc[4174]*100 #2000\n",
      "curve_3 = yc.iloc[5600]*100\n",
      "curve_4 = yc.iloc[7850]*100\n",
      "curve_1.plot()\n",
      "curve_2.plot()\n",
      "curve_3.plot()\n",
      "curve_4.plot()\n",
      "59/12:\n",
      "# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)\n",
      "# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities\n",
      "first_autocorrelation = []\n",
      "tenth_autocorrelation = []\n",
      "twentieth_autocorrelation = []\n",
      "for maturity in yc:\n",
      "    first_autocorrelation.append(yc[maturity].autocorr())\n",
      "    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))\n",
      "    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))\n",
      "\n",
      "# autocorrelations = \n",
      "# s = yc['3 M']\n",
      "# s.autocorr()\n",
      "data = []\n",
      "for maturity in yc:\n",
      "    data.append(maturity)\n",
      "\n",
      "autocorrelations = pd.DataFrame(data, columns=['Maturity'])\n",
      "autocorrelations['First autocorrelation'] = first_autocorrelation \n",
      "autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation\n",
      "autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation\n",
      "autocorrelations.round(5)\n",
      "59/13:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "betas = np.zeros((rows, 3))\n",
      "levels = []\n",
      "slopes = []\n",
      "curvatures = []\n",
      "for maturity in header:\n",
      "    levels.append(1)\n",
      "    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))\n",
      "    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))\n",
      "59/14:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "59/15:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "betas = np.zeros((rows, 3))\n",
      "levels = []\n",
      "slopes = []\n",
      "curvatures = []\n",
      "for maturity in header:\n",
      "    levels.append(1)\n",
      "    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))\n",
      "    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))\n",
      "59/16:\n",
      "lambda_max = lmbd_range[np.argmax(lmbd_values)]\n",
      "print(lambda_max)\n",
      "59/17:\n",
      "def find_lambda(lmbd):\n",
      "    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)\n",
      "    return  lmbd_val\n",
      "    \n",
      "lmbd_range = np.arange(-0.1,2,0.0001)\n",
      "lmbd_values = []\n",
      "for i in lmbd_range:\n",
      "    lmbd_values.append(find_lambda(i))\n",
      "plt.plot(lmbd_range,lmbd_values)\n",
      "59/18:\n",
      "lambda_max = lmbd_range[np.argmax(lmbd_values)]\n",
      "print(lambda_max)\n",
      "59/19:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "betas = np.zeros((rows, 3))\n",
      "levels = []\n",
      "slopes = []\n",
      "curvatures = []\n",
      "for maturity in header:\n",
      "    levels.append(1)\n",
      "    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))\n",
      "    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))\n",
      "59/20: one_row = levels + slopes + curvatures\n",
      "59/21:\n",
      "(rows, cols)= np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    B = np.reshape(one_row, (np.size(levels), 3))\n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "    \n",
      "print(beta)\n",
      "59/22:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "betas = np.zeros((rows, 3))\n",
      "levels = []\n",
      "slopes = []\n",
      "curvatures = []\n",
      "for maturity in header:\n",
      "    levels.append(1)\n",
      "    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))\n",
      "    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))\n",
      "\n",
      "print(curvatures)\n",
      "59/23:\n",
      "# Create the forecast functions\n",
      "\n",
      "# Nelson-siegel function\n",
      "def nelson_siegel(t, beta1, beta2, beta3):\n",
      "    return beta1 + beta2*(1 - math.exp(-lambda_max*t))/(lambda_max*t) + beta3*((1 - math.exp(-lambda_max*t))/(lambda_max*t) - math.exp(-lambda_max*t))\n",
      "\n",
      "# Nelson-siegel-svensson function\n",
      "def nelson_siegel_svensson(t, beta1, beta2, beta3, beta4, beta5):\n",
      "    return beta1 + beta2*(1 - math.exp(-lambda_max*t))/(lambda_max*t) + beta3*((1 - math.exp(-lambda_max*t))/(lambda_max*t) - math.exp(-lambda_max*t)) + beta4*(1 - math.exp(-lambda_max*t))/(lambda_max*t) + beta5*((1 - math.exp(-lambda_max*t))/(lambda_max*t) - math.exp(-lambda_max*t))\n",
      "\n",
      "# Build a neural network with algorithm\n",
      "59/24:\n",
      "# Create the forecast functions\n",
      "\n",
      "# Convert the following r function to python\n",
      "# Nelson siegel forecasts\n",
      "# ns_ar_forecast <- function(beta,step_size){\n",
      "#   shift_beta <- lag(beta,step_size)\n",
      "#   temp_beta <- beta[-c(1:step_size),]\n",
      "#   shift_beta <- shift_beta[-c(1:step_size),]\n",
      "  \n",
      "#   regr <- vector(mode = 'list', length = 3)\n",
      "  \n",
      "#   forecasted_beta <- beta # To be adjusted in following loop\n",
      "#   for(f in 1:3){\n",
      "#     subset <- cbind(temp_beta[,f], shift_beta[,f] %>% as.data.frame())\n",
      "#     colnames(subset) <- c(\"y\",\"x\")\n",
      "    \n",
      "#     regr[[f]] <- lm(formula = 'y ~ x', data = subset)\n",
      "#     names(regr)[f] <- colnames(temp_beta)[f]\n",
      "    \n",
      "#     const <- coef(regr[[f]])[1]\n",
      "#     gamma <- coef(regr[[f]])[2]\n",
      "    \n",
      "#     forecasted_beta[,f] <- const + gamma * beta[,f]\n",
      "    \n",
      "#   }\n",
      "  \n",
      "  \n",
      "  \n",
      "#   forecasted_y <- forecasted_beta[,1] + forecasted_beta[,2] %*% slope_vector +  forecasted_beta[,3] %*% curvature_vector\n",
      "  \n",
      "#   return(tail(forecasted_y,1))\n",
      "# }\n",
      "59/25:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "print(B)\n",
      "60/1:\n",
      "(rows, cols) = np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "\n",
      "print(beta)\n",
      "59/26:\n",
      "(rows, cols) = np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "\n",
      "print(beta)\n",
      "59/27: np.shape(beta)\n",
      "59/28:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = beta # To be adjusted in following loop\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[:,f] = const + gamma * beta[:,f]\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/29:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = beta # To be adjusted in following loop\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[:,f] = const + gamma * beta[:,f]\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/30:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "59/31:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = beta # To be adjusted in following loop\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[:,f] = const + gamma * beta[:,f]\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/32:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = beta # To be adjusted in following loop\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[:,f] = const + gamma * beta[:,f]\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/33:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "59/34:\n",
      "(rows, cols) = np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "\n",
      "print(beta)\n",
      "59/35:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = beta # To be adjusted in following loop\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[:,f] = const + gamma * beta[:,f]\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/36:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = beta # To be adjusted in following loop\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[:,f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/37:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/38:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        print(const + gamma * beta[:,f])\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/39:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())\n",
      "        const = regr[f].params[0]\n",
      "        gamma = regr[f].params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/40:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr = sm.OLS(subset[:,0], subset[:,1]).fit()\n",
      "        const = regr.params[0]\n",
      "        gamma = regr.params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/41:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        regr = sm.OLS(subset[:,0], subset[:,1]).fit()\n",
      "        print(regr.summary())\n",
      "        const = regr.params[0]\n",
      "        gamma = regr.params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/42:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        # fit regr with constant\n",
      "        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()\n",
      "        print(regr.summary())\n",
      "        const = regr.params[0]\n",
      "        gamma = regr.params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(forecasted_beta[:,1])\n",
      "    print(slopes)\n",
      "    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures\n",
      "    return forecasted_y[-1]\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/43:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        # fit regr with constant\n",
      "        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()\n",
      "        print(regr.summary())\n",
      "        const = regr.params[0]\n",
      "        gamma = regr.params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures\n",
      "    print(forecasted_y)\n",
      "    return forecasted_y\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/44:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        # fit regr with constant\n",
      "        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()\n",
      "        print(regr.summary())\n",
      "        const = regr.params[0]\n",
      "        gamma = regr.params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(np.shape(slopes))\n",
      "    print(np.shape(curvatures))\n",
      "    forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures\n",
      "    print(forecasted_y)\n",
      "    return forecasted_y\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/45:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast\n",
      "def ns_ar_forecast(beta, maturity, t):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    shift_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    shift_beta = shift_beta[step_size:,:]\n",
      "    regr = []\n",
      "    forecasted_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        print(f)\n",
      "        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "        # fit regr with constant\n",
      "        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()\n",
      "        print(regr.summary())\n",
      "        const = regr.params[0]\n",
      "        gamma = regr.params[1]\n",
      "        forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "    print(np.shape(slopes))\n",
      "    print(np.shape(curvatures))\n",
      "    forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures\n",
      "    print(forecasted_y)\n",
      "    return forecasted_y\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/46:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "\n",
      "    # Create the X matrix (iota, lagged_beta)\n",
      "    x = np.concatenate((np.ones((np.shape(temp_beta)[0],1)), lagged_beta), axis=1)\n",
      "    # Create the Y matrix (temp_beta)\n",
      "    y = temp_beta\n",
      "    \n",
      "    regression = sm.OLS(y, x).fit()\n",
      "    print(regression.summary())\n",
      "\n",
      "\n",
      "# def ns_ar_forecast(beta, step_size):\n",
      "#     shift_beta = np.roll(beta, step_size, axis=0)\n",
      "#     temp_beta = beta[step_size:,:]\n",
      "#     shift_beta = shift_beta[step_size:,:]\n",
      "#     regr = []\n",
      "#     forecasted_beta = [None, None, None]\n",
      "#     for f in range(3):\n",
      "#         print(f)\n",
      "#         subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "#         # fit regr with constant\n",
      "#         regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()\n",
      "#         print(regr.summary())\n",
      "#         const = regr.params[0]\n",
      "#         gamma = regr.params[1]\n",
      "#         forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "#     print(np.shape(slopes))\n",
      "#     print(np.shape(curvatures))\n",
      "#     forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures\n",
      "#     print(forecasted_y)\n",
      "#     return forecasted_y\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/47:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "\n",
      "    # Create the X matrix (iota, lagged_beta)\n",
      "    x = np.concatenate((np.ones((np.shape(temp_beta)[0],1)), lagged_beta), axis=1)\n",
      "    # Create the Y matrix (temp_beta)\n",
      "    y = temp_beta\n",
      "    \n",
      "    regression = sm.OLS(y, x).fit()\n",
      "    print(regression.summary())\n",
      "\n",
      "\n",
      "# def ns_ar_forecast(beta, step_size):\n",
      "#     shift_beta = np.roll(beta, step_size, axis=0)\n",
      "#     temp_beta = beta[step_size:,:]\n",
      "#     shift_beta = shift_beta[step_size:,:]\n",
      "#     regr = []\n",
      "#     forecasted_beta = [None, None, None]\n",
      "#     for f in range(3):\n",
      "#         print(f)\n",
      "#         subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)\n",
      "#         # fit regr with constant\n",
      "#         regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()\n",
      "#         print(regr.summary())\n",
      "#         const = regr.params[0]\n",
      "#         gamma = regr.params[1]\n",
      "#         forecasted_beta[f] = const + gamma * beta[:,f]\n",
      "        \n",
      "#     print(np.shape(slopes))\n",
      "#     print(np.shape(curvatures))\n",
      "#     forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures\n",
      "#     print(forecasted_y)\n",
      "#     return forecasted_y\n",
      "\n",
      "# Reset\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/48:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size, slope, curvature):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = np.concatenate((np.ones((np.shape(temp_beta)[0],1)), lagged_beta[:,f]), axis=1)\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, x).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slope + fc_beta[2] * curvature\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    slope = slopes[i]\n",
      "    curvature = curvatures[i]\n",
      "    \n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE, slope , curvature)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/49:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size, slope, curvature):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slope + fc_beta[2] * curvature\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    slope = slopes[i]\n",
      "    curvature = curvatures[i]\n",
      "    \n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE, slope , curvature)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/50:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "print(np.shape(B))\n",
      "59/51:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size, slope, curvature):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    slope = slopes[i]\n",
      "    curvature = curvatures[i]\n",
      "    \n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/52:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    slope = slopes[i]\n",
      "    curvature = curvatures[i]\n",
      "    \n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/53:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(subset_beta)\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/54:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/55:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "print(f\"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}\")\n",
      "59/56:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "print(f\"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}\")\n",
      "\n",
      "print(forecast_yields['random_walk'])\n",
      "59/57:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "#print(f\"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}\")\n",
      "\n",
      "print(forecast_yields['nelson_siegel'])\n",
      "59/58:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "#print(f\"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}\")\n",
      "\n",
      "#print(forecast_yields['nelson_siegel'])\n",
      "\n",
      "# Convert to dataframe\n",
      "\n",
      "forecast_yields_df = pd.DataFrame(forecast_yields)\n",
      "print(forecast_yields_df)\n",
      "59/59:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    #'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    #forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/60:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "#print(f\"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}\")\n",
      "\n",
      "#print(forecast_yields['nelson_siegel'])\n",
      "\n",
      "# Convert to dataframe\n",
      "\n",
      "forecast_yields_df = pd.DataFrame(forecast_yields)\n",
      "print(forecast_yields_df)\n",
      "59/61:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    #'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = subset.iloc[-1]\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    print(nelson_siegel)\n",
      "    break\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    #forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/62:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    #'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    print(nelson_siegel)\n",
      "    print(random_walk)\n",
      "    break\n",
      "    # Realized\n",
      "    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]\n",
      "\n",
      "    # Append to dictionary\n",
      "    #forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/63:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    #'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    print(nelson_siegel)\n",
      "    print(random_walk)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE+STEP_SIZE])\n",
      "    print(realized)\n",
      "    break\n",
      "    # Append to dictionary\n",
      "    #forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/64:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    #'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    print(nelson_siegel)\n",
      "    print(random_walk)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE+STEP_SIZE])\n",
      "    print(realized)\n",
      "    print(yields.index)\n",
      "    break\n",
      "    # Append to dictionary\n",
      "    #forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "59/65:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE+STEP_SIZE])\n",
      "    index = yields.index[i+WINDOW_SIZE+STEP_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/66:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    tmp = {\n",
      "        'index' : forecast_yields['index'],\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/67:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    tmp = {\n",
      "        'index' : forecast_yields['index'],\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/68:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/69:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.dim(np.array(forecast_yields['index'])))\n",
      "    print(np.dim(np.array(forecast_yields['random_walk'][idx])))\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/70:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(np.array(forecast_yields['random_walk'][idx])))\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/71:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(np.array(forecast_yields['random_walk'][idx,:])))\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/72:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/73:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk'][:,idx]))\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/74:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/75:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    print(forecast_yields['random_walk'])\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/76:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    print(forecast_yields['random_walk'])\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    print(random_walk)\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/77:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    print(forecast_yields['random_walk'])\n",
      "    # Select the idx element from every forecast\n",
      "    print('SELECTINg')\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    print(random_walk)\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/78:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    print(forecast_yields['random_walk'])\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    print(random_walk)\n",
      "    break\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : forecast_yields['random_walk'][idx],\n",
      "        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],\n",
      "        'realized' : forecast_yields['realized'][idx]\n",
      "    }\n",
      "    print(tmp)\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/79:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    print(np.shape(np.array(forecast_yields['index'])))\n",
      "    print(np.shape(forecast_yields['random_walk']))\n",
      "    print(forecast_yields['random_walk'])\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/80:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "59/81:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "print(maturity_forecasts['1 M'].head())\n",
      "59/82:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "print(maturity_forecasts)\n",
      "\n",
      "print(maturity_forecasts['1 M'].head())\n",
      "59/83:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts)\n",
      "59/84:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "59/85:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/86:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "59/87:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "59/88:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "plot_forecasts(maturity_forecasts, '30 Y')\n",
      "59/89:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    \n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "59/90:\n",
      "# Do analysis on the forecasts\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "59/91:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "59/92:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"value\", hue=\"variable\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "    plt.show()\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "59/93:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "59/94:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/95:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/96:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "59/97:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "59/98:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/99:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "59/100:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/101:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "59/102:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yields, step_size):\n",
      "    lagged = np.roll(yields, step_size, axis=0)\n",
      "    temp = yields[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yields[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/103:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    \n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/104:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(subset)\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/105:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to matrix (drop the date column)\n",
      "yields = yields.drop('Date', axis=1)\n",
      "yields = yields.as_matrix()\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/106:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to matrix (drop the date column)\n",
      "print(yields)\n",
      "yields = yields.drop('Date', axis=1)\n",
      "yields = yields.as_matrix()\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/107:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to matrix (drop the date column)\n",
      "print(yields)\n",
      "yields = yields.as_matrix()\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/108:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields.iloc[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/109:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset.iloc[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields.iloc[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/110:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[maturity]\n",
      "        x = lagged[maturity]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/111:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/112:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx,] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/113:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty array to store forecasted yield levels\n",
      "    fc_y = np.zeros((1, len(header)))\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        print(fc_y)\n",
      "        fc_y[idx,] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/114:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yields = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yields[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yields[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/115:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*2\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "59/116:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "61/1:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/2:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "61/4:\n",
      "def find_lambda(lmbd):\n",
      "    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)\n",
      "    return  lmbd_val\n",
      "    \n",
      "lmbd_range = np.arange(-0.1,2,0.0001)\n",
      "lmbd_values = []\n",
      "for i in lmbd_range:\n",
      "    lmbd_values.append(find_lambda(i))\n",
      "plt.plot(lmbd_range,lmbd_values)\n",
      "61/5:\n",
      "import math\n",
      "def find_lambda(lmbd):\n",
      "    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)\n",
      "    return  lmbd_val\n",
      "    \n",
      "lmbd_range = np.arange(-0.1,2,0.0001)\n",
      "lmbd_values = []\n",
      "for i in lmbd_range:\n",
      "    lmbd_values.append(find_lambda(i))\n",
      "plt.plot(lmbd_range,lmbd_values)\n",
      "61/6:\n",
      "lambda_max = lmbd_range[np.argmax(lmbd_values)]\n",
      "print(lambda_max)\n",
      "61/7:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "print(np.shape(B))\n",
      "61/8: header\n",
      "61/9:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "61/10: print(yc)\n",
      "61/11:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "yc['3 M'].plot(figsize=(15,5), lw=1, color=\"blue\", label='3-Month Government Bond')\n",
      "yc['6 M'].plot( lw=0.5, color=\"lightblue\",label='6-Month Government Bond')\n",
      "yc['9 M'].plot( lw=0.5, color=\"darkgreen\",label='9-Month Government Bond')\n",
      "yc['1 Y'].plot( lw=0.5, color=\"deepskyblue\",label='1-Year Government Bond')\n",
      "yc['2 Y'].plot( lw=0.5, color=\"dodgerblue\",label='2-Year Government Bond')\n",
      "yc['3 Y'].plot( lw=0.5, color=\"steelblue\",label='3-Year Government Bond')\n",
      "yc['4 Y'].plot( lw=0.5, color=\"blue\",label='4-Year Government Bond')\n",
      "yc['5 Y'].plot( lw=0.5, color=\"mediumblue\",label='5-Year Government Bond')\n",
      "yc['6 Y'].plot( lw=0.5, color=\"slategrey\", label='6-Year Government Bond')\n",
      "yc['7 Y'].plot( lw=0.5, color=\"gray\",label='7-Year Government Bond')\n",
      "yc['8 Y'].plot( lw=0.5, color=\"red\", grid=True,label='8-Year Government Bond')\n",
      "yc['9 Y'].plot( lw=0.5, color=\"orange\", grid=True,label='9-Year Government Bond')\n",
      "yc['10 Y'].plot( lw=0.5, color=\"green\", grid=True,label='10-Year Government Bond')\n",
      "yc['15 Y'].plot( lw=0.5, color=\"cyan\", grid=True,label='15-Year Government Bond')\n",
      "yc['20 Y'].plot( lw=0.5, color=\"yellow\", grid=True,label='20-Year Government Bond')\n",
      "yc['25 Y'].plot( lw=0.5, color=\"pink\", grid=True,label='25-Year Government Bond')\n",
      "yc['30 Y'].plot( lw=0.5, color=\"black\", grid=True,label='30-Year Government Bond')\n",
      "plt.legend(bbox_to_anchor=(1, 1.02))\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Canadian Government Bond Rate\")\n",
      "plt.title(\"The Canadian Term Structure of Interest Rate\", fontweight='bold')\n",
      "61/12:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "61/13:\n",
      "# Maturity\n",
      "header = []\n",
      "for name in ycn.dtype.names[1:]:\n",
      "    maturity = float(name.split(\" \")[0])\n",
      "    if name.split(\" \")[1] == 'M':\n",
      "        maturity = maturity / 12\n",
      "    header.append(maturity)\n",
      "61/14:\n",
      "# We create three empty lists \n",
      "x_data = []; y_data = []; z_data = []\n",
      "for dt in ycn.Date:\n",
      "    dt_num = dates.date2num(dt)\n",
      "    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])\n",
      "# print ('x_data: ', x_data[1:5])\n",
      "61/15:\n",
      "for row in ycn:\n",
      "    y_data.append(header)\n",
      "    z_data.append(list(row.tolist()[1:]))\n",
      "# print ('y_data: ', y_data[1:5])\n",
      "# print ('z_data: ', z_data[1:5])\n",
      "61/16:\n",
      "x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')\n",
      "# x ==> Dates\n",
      "# y ==> Maturities\n",
      "# z ==> Yields\n",
      "# print ('x:', x) \n",
      "# print ('y: ', y)\n",
      "# print ('z: ', z)\n",
      "61/17:\n",
      "%matplotlib inline\n",
      "fig = plt.figure(figsize=(15, 10))\n",
      "ax = fig.add_subplot(111, projection='3d')\n",
      "z_percent = z*100\n",
      "ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))\n",
      "ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')\n",
      "ax.set_ylabel('Maturity (\\u03C4)')\n",
      "ax.set_zlabel('Yield (Percent)')\n",
      "plt.savefig('my_pgf_plot.jpeg')\n",
      "\n",
      "def format_date(x, pos=None):\n",
      "     return dates.num2date(x).strftime('%Y')\n",
      "ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
      "for tl in ax.w_xaxis.get_ticklabels():\n",
      "    tl.set_ha('right')\n",
      "    tl.set_rotation(40)\n",
      "plt.show()\n",
      "61/18:\n",
      "# Stylized fact 1: The average yield curve over time is increasing and concave\n",
      "average_yc = yc.mean(axis=0) * 100\n",
      "average_yc.plot()\n",
      "61/19:\n",
      "# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, \n",
      "# humped, inverted humped, S-shapes)\n",
      "curve_1 = yc.iloc[1]*100\n",
      "curve_2 = yc.iloc[4174]*100 #2000\n",
      "curve_3 = yc.iloc[5600]*100\n",
      "curve_4 = yc.iloc[7850]*100\n",
      "curve_1.plot()\n",
      "curve_2.plot()\n",
      "curve_3.plot()\n",
      "curve_4.plot()\n",
      "61/20: curve_1.plot()\n",
      "61/21: curve_2.plot()\n",
      "61/22: curve_3.plot()\n",
      "61/23: curve_4.plot()\n",
      "61/24:\n",
      "# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)\n",
      "# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities\n",
      "first_autocorrelation = []\n",
      "tenth_autocorrelation = []\n",
      "twentieth_autocorrelation = []\n",
      "for maturity in yc:\n",
      "    first_autocorrelation.append(yc[maturity].autocorr())\n",
      "    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))\n",
      "    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))\n",
      "\n",
      "# autocorrelations = \n",
      "# s = yc['3 M']\n",
      "# s.autocorr()\n",
      "data = []\n",
      "for maturity in yc:\n",
      "    data.append(maturity)\n",
      "\n",
      "autocorrelations = pd.DataFrame(data, columns=['Maturity'])\n",
      "autocorrelations['First autocorrelation'] = first_autocorrelation \n",
      "autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation\n",
      "autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation\n",
      "autocorrelations.round(5)\n",
      "61/25:\n",
      "# Stylized fact 5: The short end of the yield curve is more volatile than the long end of the curve\n",
      "stdevs = yc.std()\n",
      "stdevs.round(5)\n",
      "61/26:\n",
      "# Stylized fact 6: Yields for different maturities have high cross-correlations\n",
      "yc.corr().round(3)\n",
      "61/27: yc\n",
      "61/28: header\n",
      "61/29:\n",
      "import math\n",
      "def find_lambda(lmbd):\n",
      "    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)\n",
      "    return  lmbd_val\n",
      "    \n",
      "lmbd_range = np.arange(-0.1,2,0.0001)\n",
      "lmbd_values = []\n",
      "for i in lmbd_range:\n",
      "    lmbd_values.append(find_lambda(i))\n",
      "plt.plot(lmbd_range,lmbd_values)\n",
      "61/30:\n",
      "lambda_max = lmbd_range[np.argmax(lmbd_values)]\n",
      "print(lambda_max)\n",
      "61/31:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "print(np.shape(B))\n",
      "61/32:\n",
      "(rows, cols) = np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "\n",
      "print(beta)\n",
      "61/33: np.shape(beta)\n",
      "61/34:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/35:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/36:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/37:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/38:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/39:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "\n",
      "\n",
      "# Group by month and take the average\n",
      "yc = yc.groupby(yc.index.month).mean()\n",
      "print(yc)\n",
      "61/40:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "\n",
      "\n",
      "# Group by month (timestamp) and take the average\n",
      "yc = yc.groupby(pd.Grouper(freq='M')).mean()\n",
      "print(yc)\n",
      "61/41:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "# Group by month (timestamp) and take the average\n",
      "yc = yc.groupby(pd.Grouper(freq='M')).mean()\n",
      "61/42: print(yc)\n",
      "61/43:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "yc['3 M'].plot(figsize=(15,5), lw=1, color=\"blue\", label='3-Month Government Bond')\n",
      "yc['6 M'].plot( lw=0.5, color=\"lightblue\",label='6-Month Government Bond')\n",
      "yc['9 M'].plot( lw=0.5, color=\"darkgreen\",label='9-Month Government Bond')\n",
      "yc['1 Y'].plot( lw=0.5, color=\"deepskyblue\",label='1-Year Government Bond')\n",
      "yc['2 Y'].plot( lw=0.5, color=\"dodgerblue\",label='2-Year Government Bond')\n",
      "yc['3 Y'].plot( lw=0.5, color=\"steelblue\",label='3-Year Government Bond')\n",
      "yc['4 Y'].plot( lw=0.5, color=\"blue\",label='4-Year Government Bond')\n",
      "yc['5 Y'].plot( lw=0.5, color=\"mediumblue\",label='5-Year Government Bond')\n",
      "yc['6 Y'].plot( lw=0.5, color=\"slategrey\", label='6-Year Government Bond')\n",
      "yc['7 Y'].plot( lw=0.5, color=\"gray\",label='7-Year Government Bond')\n",
      "yc['8 Y'].plot( lw=0.5, color=\"red\", grid=True,label='8-Year Government Bond')\n",
      "yc['9 Y'].plot( lw=0.5, color=\"orange\", grid=True,label='9-Year Government Bond')\n",
      "yc['10 Y'].plot( lw=0.5, color=\"green\", grid=True,label='10-Year Government Bond')\n",
      "yc['15 Y'].plot( lw=0.5, color=\"cyan\", grid=True,label='15-Year Government Bond')\n",
      "yc['20 Y'].plot( lw=0.5, color=\"yellow\", grid=True,label='20-Year Government Bond')\n",
      "yc['25 Y'].plot( lw=0.5, color=\"pink\", grid=True,label='25-Year Government Bond')\n",
      "yc['30 Y'].plot( lw=0.5, color=\"black\", grid=True,label='30-Year Government Bond')\n",
      "plt.legend(bbox_to_anchor=(1, 1.02))\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Canadian Government Bond Rate\")\n",
      "plt.title(\"The Canadian Term Structure of Interest Rate\", fontweight='bold')\n",
      "61/44:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "61/45:\n",
      "# Maturity\n",
      "header = []\n",
      "for name in ycn.dtype.names[1:]:\n",
      "    maturity = float(name.split(\" \")[0])\n",
      "    if name.split(\" \")[1] == 'M':\n",
      "        maturity = maturity / 12\n",
      "    header.append(maturity)\n",
      "61/46:\n",
      "# We create three empty lists \n",
      "x_data = []; y_data = []; z_data = []\n",
      "for dt in ycn.Date:\n",
      "    dt_num = dates.date2num(dt)\n",
      "    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])\n",
      "# print ('x_data: ', x_data[1:5])\n",
      "61/47:\n",
      "for row in ycn:\n",
      "    y_data.append(header)\n",
      "    z_data.append(list(row.tolist()[1:]))\n",
      "# print ('y_data: ', y_data[1:5])\n",
      "# print ('z_data: ', z_data[1:5])\n",
      "61/48:\n",
      "x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')\n",
      "# x ==> Dates\n",
      "# y ==> Maturities\n",
      "# z ==> Yields\n",
      "# print ('x:', x) \n",
      "# print ('y: ', y)\n",
      "# print ('z: ', z)\n",
      "61/49:\n",
      "# %matplotlib inline\n",
      "# fig = plt.figure(figsize=(15, 10))\n",
      "# ax = fig.add_subplot(111, projection='3d')\n",
      "# z_percent = z*100\n",
      "# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))\n",
      "# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')\n",
      "# ax.set_ylabel('Maturity (\\u03C4)')\n",
      "# ax.set_zlabel('Yield (Percent)')\n",
      "# plt.savefig('my_pgf_plot.jpeg')\n",
      "\n",
      "# def format_date(x, pos=None):\n",
      "#      return dates.num2date(x).strftime('%Y')\n",
      "# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
      "# for tl in ax.w_xaxis.get_ticklabels():\n",
      "#     tl.set_ha('right')\n",
      "#     tl.set_rotation(40)\n",
      "# plt.show()\n",
      "61/50:\n",
      "# Stylized fact 1: The average yield curve over time is increasing and concave\n",
      "average_yc = yc.mean(axis=0) * 100\n",
      "average_yc.plot()\n",
      "61/51:\n",
      "# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, \n",
      "# humped, inverted humped, S-shapes)\n",
      "curve_1 = yc.iloc[1]*100\n",
      "curve_2 = yc.iloc[4174]*100 #2000\n",
      "curve_3 = yc.iloc[5600]*100\n",
      "curve_4 = yc.iloc[7850]*100\n",
      "curve_1.plot()\n",
      "curve_2.plot()\n",
      "curve_3.plot()\n",
      "curve_4.plot()\n",
      "61/52:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "# Group by month (timestamp) and take the average\n",
      "yc = yc.groupby(pd.Grouper(freq='M')).mean()\n",
      "61/53: print(yc)\n",
      "61/54:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "yc['3 M'].plot(figsize=(15,5), lw=1, color=\"blue\", label='3-Month Government Bond')\n",
      "yc['6 M'].plot( lw=0.5, color=\"lightblue\",label='6-Month Government Bond')\n",
      "yc['9 M'].plot( lw=0.5, color=\"darkgreen\",label='9-Month Government Bond')\n",
      "yc['1 Y'].plot( lw=0.5, color=\"deepskyblue\",label='1-Year Government Bond')\n",
      "yc['2 Y'].plot( lw=0.5, color=\"dodgerblue\",label='2-Year Government Bond')\n",
      "yc['3 Y'].plot( lw=0.5, color=\"steelblue\",label='3-Year Government Bond')\n",
      "yc['4 Y'].plot( lw=0.5, color=\"blue\",label='4-Year Government Bond')\n",
      "yc['5 Y'].plot( lw=0.5, color=\"mediumblue\",label='5-Year Government Bond')\n",
      "yc['6 Y'].plot( lw=0.5, color=\"slategrey\", label='6-Year Government Bond')\n",
      "yc['7 Y'].plot( lw=0.5, color=\"gray\",label='7-Year Government Bond')\n",
      "yc['8 Y'].plot( lw=0.5, color=\"red\", grid=True,label='8-Year Government Bond')\n",
      "yc['9 Y'].plot( lw=0.5, color=\"orange\", grid=True,label='9-Year Government Bond')\n",
      "yc['10 Y'].plot( lw=0.5, color=\"green\", grid=True,label='10-Year Government Bond')\n",
      "yc['15 Y'].plot( lw=0.5, color=\"cyan\", grid=True,label='15-Year Government Bond')\n",
      "yc['20 Y'].plot( lw=0.5, color=\"yellow\", grid=True,label='20-Year Government Bond')\n",
      "yc['25 Y'].plot( lw=0.5, color=\"pink\", grid=True,label='25-Year Government Bond')\n",
      "yc['30 Y'].plot( lw=0.5, color=\"black\", grid=True,label='30-Year Government Bond')\n",
      "plt.legend(bbox_to_anchor=(1, 1.02))\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Canadian Government Bond Rate\")\n",
      "plt.title(\"The Canadian Term Structure of Interest Rate\", fontweight='bold')\n",
      "61/55:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "61/56:\n",
      "# Maturity\n",
      "header = []\n",
      "for name in ycn.dtype.names[1:]:\n",
      "    maturity = float(name.split(\" \")[0])\n",
      "    if name.split(\" \")[1] == 'M':\n",
      "        maturity = maturity / 12\n",
      "    header.append(maturity)\n",
      "61/57:\n",
      "# We create three empty lists \n",
      "x_data = []; y_data = []; z_data = []\n",
      "for dt in ycn.Date:\n",
      "    dt_num = dates.date2num(dt)\n",
      "    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])\n",
      "# print ('x_data: ', x_data[1:5])\n",
      "61/58:\n",
      "for row in ycn:\n",
      "    y_data.append(header)\n",
      "    z_data.append(list(row.tolist()[1:]))\n",
      "# print ('y_data: ', y_data[1:5])\n",
      "# print ('z_data: ', z_data[1:5])\n",
      "61/59:\n",
      "x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')\n",
      "# x ==> Dates\n",
      "# y ==> Maturities\n",
      "# z ==> Yields\n",
      "# print ('x:', x) \n",
      "# print ('y: ', y)\n",
      "# print ('z: ', z)\n",
      "61/60:\n",
      "# %matplotlib inline\n",
      "# fig = plt.figure(figsize=(15, 10))\n",
      "# ax = fig.add_subplot(111, projection='3d')\n",
      "# z_percent = z*100\n",
      "# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))\n",
      "# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')\n",
      "# ax.set_ylabel('Maturity (\\u03C4)')\n",
      "# ax.set_zlabel('Yield (Percent)')\n",
      "# plt.savefig('my_pgf_plot.jpeg')\n",
      "\n",
      "# def format_date(x, pos=None):\n",
      "#      return dates.num2date(x).strftime('%Y')\n",
      "# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
      "# for tl in ax.w_xaxis.get_ticklabels():\n",
      "#     tl.set_ha('right')\n",
      "#     tl.set_rotation(40)\n",
      "# plt.show()\n",
      "61/61:\n",
      "# Stylized fact 1: The average yield curve over time is increasing and concave\n",
      "average_yc = yc.mean(axis=0) * 100\n",
      "average_yc.plot()\n",
      "61/62:\n",
      "# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, \n",
      "# humped, inverted humped, S-shapes)\n",
      "curve_1 = yc.iloc[1]*100\n",
      "curve_2 = yc.iloc[4174]*100 #2000\n",
      "curve_3 = yc.iloc[5600]*100\n",
      "curve_4 = yc.iloc[7850]*100\n",
      "curve_1.plot()\n",
      "curve_2.plot()\n",
      "curve_3.plot()\n",
      "curve_4.plot()\n",
      "61/63:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "# Group by month (timestamp) and take the average\n",
      "yc_monthly = yc.groupby(pd.Grouper(freq='M')).mean()\n",
      "61/64: print(yc)\n",
      "61/65:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "yc['3 M'].plot(figsize=(15,5), lw=1, color=\"blue\", label='3-Month Government Bond')\n",
      "yc['6 M'].plot( lw=0.5, color=\"lightblue\",label='6-Month Government Bond')\n",
      "yc['9 M'].plot( lw=0.5, color=\"darkgreen\",label='9-Month Government Bond')\n",
      "yc['1 Y'].plot( lw=0.5, color=\"deepskyblue\",label='1-Year Government Bond')\n",
      "yc['2 Y'].plot( lw=0.5, color=\"dodgerblue\",label='2-Year Government Bond')\n",
      "yc['3 Y'].plot( lw=0.5, color=\"steelblue\",label='3-Year Government Bond')\n",
      "yc['4 Y'].plot( lw=0.5, color=\"blue\",label='4-Year Government Bond')\n",
      "yc['5 Y'].plot( lw=0.5, color=\"mediumblue\",label='5-Year Government Bond')\n",
      "yc['6 Y'].plot( lw=0.5, color=\"slategrey\", label='6-Year Government Bond')\n",
      "yc['7 Y'].plot( lw=0.5, color=\"gray\",label='7-Year Government Bond')\n",
      "yc['8 Y'].plot( lw=0.5, color=\"red\", grid=True,label='8-Year Government Bond')\n",
      "yc['9 Y'].plot( lw=0.5, color=\"orange\", grid=True,label='9-Year Government Bond')\n",
      "yc['10 Y'].plot( lw=0.5, color=\"green\", grid=True,label='10-Year Government Bond')\n",
      "yc['15 Y'].plot( lw=0.5, color=\"cyan\", grid=True,label='15-Year Government Bond')\n",
      "yc['20 Y'].plot( lw=0.5, color=\"yellow\", grid=True,label='20-Year Government Bond')\n",
      "yc['25 Y'].plot( lw=0.5, color=\"pink\", grid=True,label='25-Year Government Bond')\n",
      "yc['30 Y'].plot( lw=0.5, color=\"black\", grid=True,label='30-Year Government Bond')\n",
      "plt.legend(bbox_to_anchor=(1, 1.02))\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Canadian Government Bond Rate\")\n",
      "plt.title(\"The Canadian Term Structure of Interest Rate\", fontweight='bold')\n",
      "61/66:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "61/67:\n",
      "# Maturity\n",
      "header = []\n",
      "for name in ycn.dtype.names[1:]:\n",
      "    maturity = float(name.split(\" \")[0])\n",
      "    if name.split(\" \")[1] == 'M':\n",
      "        maturity = maturity / 12\n",
      "    header.append(maturity)\n",
      "61/68:\n",
      "# We create three empty lists \n",
      "x_data = []; y_data = []; z_data = []\n",
      "for dt in ycn.Date:\n",
      "    dt_num = dates.date2num(dt)\n",
      "    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])\n",
      "# print ('x_data: ', x_data[1:5])\n",
      "61/69:\n",
      "for row in ycn:\n",
      "    y_data.append(header)\n",
      "    z_data.append(list(row.tolist()[1:]))\n",
      "# print ('y_data: ', y_data[1:5])\n",
      "# print ('z_data: ', z_data[1:5])\n",
      "61/70:\n",
      "x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')\n",
      "# x ==> Dates\n",
      "# y ==> Maturities\n",
      "# z ==> Yields\n",
      "# print ('x:', x) \n",
      "# print ('y: ', y)\n",
      "# print ('z: ', z)\n",
      "61/71:\n",
      "# %matplotlib inline\n",
      "# fig = plt.figure(figsize=(15, 10))\n",
      "# ax = fig.add_subplot(111, projection='3d')\n",
      "# z_percent = z*100\n",
      "# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))\n",
      "# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')\n",
      "# ax.set_ylabel('Maturity (\\u03C4)')\n",
      "# ax.set_zlabel('Yield (Percent)')\n",
      "# plt.savefig('my_pgf_plot.jpeg')\n",
      "\n",
      "# def format_date(x, pos=None):\n",
      "#      return dates.num2date(x).strftime('%Y')\n",
      "# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
      "# for tl in ax.w_xaxis.get_ticklabels():\n",
      "#     tl.set_ha('right')\n",
      "#     tl.set_rotation(40)\n",
      "# plt.show()\n",
      "61/72:\n",
      "# Stylized fact 1: The average yield curve over time is increasing and concave\n",
      "average_yc = yc.mean(axis=0) * 100\n",
      "average_yc.plot()\n",
      "61/73:\n",
      "# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, \n",
      "# humped, inverted humped, S-shapes)\n",
      "curve_1 = yc.iloc[1]*100\n",
      "curve_2 = yc.iloc[4174]*100 #2000\n",
      "curve_3 = yc.iloc[5600]*100\n",
      "curve_4 = yc.iloc[7850]*100\n",
      "curve_1.plot()\n",
      "curve_2.plot()\n",
      "curve_3.plot()\n",
      "curve_4.plot()\n",
      "61/74: curve_1.plot()\n",
      "61/75: curve_2.plot()\n",
      "61/76: curve_3.plot()\n",
      "61/77: curve_4.plot()\n",
      "61/78:\n",
      "# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)\n",
      "# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities\n",
      "first_autocorrelation = []\n",
      "tenth_autocorrelation = []\n",
      "twentieth_autocorrelation = []\n",
      "for maturity in yc:\n",
      "    first_autocorrelation.append(yc[maturity].autocorr())\n",
      "    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))\n",
      "    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))\n",
      "\n",
      "# autocorrelations = \n",
      "# s = yc['3 M']\n",
      "# s.autocorr()\n",
      "data = []\n",
      "for maturity in yc:\n",
      "    data.append(maturity)\n",
      "\n",
      "autocorrelations = pd.DataFrame(data, columns=['Maturity'])\n",
      "autocorrelations['First autocorrelation'] = first_autocorrelation \n",
      "autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation\n",
      "autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation\n",
      "autocorrelations.round(5)\n",
      "61/79:\n",
      "# Stylized fact 5: The short end of the yield curve is more volatile than the long end of the curve\n",
      "stdevs = yc.std()\n",
      "stdevs.round(5)\n",
      "61/80:\n",
      "# Stylized fact 6: Yields for different maturities have high cross-correlations\n",
      "yc.corr().round(3)\n",
      "61/81:\n",
      "yc = yc_monthly\n",
      "yc\n",
      "61/82: header\n",
      "61/83:\n",
      "import math\n",
      "def find_lambda(lmbd):\n",
      "    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)\n",
      "    return  lmbd_val\n",
      "    \n",
      "lmbd_range = np.arange(-0.1,2,0.0001)\n",
      "lmbd_values = []\n",
      "for i in lmbd_range:\n",
      "    lmbd_values.append(find_lambda(i))\n",
      "plt.plot(lmbd_range,lmbd_values)\n",
      "61/84:\n",
      "lambda_max = lmbd_range[np.argmax(lmbd_values)]\n",
      "print(lambda_max)\n",
      "61/85:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "print(np.shape(B))\n",
      "61/86:\n",
      "(rows, cols) = np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "\n",
      "print(beta)\n",
      "61/87: np.shape(beta)\n",
      "61/88:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 30\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/89:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/90:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/91:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 6\n",
      "WINDOW_SIZE = 365*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/92:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 6\n",
      "WINDOW_SIZE = 12*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/93:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "plot_forecasts_facet(residual_forecasts)\n",
      "61/94:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 6\n",
      "WINDOW_SIZE = 12*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/95:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "61/96:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    \n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        \n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 12*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/97:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "61/98:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "61/99:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['Realized'], df['Random walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['Realized'], df['Random walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['Realized'], df['Random walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['Realized'], df['Random walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['Realized'], df['Random walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['Realized'], df['Random walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['Realized'], df['Random walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['Realized'], df['Random walk'])\n",
      "            }\n",
      "        }\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "print(errors)\n",
      "61/100:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(df):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(ddf['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "print(errors)\n",
      "61/101:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(df):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(ddf['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "print(errors)\n",
      "61/102:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(df):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "print(errors)\n",
      "61/103:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "print(errors)\n",
      "61/104:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "errors\n",
      "61/105:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "errors\n",
      "61/106:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "errors['30 Y']\n",
      "\n",
      "errors['10 Y']\n",
      "61/107:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        #'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        #'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(pd.DataFrame(errors[key]).T)\n",
      "    print(\"\")\n",
      "61/108:\n",
      "np.shape(beta)\n",
      "print(beta[1,:])\n",
      "61/109:\n",
      "np.shape(beta)\n",
      "print(beta[:,1])\n",
      "61/110:\n",
      "np.shape(beta)\n",
      "print(beta[:,2])\n",
      "61/111:\n",
      "np.shape(beta)\n",
      "print(beta[:,3])\n",
      "61/112:\n",
      "np.shape(beta)\n",
      "print(beta[:,0])\n",
      "61/113:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 12*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "61/114:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(pd.DataFrame(errors[key]).T)\n",
      "    print(\"\")\n",
      "61/115:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(pd.DataFrame(errors[key]).T)\n",
      "    print(\"\")\n",
      "61/116:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(pd.DataFrame(errors[key]).T)\n",
      "    print(\"\")\n",
      "62/1:\n",
      "import pandas as pd\n",
      "import wetterdienst as wd\n",
      "\n",
      "# Get all stations from NOAA GHCN\n",
      "stations = wd.provider.noaa.ghcn.stations()\n",
      "print(stations)\n",
      "63/1:\n",
      "import pandas as pd\n",
      "import wetterdienst as wd\n",
      "\n",
      "# Get all stations from NOAA GHCN\n",
      "stations = wd.provider.noaa.ghcn.stations()\n",
      "print(stations)\n",
      "63/2:\n",
      "import pandas as pd\n",
      "import wetterdienst as wd\n",
      "\n",
      "# What is available for us\n",
      "wd.discover()\n",
      "\n",
      "\n",
      "# Get all stations from NOAA GHCN\n",
      "#stations = wd.provider.noaa.ghcn.stations()\n",
      "#print(stations)\n",
      "63/3:\n",
      "import pandas as pd\n",
      "from wetterdienst import Wetterdienst\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "\n",
      "\n",
      "# Get all stations from NOAA GHCN\n",
      "#stations = wd.provider.noaa.ghcn.stations()\n",
      "#print(stations)\n",
      "63/4:\n",
      "import pandas as pd\n",
      "from wetterdienst import Wetterdienst\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "API.discover()\n",
      "63/5:\n",
      "import pandas as pd\n",
      "from wetterdienst import Wetterdienst\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#API.discover()\n",
      "63/6:\n",
      "import pandas as pd\n",
      "from wetterdienst import Wetterdienst\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "63/7:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#API.discover()\n",
      "# Fetch all stations\n",
      "API.endpoints()\n",
      "63/8:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#API.discover()\n",
      "# Fetch all stations\n",
      "API.endpoints\n",
      "63/9:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#API.discover()\n",
      "# Fetch all stations\n",
      "API.stations()\n",
      "63/10:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#API.discover()\n",
      "# Fetch all stations\n",
      "API._all()\n",
      "63/11:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#API.discover()\n",
      "# Fetch all stations\n",
      "API.all()\n",
      "63/12:\n",
      "# We are interested in data from NOAA GHCN\n",
      "stations = wd.provider.noaa.ghcn.stations()\n",
      "63/13:\n",
      "# We are interested in data from NOAA GHCN\n",
      "stations = wd.provider.noaa.ghcn._all()\n",
      "63/14:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "# Show all functions for this API\n",
      "dir(API)\n",
      "63/15:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#dir(API)\n",
      "API.datasets\n",
      "63/16:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#dir(API)\n",
      "API.discover()\n",
      "63/17:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#dir(API)\n",
      "API._dataset_base\n",
      "63/18:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#dir(API)\n",
      "stations = API._all()\n",
      "63/19:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "#dir(API)\n",
      "stations = API._all(API)\n",
      "63/20: print(stations)\n",
      "63/21:\n",
      "# Plot all the stations on an interactive map with labels (using leaflet)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=row['NAME']).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations)\n",
      "63/22:\n",
      "# Plot all the stations on an interactive map with labels (using leaflet)\n",
      "print(stations)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=row['NAME']).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations)\n",
      "63/23:\n",
      "# Plot all the stations on an interactive map with labels (using leaflet)\n",
      "print(stations)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude '], row['longitude  ']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations)\n",
      "63/24:\n",
      "# Plot all the stations on an interactive map with labels (using leaflet)\n",
      "print(stations)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude '], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations)\n",
      "63/25:\n",
      "# Plot all the stations on an interactive map with labels (using leaflet)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations)\n",
      "63/26:\n",
      "# Plot all the stations on an interactive map with labels using clustering\n",
      "print(stations)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "#plot_stations(stations)\n",
      "63/27:\n",
      "# Plot all the stations on an interactive map with labels using clustering\n",
      "print(stations.columns)\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "#plot_stations(stations)\n",
      "63/28:\n",
      "# Plot all the stations on an interactive map with labels using clustering\n",
      "print(stations.columns)\n",
      "# Unique state\n",
      "print(stations['state'].unique())\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "#plot_stations(stations)\n",
      "63/29:\n",
      "# Plot all the stations on an interactive map with labels using clustering\n",
      "print(stations.head())\n",
      "print(stations.columns)\n",
      "# Unique state\n",
      "print(stations['state'].unique())\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "#plot_stations(stations)\n",
      "63/30:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['id'].str[0:2]\n",
      "stations.head()\n",
      "63/31:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id '].str[0:2]\n",
      "stations.head()\n",
      "63/32:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "stations.head()\n",
      "63/33:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "63/34:\n",
      "# What did not join\n",
      "print(stations[stations['alpha-3'].isna()])\n",
      "63/35:\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/36:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_ghcnd = pd.read_csv(\"http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-countries.txt\", sep = '\\t', header = ['country_code', 'country_name'])\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_ghcnd, left_on = 'country_code', right_on = 'country_code', how = 'left')\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "63/37:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_ghcnd = pd.read_csv(\"http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-countries.txt\", sep = '\\s', header = ['country_code', 'country_name'])\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_ghcnd, left_on = 'country_code', right_on = 'country_code', how = 'left')\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "63/38:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/39:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region_code'].isin(['EU'])]\n",
      "print(stations_eu.shape)\n",
      "63/40:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['EU'])]\n",
      "print(stations_eu.shape)\n",
      "63/41:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['EU'])]\n",
      "print(stations_eu)\n",
      "63/42:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu)\n",
      "63/43:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/44:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/45:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "#plot_stations(stations_eu)\n",
      "63/46:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "print(stations.shape())\n",
      "\n",
      "\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "print(stations.shape())\n",
      "63/47:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "# Dimensions of the data\n",
      "print(stations.shape)\n",
      "\n",
      "\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "print(stations.shape)\n",
      "63/48:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by a new line.\n",
      "        label  = row.to_string(header=False, index=False)\n",
      "        \n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/49:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/50:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['longitude'], row['latitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "63/51:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['longitude'], row['latitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/52:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/53:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv', header = ['fips','iso','fips_name'])\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/54:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')\n",
      "lookup_table_fips.columns = ['fips', 'iso', 'fips_name']\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/55:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "# Is Germany in the list?\n",
      "print (stations[stations['country_name'] == 'Germany'])\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "\n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/56:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    # Create fixed size for the map to display\n",
      "    m._build_map()\n",
      "    m.fit_bounds(m.get_bounds())\n",
      "    \n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    return m\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/57:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = row.to_string(header=False, index=False)\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/58:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Label is all the information in the row, seperated by \\n (new line)\n",
      "        label = '\\n'.join(row.astype(str))\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/59:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/60:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:6]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/61:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/62:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/63:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/64:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "# Generate some summary statistics\n",
      "\n",
      "# (1) How many stations are there per country?\n",
      "stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# (2) Average years of data per country (to_date - from_date) in years\n",
      "stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / 365\n",
      "stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)\n",
      "63/65:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "print(plot_stations(stations_eu))\n",
      "# Generate some summary statistics\n",
      "\n",
      "# (1) How many stations are there per country?\n",
      "print(stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False))\n",
      "# (2) Average years of data per country (to_date - from_date) in years\n",
      "stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)\n",
      "print(stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False))\n",
      "63/66:\n",
      "# (1) How many stations are there per country?\n",
      "result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# Show as bar chart\n",
      "result.plot(kind='bar', figsize=(20, 5), title='Number of stations per country')\n",
      "63/67:\n",
      "# (1) How many stations are there per country?\n",
      "result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "plt.title('Number of stations per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/68:\n",
      "# (1) How many stations are there per country?\n",
      "result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "plt.title('Number of stations per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/69:\n",
      "# (1) How many stations are there per country?\n",
      "result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "# Put numbers on top of bars\n",
      "for index, data in enumerate(result.values):\n",
      "    plt.text(x=index, y =data+1 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
      "plt.title('Number of stations per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/70:\n",
      "# (1) How many stations are there per country?\n",
      "result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "# Put numbers on top of bars\n",
      "for index, data in enumerate(result.values):\n",
      "    plt.text(x=index, y =data , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
      "plt.title('Number of stations per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/71:\n",
      "# (1) How many stations are there per country?\n",
      "result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "# Put numbers on top of bars\n",
      "for index, data in enumerate(result.values):\n",
      "    plt.text(x=index, y =data+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
      "plt.title('Number of stations per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/72:\n",
      "# (2) Average years of data per country (to_date - from_date) in years\n",
      "stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)\n",
      "result = stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "# Put numbers on top of bars \n",
      "for index, data in enumerate(result.values):\n",
      "    plt.text(x=index, y =data+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
      "plt.title('Average years of data per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/73:\n",
      "# (2) Average years of data per country (to_date - from_date) in years\n",
      "stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)\n",
      "result = stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)\n",
      "# Round to 2 decimals\n",
      "result = result.round(2)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "# Put numbers on top of bars \n",
      "for index, data in enumerate(result.values):\n",
      "    plt.text(x=index, y =data+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
      "plt.title('Average years of data per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/74:\n",
      "# (2) Average years of data per country (to_date - from_date) in years\n",
      "stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)\n",
      "result = stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)\n",
      "# Round to 2 decimals\n",
      "result = result.round(2)\n",
      "# Show as bar chart using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.bar(result.index, result.values)\n",
      "plt.title('Average years of data per country')\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/75:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/76:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "\n",
      "API.discover()\n",
      "63/77:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "request = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    resolution = 'daily',\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "print(request)\n",
      "63/78:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "request = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "print(request)\n",
      "63/79:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "request = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "df = request.values.all().df.dropna()\n",
      "63/80:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "request = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "df = request.values.query()\n",
      "63/81: print(df.head())\n",
      "63/82:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "request = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "data = request.values.query()\n",
      "63/83:\n",
      "for result in data:\n",
      "    print(result.df.dropna().head())\n",
      "63/84:\n",
      "for result in data:\n",
      "    print(result.df.head())\n",
      "63/85:\n",
      "for result in data:\n",
      "    print(result.df.head())\n",
      "63/86:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "request = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "print(request)\n",
      "#data = request.values.query()\n",
      "63/87:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "63/88:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = '2010-01-01'\n",
      "end_date = '2020-01-01'\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter = parameter,\n",
      "    start_date = start_date,\n",
      "    end_date = end_date,\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "63/89:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = pd.datetime(2010, 1, 1)\n",
      "end_date =  pd.datetime(2020, 1, 1)\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,\n",
      "    start_date=pd.datetime(1992, 1, 1),\n",
      "    end_date=pd.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=39.559952,\n",
      "     longitude=2.678001,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "63/90:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,\n",
      "    start_date=dt.datetime(1992, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=39.559952,\n",
      "     longitude=2.678001,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/91:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.WIND_SPEED, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(1992, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/92:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "# Amsterdam location\n",
      "LATITUDE = 52.370216\n",
      "LONGITUDE = 4.895168\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.WIND_SPEED, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(1992, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=LATITUDE,\n",
      "     longitude=LONGITUDE,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/93:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "# Amsterdam location\n",
      "LATITUDE = 52.370216\n",
      "LONGITUDE = 4.895168\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(1992, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=LATITUDE,\n",
      "     longitude=LONGITUDE,\n",
      "     distance=10\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/94:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "# Amsterdam location\n",
      "LATITUDE = 52.370216\n",
      "LONGITUDE = 4.895168\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(1992, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/95:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "# Amsterdam location\n",
      "LATITUDE = 52.370216\n",
      "LONGITUDE = 4.895168\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,\n",
      "    start_date=dt.datetime(1992, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/96:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "#API.discover()\n",
      "parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']\n",
      "start_date = dt.datetime(2010, 1, 1)\n",
      "end_date =  dt.datetime(2020, 1, 1)\n",
      "\n",
      "# Amsterdam location\n",
      "LATITUDE = 52.370216\n",
      "LONGITUDE = 4.895168\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_station_id('NLE00101920')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "64/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "directory = \"C:/Users/Nicky/Documents/School/Assignments/QMFI\"\n",
      "yc = pd.read_excel(f\"{directory}/YC.xls\", index_col='Date')\n",
      "# Group by month (timestamp) and take the average\n",
      "yc_monthly = yc.groupby(pd.Grouper(freq='M')).mean()\n",
      "64/2: print(yc)\n",
      "64/3:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "yc['3 M'].plot(figsize=(15,5), lw=1, color=\"blue\", label='3-Month Government Bond')\n",
      "yc['6 M'].plot( lw=0.5, color=\"lightblue\",label='6-Month Government Bond')\n",
      "yc['9 M'].plot( lw=0.5, color=\"darkgreen\",label='9-Month Government Bond')\n",
      "yc['1 Y'].plot( lw=0.5, color=\"deepskyblue\",label='1-Year Government Bond')\n",
      "yc['2 Y'].plot( lw=0.5, color=\"dodgerblue\",label='2-Year Government Bond')\n",
      "yc['3 Y'].plot( lw=0.5, color=\"steelblue\",label='3-Year Government Bond')\n",
      "yc['4 Y'].plot( lw=0.5, color=\"blue\",label='4-Year Government Bond')\n",
      "yc['5 Y'].plot( lw=0.5, color=\"mediumblue\",label='5-Year Government Bond')\n",
      "yc['6 Y'].plot( lw=0.5, color=\"slategrey\", label='6-Year Government Bond')\n",
      "yc['7 Y'].plot( lw=0.5, color=\"gray\",label='7-Year Government Bond')\n",
      "yc['8 Y'].plot( lw=0.5, color=\"red\", grid=True,label='8-Year Government Bond')\n",
      "yc['9 Y'].plot( lw=0.5, color=\"orange\", grid=True,label='9-Year Government Bond')\n",
      "yc['10 Y'].plot( lw=0.5, color=\"green\", grid=True,label='10-Year Government Bond')\n",
      "yc['15 Y'].plot( lw=0.5, color=\"cyan\", grid=True,label='15-Year Government Bond')\n",
      "yc['20 Y'].plot( lw=0.5, color=\"yellow\", grid=True,label='20-Year Government Bond')\n",
      "yc['25 Y'].plot( lw=0.5, color=\"pink\", grid=True,label='25-Year Government Bond')\n",
      "yc['30 Y'].plot( lw=0.5, color=\"black\", grid=True,label='30-Year Government Bond')\n",
      "plt.legend(bbox_to_anchor=(1, 1.02))\n",
      "plt.xlabel(\"Date\")\n",
      "plt.ylabel(\"Canadian Government Bond Rate\")\n",
      "plt.title(\"The Canadian Term Structure of Interest Rate\", fontweight='bold')\n",
      "64/4:\n",
      "import numpy as np\n",
      "from mpl_toolkits.mplot3d import axes3d\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as ticker\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "# Numpy.recarray\n",
      "ycn = yc.to_records()\n",
      "# type(ycn)\n",
      "# ycn\n",
      "64/5:\n",
      "# Maturity\n",
      "header = []\n",
      "for name in ycn.dtype.names[1:]:\n",
      "    maturity = float(name.split(\" \")[0])\n",
      "    if name.split(\" \")[1] == 'M':\n",
      "        maturity = maturity / 12\n",
      "    header.append(maturity)\n",
      "64/6:\n",
      "# We create three empty lists \n",
      "x_data = []; y_data = []; z_data = []\n",
      "for dt in ycn.Date:\n",
      "    dt_num = dates.date2num(dt)\n",
      "    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])\n",
      "# print ('x_data: ', x_data[1:5])\n",
      "64/7:\n",
      "for row in ycn:\n",
      "    y_data.append(header)\n",
      "    z_data.append(list(row.tolist()[1:]))\n",
      "# print ('y_data: ', y_data[1:5])\n",
      "# print ('z_data: ', z_data[1:5])\n",
      "64/8:\n",
      "x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')\n",
      "# x ==> Dates\n",
      "# y ==> Maturities\n",
      "# z ==> Yields\n",
      "# print ('x:', x) \n",
      "# print ('y: ', y)\n",
      "# print ('z: ', z)\n",
      "64/9:\n",
      "# %matplotlib inline\n",
      "# fig = plt.figure(figsize=(15, 10))\n",
      "# ax = fig.add_subplot(111, projection='3d')\n",
      "# z_percent = z*100\n",
      "# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))\n",
      "# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')\n",
      "# ax.set_ylabel('Maturity (\\u03C4)')\n",
      "# ax.set_zlabel('Yield (Percent)')\n",
      "# plt.savefig('my_pgf_plot.jpeg')\n",
      "\n",
      "# def format_date(x, pos=None):\n",
      "#      return dates.num2date(x).strftime('%Y')\n",
      "# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))\n",
      "# for tl in ax.w_xaxis.get_ticklabels():\n",
      "#     tl.set_ha('right')\n",
      "#     tl.set_rotation(40)\n",
      "# plt.show()\n",
      "64/10:\n",
      "# Stylized fact 1: The average yield curve over time is increasing and concave\n",
      "average_yc = yc.mean(axis=0) * 100\n",
      "average_yc.plot()\n",
      "64/11:\n",
      "# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, \n",
      "# humped, inverted humped, S-shapes)\n",
      "curve_1 = yc.iloc[1]*100\n",
      "curve_2 = yc.iloc[4174]*100 #2000\n",
      "curve_3 = yc.iloc[5600]*100\n",
      "curve_4 = yc.iloc[7850]*100\n",
      "curve_1.plot()\n",
      "curve_2.plot()\n",
      "curve_3.plot()\n",
      "curve_4.plot()\n",
      "64/12: curve_1.plot()\n",
      "64/13: curve_2.plot()\n",
      "64/14: curve_3.plot()\n",
      "64/15: curve_4.plot()\n",
      "64/16:\n",
      "# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)\n",
      "# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities\n",
      "first_autocorrelation = []\n",
      "tenth_autocorrelation = []\n",
      "twentieth_autocorrelation = []\n",
      "for maturity in yc:\n",
      "    first_autocorrelation.append(yc[maturity].autocorr())\n",
      "    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))\n",
      "    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))\n",
      "\n",
      "# autocorrelations = \n",
      "# s = yc['3 M']\n",
      "# s.autocorr()\n",
      "data = []\n",
      "for maturity in yc:\n",
      "    data.append(maturity)\n",
      "\n",
      "autocorrelations = pd.DataFrame(data, columns=['Maturity'])\n",
      "autocorrelations['First autocorrelation'] = first_autocorrelation \n",
      "autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation\n",
      "autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation\n",
      "autocorrelations.round(5)\n",
      "64/17:\n",
      "# Stylized fact 5: The short end of the yield curve is more volatile than the long end of the curve\n",
      "stdevs = yc.std()\n",
      "stdevs.round(5)\n",
      "64/18:\n",
      "# Stylized fact 6: Yields for different maturities have high cross-correlations\n",
      "yc.corr().round(3)\n",
      "64/19:\n",
      "yc = yc_monthly\n",
      "yc\n",
      "64/20: header\n",
      "64/21:\n",
      "import math\n",
      "def find_lambda(lmbd):\n",
      "    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)\n",
      "    return  lmbd_val\n",
      "    \n",
      "lmbd_range = np.arange(-0.1,2,0.0001)\n",
      "lmbd_values = []\n",
      "for i in lmbd_range:\n",
      "    lmbd_values.append(find_lambda(i))\n",
      "plt.plot(lmbd_range,lmbd_values)\n",
      "64/22:\n",
      "lambda_max = lmbd_range[np.argmax(lmbd_values)]\n",
      "print(lambda_max)\n",
      "64/23:\n",
      "# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max\n",
      "(rows, cols)= np.shape(yc)\n",
      "B = []\n",
      "for maturity in header:\n",
      "    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])\n",
      "    \n",
      "B = np.array(B)\n",
      "# Slopes is second column of B\n",
      "slopes = B[:,1]\n",
      "curvatures = B[:,2]\n",
      "print(B)\n",
      "print(np.shape(B))\n",
      "64/24:\n",
      "(rows, cols) = np.shape(yc)\n",
      "beta = np.zeros((rows, 3))\n",
      "\n",
      "for i in range(1, rows): \n",
      "    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))\n",
      "\n",
      "print(beta)\n",
      "64/25:\n",
      "np.shape(beta)\n",
      "print(beta[:,0])\n",
      "64/26:\n",
      "# Create the forecast functions\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Nelson-Siegel AR forecast with comments\n",
      "def ns_ar_forecast(beta, step_size):\n",
      "    lagged_beta = np.roll(beta, step_size, axis=0)\n",
      "    temp_beta = beta[step_size:,:]\n",
      "    lagged_beta = lagged_beta[step_size:,:]\n",
      "    fc_beta = [None, None, None]\n",
      "    for f in range(3):\n",
      "        # Create the X matrix (iota, lagged_beta)\n",
      "        x = lagged_beta[:,f]\n",
      "        # Create the Y matrix (temp_beta)\n",
      "        y = temp_beta[:,f]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_beta[f] = const + gamma*beta[-1,f]\n",
      "        #print(regression.summary())\n",
      "\n",
      "    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures\n",
      "    return fc_y\n",
      "\n",
      "def ar_yield_level_forecast(yield_data, step_size):\n",
      "    lagged = np.roll(yield_data, step_size, axis=0)\n",
      "    temp = yield_data[step_size:,:]\n",
      "    lagged = lagged[step_size:,:]\n",
      "    \n",
      "    # Create empty list of length len(header)\n",
      "    fc_y = [None] * len(header)\n",
      "    for idx, maturity in enumerate(header):\n",
      "        y = temp[idx]\n",
      "        x = lagged[idx]\n",
      "        regression = sm.OLS(y, sm.add_constant(x)).fit()\n",
      "        const = regression.params[0]\n",
      "        gamma = regression.params[1]\n",
      "        fc_y[idx] = const + gamma*yield_data[-1,idx]\n",
      "        \n",
      "    return fc_y\n",
      "\n",
      "yields = yc\n",
      "# Transform to np array\n",
      "yield_data = np.array(yields)\n",
      "print(yields)\n",
      "\n",
      "STEP_SIZE = 1\n",
      "WINDOW_SIZE = 12*5\n",
      "\n",
      "forecast_yields = {\n",
      "    'index' : [],\n",
      "    'random_walk' : [],\n",
      "    'nelson_siegel' : [],\n",
      "    'ar_yield_levels' : [],\n",
      "    'realized' : []\n",
      "}\n",
      "\n",
      "for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):\n",
      "    subset = yield_data[i:i+WINDOW_SIZE]\n",
      "    subset_beta = beta[i:i+WINDOW_SIZE]\n",
      "    print(f\"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}\")\n",
      "    # Random walk\n",
      "    random_walk = np.array(subset[-1])\n",
      "    # Nelson-Siegel\n",
      "    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)\n",
      "    # Ar yield levels\n",
      "    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)\n",
      "    # Realized\n",
      "    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])\n",
      "    \n",
      "    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]\n",
      "    # Append to dictionary\n",
      "    forecast_yields['random_walk'].append(random_walk)\n",
      "    forecast_yields['nelson_siegel'].append(nelson_siegel)\n",
      "    forecast_yields['ar_yield_levels'].append(ar_yield_levels)\n",
      "\n",
      "    forecast_yields['realized'].append(realized)\n",
      "    forecast_yields['index'].append(index)\n",
      "64/27:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        error_measures[maturity] = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(pd.DataFrame(errors[key]).T)\n",
      "    print(\"\")\n",
      "64/28:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "64/29:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures\n",
      "def plot_error_measures(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Error'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"Error\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error\", \"Value\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecast error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_error_measures(errors)\n",
      "64/30:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures\n",
      "def plot_error_measures(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Error'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"Error\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error\", \"Value\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecast error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_error_measures(errors)\n",
      "64/31:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures\n",
      "def plot_error_measures(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'forecast'], var_name='maturity', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Error'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"forecast\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"Error\", y=\"error\", hue=\"maturity\")\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error\", \"Value\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecast error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_error_measures(errors)\n",
      "64/32:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures\n",
      "def plot_error_measures(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'forecast'], var_name='maturity', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Error'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"forecast\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"Error\", y=\"error\", hue=\"maturity\")\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error\", \"Value\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecast error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_error_measures(errors)\n",
      "64/33:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures\n",
      "def plot_error_measures(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Error'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"forecast\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"Error\", y=\"error\", hue=\"maturity\")\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Maturity\", \"Value\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecast error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_error_measures(errors)\n",
      "64/34:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"error_measure\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures(errors)\n",
      "64/35:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"error_measure\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/36:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/37:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Free y axis\n",
      "    g.set(ylim=(None, None))\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/38:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, row=\"error_measure\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/39:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=1, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/40:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=1, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=(None, None))\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/41:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=1, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/42:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=1, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/43:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=1, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", dodge=False)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    # Increase bar width\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/44:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    # Increase bar width\n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/45:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    # Slightly increasse bar widt\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=3, edgecolor=\".2\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    \n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/46:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    # Slightly increasse bar widt\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=1, edgecolor=\".5\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    \n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/47:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    # Slightly increasse bar widt\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=1, edgecolor=\".2\", width=.5)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    \n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/48:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    # Slightly increasse bar widt\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8,  edgecolor=\".2\", width=.5)\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    \n",
      "    \n",
      "    \n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/49:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=1, edgecolor=\".2\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/50:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=1, edgecolor=\".2\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Increase margin width between x-axis labels\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90)\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=1, wspace=0.3)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/51:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=1, edgecolor=\".2\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Increase margin width between x-axis labels\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90, fontsize=8)\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=.9)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "64/52:\n",
      "# Do analysis on the forecasts\n",
      "import seaborn as sns\n",
      "\n",
      "# Convert to dataframe (for every maturity)\n",
      "maturity_forecasts = {}\n",
      "residual_forecasts = {}\n",
      "\n",
      "for idx, maturity in enumerate(yields.columns):\n",
      "    # Select the idx element from every forecast\n",
      "    random_walk = np.array(forecast_yields['random_walk'])[:,idx]\n",
      "    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]\n",
      "    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]\n",
      "    realized = np.array(forecast_yields['realized'])[:,idx]\n",
      "    tmp = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : random_walk,\n",
      "        'nelson_siegel' : nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels,\n",
      "        'realized' : realized\n",
      "    }\n",
      "    tmp_residual = {\n",
      "        'index' : np.array(forecast_yields['index']),\n",
      "        'random_walk' : realized - random_walk,\n",
      "        'nelson_siegel' : realized - nelson_siegel,\n",
      "        'ar_yield_levels' : ar_yield_levels - realized\n",
      "\n",
      "    }\n",
      "    \n",
      "    maturity_forecasts[maturity] = pd.DataFrame(tmp)\n",
      "    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)\n",
      "\n",
      "\n",
      "# Print all the maturities\n",
      "print(maturity_forecasts.keys())\n",
      "print(maturity_forecasts['30 Y'])\n",
      "\n",
      "# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)\n",
      "def plot_forecasts(maturity_forecasts, maturity):\n",
      "    df = maturity_forecasts[maturity]\n",
      "    df = df.set_index('index')\n",
      "    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "    df.plot(figsize=(15,5), title=f\"Yield curve forecasts for {maturity} maturity\")\n",
      "    \n",
      "# Plot within a facet grid\n",
      "def plot_forecasts_facet(maturity_forecasts):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        tmp = maturity_forecasts[maturity]\n",
      "        tmp = tmp.set_index('index')\n",
      "        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'Date'})\n",
      "    \n",
      "    g = sns.FacetGrid(df, col=\"maturity\", col_wrap=4, height=3, aspect=1.5)\n",
      "    g.map_dataframe(sns.lineplot, x=\"Date\", y=\"yield\", hue=\"forecast\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    g.set_axis_labels(\"Date\", \"Yield\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Yield curve forecasts\")\n",
      "    g.fig.subplots_adjust(top=0.9)\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '1 Y')\n",
      "\n",
      "plot_forecasts(maturity_forecasts, '25 Y')\n",
      "\n",
      "plot_forecasts(residual_forecasts, '1 Y')\n",
      "\n",
      "#plot_forecasts_facet(maturity_forecasts)\n",
      "\n",
      "#plot_forecasts_facet(residual_forecasts)\n",
      "\n",
      "# Compute all error measures\n",
      "import sklearn.metrics as metrics\n",
      "\n",
      "def compute_error_measures(maturity_forecasts):\n",
      "    error_measures = {}\n",
      "    for maturity in maturity_forecasts.keys():\n",
      "        df = maturity_forecasts[maturity]\n",
      "        # Compute error measures\n",
      "        errors_dict = {\n",
      "            'random_walk' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])\n",
      "            },\n",
      "            'nelson_siegel' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])\n",
      "            },\n",
      "            'ar' : {\n",
      "                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),\n",
      "                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),\n",
      "                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),\n",
      "                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])\n",
      "            }\n",
      "        }\n",
      "        # Convert to dataframe\n",
      "        error_df = pd.DataFrame(errors_dict)\n",
      "        error_measures[maturity]  = error_df\n",
      "    return error_measures\n",
      "\n",
      "# Show in tabular format\n",
      "errors = compute_error_measures(maturity_forecasts)\n",
      "for key in errors.keys():\n",
      "    print(f\"Error measures for {key} maturity\")\n",
      "    print(errors[key].T)\n",
      "    print(\"\")\n",
      "\n",
      "# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)\n",
      "def plot_error_measures_facet(error_measures):\n",
      "    df = pd.DataFrame()\n",
      "    for maturity in error_measures.keys():\n",
      "        tmp = error_measures[maturity]\n",
      "        tmp['maturity'] = maturity\n",
      "        # pivot longer\n",
      "        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')\n",
      "        df = df.append(tmp)\n",
      "    df = df.reset_index()\n",
      "    df = df.rename(columns={'index': 'error_measure'})\n",
      "    g = sns.FacetGrid(df, col=\"error_measure\", col_wrap=2, height=3, aspect=1.5, sharey = False)\n",
      "    g.map_dataframe(sns.barplot, x=\"maturity\", y=\"error\", hue=\"forecast\", palette=\"Set2\", ci=None, alpha=0.8, linewidth=1, edgecolor=\".2\")\n",
      "    g.set_titles(col_template=\"{col_name}\")\n",
      "    # Increase margin width between x-axis labels\n",
      "    # Disable shared y axis across facets\n",
      "    g.set(ylim=None)\n",
      "    # Increase size between x axis labels\n",
      "    g.set_xticklabels(rotation=90, fontsize=8)\n",
      "    g.set_axis_labels(\"Error measure\", \"Error\")\n",
      "    g.add_legend()\n",
      "    g.fig.suptitle(\"Error measures\")\n",
      "    g.fig.subplots_adjust(top=.9)\n",
      "    # Increase plot size\n",
      "    g.fig.set_size_inches(15, 5)\n",
      "\n",
      "\n",
      "plot_error_measures_facet(errors)\n",
      "63/97:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('AMSTERDAM')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/98:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('BERLIN')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/99:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=39.559952,\n",
      "     longitude=2.678001,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/100:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=39.559952,\n",
      "     longitude=2.678001,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/101:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('AMSTERDAM')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/102:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('BERLIN')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/103:\n",
      "# Select a station in Amsterdam and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_station_id('GME00127990')\n",
      "\n",
      "# CHeck which parameters are available for stations in Berlin\n",
      "\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/104:\n",
      "# Select a station in London and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_sql(\"name = '%London%'\")\n",
      "\n",
      "# CHeck which parameters are available for stations in Berlin\n",
      "\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/105:\n",
      "# Select a station in London and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('London')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/106:\n",
      "# Select a station in London and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=39.559952,\n",
      "     longitude=2.678001,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/107:\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['value'])\n",
      "plt.title('Temperature in London')\n",
      "plt.show()\n",
      "63/108:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_distance(\n",
      "     latitude=39.559952,\n",
      "     longitude=2.678001,\n",
      "     distance=30\n",
      ")\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/109:\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['value'])\n",
      "plt.title('Temperature in London')\n",
      "plt.show()\n",
      "63/110:\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['value'])\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/111:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,\n",
      "    start_date=dt.datetime(2010, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('AMSTERDAM')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/112:\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['value'])\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "63/113:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('AMSTERDAM')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/114:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_200,\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('AMSTERDAM')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/115:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.WIND_SPEED,\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('AMSTERDAM')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/116:\n",
      "# Get historical temperature data for Dresden.\n",
      "from wetterdienst import Settings\n",
      "from wetterdienst.provider.dwd.observation import DwdObservationRequest\n",
      "\n",
      "request = DwdObservationRequest(\n",
      "   parameter=[\"tavg\"],\n",
      "   resolution=\"daily\",\n",
      "   start_date=\"1990-01-01\",  # if not given timezone defaulted to UTC\n",
      "   end_date=\"2020-01-01\",  # if not given timezone defaulted to UTC\n",
      ").filter_by_station_id(station_id=(1048))\n",
      "\n",
      "df = request.values.all().df\n",
      "63/117:\n",
      "# Get historical temperature data for Dresden.\n",
      "from wetterdienst import Settings\n",
      "from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter\n",
      "\n",
      "\n",
      "request = DwdObservationRequest(\n",
      "   parameter=[\"climate_summary\"],\n",
      "   resolution=\"daily\",\n",
      "   start_date=\"1990-01-01\",  # if not given timezone defaulted to UTC\n",
      "   end_date=\"2020-01-01\",  # if not given timezone defaulted to UTC\n",
      ").filter_by_station_id(station_id=(1048))\n",
      "\n",
      "df = request.values.all().df\n",
      "63/118: print(df.head())\n",
      "63/119:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "63/120:\n",
      "# Get historical temperature data for Dresden.\n",
      "from wetterdienst import Settings\n",
      "from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter\n",
      "\n",
      "\n",
      "request = DwdObservationRequest(\n",
      "   parameter=[\"climate_summary\"],\n",
      "   resolution=\"daily\",\n",
      "   start_date=\"2000-01-01\",  # if not given timezone defaulted to UTC\n",
      "   end_date=\"2020-01-01\",  # if not given timezone defaulted to UTC\n",
      ").filter_by_station_id(station_id=(4411))\n",
      "\n",
      "df = request.values.query().df\n",
      "63/121:\n",
      "# Get historical temperature data for Dresden.\n",
      "from wetterdienst import Settings\n",
      "from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter\n",
      "\n",
      "\n",
      "request = DwdObservationRequest(\n",
      "   parameter=[\"climate_summary\"],\n",
      "   resolution=\"daily\",\n",
      "   start_date=\"2000-01-01\",  # if not given timezone defaulted to UTC\n",
      "   end_date=\"2020-01-01\",  # if not given timezone defaulted to UTC\n",
      ").filter_by_station_id(station_id=(4411))\n",
      "\n",
      "df = get_data_from_stations_request(request)\n",
      "63/122:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "63/123:\n",
      "print(df)\n",
      "print(df['parameter'].unique())\n",
      "63/124:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "63/125:\n",
      "print(df.head())\n",
      "\n",
      "# Plot the temperature data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df.index, df['TT_TU'])\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/126:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df.index, df['TT_TU'])\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/127:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df.index, df['temperature_air_mean_200'])\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/128:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df.index, df['temperature_air_mean_200'])\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/129:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'])\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/130:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'].values, df['value'].values)\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/131:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temperature = df[df['parameter'] == 'temperature_air_mean_200']\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(temperature['date'].values, temperature['value'].values)\n",
      "plt.title('Temperature in Dresden')\n",
      "plt.show()\n",
      "63/132:\n",
      "print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_200', 'temperature_air_200_max', 'temperature_air_200_min'])]\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/133:\n",
      "#print(df.head())\n",
      "#print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_200', 'temperature_air_200_max', 'temperature_air_200_min'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/134:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_200', 'temperature_air_200_max', 'temperature_air_200_min'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/135:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/136:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from wetterdienst import Wetterdienst\n",
      "import wetterdienst as wd\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "63/137:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season.index.month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "63/138:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season['date'].month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "63/139:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season['date'].dt.month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "63/140:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():\n",
      "    plt.hist(temps_season[temps_season['season'] == season]['value'], alpha=0.5, label=season)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/141:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():\n",
      "    plt.hist(temps_season[temps_season['season'] == season, temps_season['parameter'] == 'temperature_air_mean_200']['value'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/142:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():\n",
      "    plt.hist(temps_season[temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200']['value'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/143:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():\n",
      "    plt.hist(temps_season[temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200']['value'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/144:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():\n",
      "    mask = temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200']\n",
      "    plt.hist(temps_season[mask]['value'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/145:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():\n",
      "    mask = temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200'\n",
      "    plt.hist(temps_season[mask]['value'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/146:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value')\n",
      "63/147:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season['date'].dt.month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "63/148:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value')\n",
      "temps\n",
      "63/149:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "temps\n",
      "63/150:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "63/151:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season['date'].dt.month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "63/152:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season[mask]['value'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/153:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season['temperature_air_mean_200'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/154:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season['temperature_air_mean_200'], alpha=0.5, label=season, bins=100)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/155:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season['temperature_air_mean_200'], alpha=0.5, label=season, bins=20)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/156:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/157:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5, bins = 100, density = True)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/158:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "# Convert Kelvin to Celsius\n",
      "temps['temperature_air_mean_200'] = temps['temperature_air_mean_200'] - 273.15\n",
      "temps['temperature_air_max_200'] = temps['temperature_air_max_200'] - 273.15\n",
      "temps['temperature_air_min_200'] = temps['temperature_air_min_200'] - 273.15\n",
      "63/159:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season['date'].dt.month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "63/160:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5, bins = 100, density = True)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/161:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps = temps.set_index('date')\n",
      "temps\n",
      "63/162:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps = temps.set_index('date')\n",
      "temps\n",
      "63/163:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "# Convert Kelvin to Celsius\n",
      "temps['temperature_air_mean_200'] = temps['temperature_air_mean_200'] - 273.15\n",
      "temps['temperature_air_max_200'] = temps['temperature_air_max_200'] - 273.15\n",
      "temps['temperature_air_min_200'] = temps['temperature_air_min_200'] - 273.15\n",
      "63/164:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition\n",
      "63/165:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "temps_decomposition['T'].rolling(window = 365*10).mean().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "temps_decomposition['T'].rolling(window = 365*10).var().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling variance over annual periods\");\n",
      "63/166:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "temps_decomposition['T'].rolling(window = 365*10).mean().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "temps_decomposition['T'].rolling(window = 365*10).var().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling variance over annual periods\");\n",
      "63/167:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling variance over annual periods\");\n",
      "63/168:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "#temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling variance over annual periods\");\n",
      "63/169:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var().plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling variance over annual periods\")\n",
      "63/170:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var(), label='Rolling variance')\n",
      "plt.title('Rolling mean and variance')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/171:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean(), label='Rolling mean')\n",
      "plt.subplot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var(), label='Rolling variance')\n",
      "plt.title('Rolling mean and variance')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/172:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).std(), label = 'Rolling std')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean and standard deviation')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).std(), label = 'Rolling std')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean and standard deviation')\n",
      "plt.show()\n",
      "63/173:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).std(), label = 'Rolling std')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean and standard deviation')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).std(), label = 'Rolling std')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Differenced Temperature with rolling mean and standard deviation')\n",
      "plt.show()\n",
      "63/174:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling std')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean and standard deviation')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).var(), label = 'Rolling std')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Differenced Temperature with rolling mean and standard deviation')\n",
      "plt.show()\n",
      "63/175:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean and variance')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).var(), label = 'Rolling variance')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Differenced Temperature with rolling mean and variance')\n",
      "plt.show()\n",
      "63/176:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Rolling Variance')\n",
      "plt.show()\n",
      "63/177:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=7).mean(), label='Rolling mean (1W)')\n",
      "\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance')\n",
      "plt.legend(loc='best')\n",
      "plt.title('Rolling Variance')\n",
      "plt.show()\n",
      "63/178:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "plt.legend(loc='best')\n",
      "plt.title('Rolling Variance')\n",
      "plt.show()\n",
      "63/179:\n",
      "decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')\n",
      "trend = decompose_result.trend\n",
      "seasonal = decompose_result.seasonal\n",
      "residual = decompose_result.resid\n",
      "### Visualise All Data\n",
      "decompose_result.plot()\n",
      "plt.show()\n",
      "### Visualise 10 years\n",
      "years_examine = 365*10\n",
      "fig, axs = plt.subplots(3, figsize=(8,6))\n",
      "fig.suptitle('Removed Trend and Seasonality')\n",
      "axs[0].plot(trend[-years_examine:])\n",
      "axs[1].plot(seasonal[-years_examine:])\n",
      "axs[1].set_ylim([-10,10])\n",
      "axs[2].plot(residual[-years_examine:])\n",
      "axs[2].set_ylim([-15,15])\n",
      "63/180:\n",
      "decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')\n",
      "trend = decompose_result.trend\n",
      "seasonal = decompose_result.seasonal\n",
      "residual = decompose_result.resid\n",
      "### Visualise All Data\n",
      "decompose_result.plot()\n",
      "plt.show()\n",
      "### Visualise 10 years\n",
      "years_examine = 365*10\n",
      "fig, axs = plt.subplots(3, figsize=(8,6))\n",
      "fig.suptitle('Removed Trend and Seasonality')\n",
      "axs[0].plot(trend[-years_examine:])\n",
      "axs[1].plot(seasonal[-years_examine:])\n",
      "axs[1].set_ylim([-10,10])\n",
      "axs[2].plot(residual[-years_examine:])\n",
      "63/181:\n",
      "decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')\n",
      "trend = decompose_result.trend\n",
      "seasonal = decompose_result.seasonal\n",
      "residual = decompose_result.resid\n",
      "### Visualise All Data\n",
      "decompose_result.plot()\n",
      "plt.show()\n",
      "63/182:\n",
      "decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')\n",
      "trend = decompose_result.trend\n",
      "seasonal = decompose_result.seasonal\n",
      "residual = decompose_result.resid\n",
      "### Visualise All Data\n",
      "decompose_result.plot()\n",
      "### Visualise 10 years\n",
      "years_examine = 365*10\n",
      "fig, axs = plt.subplots(3, figsize=(8,6))\n",
      "fig.suptitle('Removed Trend and Seasonality')\n",
      "axs[0].plot(trend[-years_examine:])\n",
      "axs[1].plot(seasonal[-years_examine:])\n",
      "axs[1].set_ylim([-10,10])\n",
      "axs[2].plot(residual[-years_examine:])\n",
      "63/183:\n",
      "dftest = adfuller(residual, autolag = 'AIC')\n",
      "print(\"1. ADF : \",dftest[0])\n",
      "print(\"2. P-Value : \", dftest[1])\n",
      "print(\"3. Num Of Lags : \", dftest[2])\n",
      "print(\"4. Num Of Observations Used For ADF Regression and Critical Values Calculation :\", dftest[3])\n",
      "print(\"5. Critical Values :\")\n",
      "for key, val in dftest[4].items():\n",
      "    print(\"\\t\",key, \": \", val)\n",
      "63/184:\n",
      "plot_acf(residual, lags=100)\n",
      "plt.show()\n",
      "63/185:\n",
      "plot_acf(residual, lags=30)\n",
      "plt.show()\n",
      "63/186:\n",
      "plot_pacf(residual, lags=40)\n",
      "plt.show()\n",
      "63/187:\n",
      "residuals = residual.copy(deep=True)\n",
      "residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')\n",
      "mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)\n",
      "aic = []\n",
      "for key, val in mod.aic.items():\n",
      "    if key != 0:\n",
      "        aic.append((key[-1], val))\n",
      "aic.sort()    \n",
      "x,y = [x for x,y in aic],[y for x,y in aic]\n",
      "plt.scatter(x, y)\n",
      "plt.plot([0,40],[y[15],y[15]], 'tab:orange')\n",
      "plt.text(3,y[15]+0.002, '{0}'.format(round(y[15],3)),color='tab:orange')\n",
      "plt.plot([0,40],[y[20],y[20]], 'k--')\n",
      "plt.text(3,y[20]-0.004, '{0}'.format(round(y[20],3)))\n",
      "plt.title(\"AIC Criterion\")\n",
      "plt.xlabel(\"Lags in AR Model\")\n",
      "plt.ylabel(\"AIC\")\n",
      "plt.show()\n",
      "63/188:\n",
      "residuals = residual.copy(deep=True)\n",
      "residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')\n",
      "mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)\n",
      "aic = []\n",
      "for key, val in mod.aic.items():\n",
      "    if key != 0:\n",
      "        aic.append((key[-1], val))\n",
      "aic.sort()    \n",
      "x,y = [x for x,y in aic],[y for x,y in aic]\n",
      "plt.scatter(x, y)\n",
      "plt.plot([0,40],[y[15],y[15]], 'tab:orange')\n",
      "plt.text(3,y[15]+0.002, '{0}'.format(round(y[15],3)),color='tab:orange')\n",
      "plt.plot([0,40],[y[20],y[20]], 'k--')\n",
      "plt.text(3,y[20]-0.004, '{0}'.format(round(y[20],3)))\n",
      "plt.title(\"AIC Criterion\")\n",
      "plt.xlabel(\"Lags in AR Model\")\n",
      "plt.ylabel(\"AIC\")\n",
      "plt.show()\n",
      "\n",
      "# Look at the AIC criterion and choose the lag with the lowest AIC\n",
      "print(aic)\n",
      "63/189:\n",
      "residuals = residual.copy(deep=True)\n",
      "residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')\n",
      "mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)\n",
      "aic = []\n",
      "for key, val in mod.aic.items():\n",
      "    if key != 0:\n",
      "        aic.append((key[-1], val))\n",
      "aic.sort()    \n",
      "x,y = [x for x,y in aic],[y for x,y in aic]\n",
      "plt.scatter(x, y)\n",
      "plt.title(\"AIC Criterion\")\n",
      "plt.xlabel(\"Lags in AR Model\")\n",
      "plt.ylabel(\"AIC\")\n",
      "plt.show()\n",
      "\n",
      "# Look at the AIC criterion and choose the lag with the lowest AIC\n",
      "print(aic)\n",
      "# Choose the lag with the lowest AIC, get the minimum AIC and the corresponding lag\n",
      "63/190:\n",
      "residuals = residual.copy(deep=True)\n",
      "residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')\n",
      "mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)\n",
      "aic = []\n",
      "for key, val in mod.aic.items():\n",
      "    if key != 0:\n",
      "        aic.append((key[-1], val))\n",
      "aic.sort()    \n",
      "x,y = [x for x,y in aic],[y for x,y in aic]\n",
      "plt.scatter(x, y)\n",
      "plt.title(\"AIC Criterion\")\n",
      "plt.xlabel(\"Lags in AR Model\")\n",
      "plt.ylabel(\"AIC\")\n",
      "plt.show()\n",
      "\n",
      "# Look at the AIC criterion and choose the lag with the lowest AIC\n",
      "print(aic)\n",
      "# Choose the lag with the lowest AIC, get the minimum AIC and the corresponding lag\n",
      "min_aic = min(aic, key=lambda x: x[1])\n",
      "print(min_aic)\n",
      "63/191:\n",
      "model = AutoReg(residuals, lags=4, old_names=True,trend='n')\n",
      "model_fit  = model.fit()\n",
      "coef = model_fit.params\n",
      "res = model_fit.resid\n",
      "res.index = res.index.to_timestamp()\n",
      "print(model_fit.summary())\n",
      "fig, axs = plt.subplots(2,2, figsize=(8,6))\n",
      "fig.suptitle('Residuals after AR(4) model')\n",
      "axs[0,0].plot(res)\n",
      "axs[1,0].plot(res[-200:])\n",
      "plot_acf(res, lags=20, ax=axs[0,1])\n",
      "plot_pacf(res, lags=20, ax=axs[1,1])\n",
      "plt.show()\n",
      "from scipy.stats import norm\n",
      "qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')\n",
      "plt.show()\n",
      "63/192:\n",
      "model = AutoReg(residuals, lags=4, old_names=True,trend='n')\n",
      "model_fit  = model.fit()\n",
      "coef = model_fit.params\n",
      "res = model_fit.resid\n",
      "res.index = res.index.to_timestamp()\n",
      "print(model_fit.summary())\n",
      "fig, axs = plt.subplots(2,2, figsize=(8,6))\n",
      "fig.suptitle('Residuals after AR(4) model')\n",
      "axs[0,0].plot(res)\n",
      "axs[1,0].plot(res[-200:])\n",
      "plot_acf(res, lags=20, ax=axs[0,1])\n",
      "plot_pacf(res, lags=20, ax=axs[1,1])\n",
      "plt.show()\n",
      "from scipy.stats import norm\n",
      "qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')\n",
      "plt.show()\n",
      "63/193:\n",
      "model = AutoReg(residuals, lags=4, old_names=True,trend='n')\n",
      "model_fit  = model.fit()\n",
      "coef = model_fit.params\n",
      "res = model_fit.resid\n",
      "res.index = res.index.to_timestamp()\n",
      "print(model_fit.summary())\n",
      "fig, axs = plt.subplots(2,2, figsize=(8,6))\n",
      "fig.suptitle('Residuals after AR(4) model')\n",
      "# Show residuals\n",
      "axs[0,0].plot(res)\n",
      "plot_acf(res, lags=20, ax=axs[0,1])\n",
      "plot_pacf(res, lags=20, ax=axs[1,1])\n",
      "plt.show()\n",
      "from scipy.stats import norm\n",
      "qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')\n",
      "plt.show()\n",
      "63/194:\n",
      "model = AutoReg(residuals, lags=4, old_names=True,trend='n')\n",
      "model_fit  = model.fit()\n",
      "coef = model_fit.params\n",
      "res = model_fit.resid\n",
      "res.index = res.index.to_timestamp()\n",
      "print(model_fit.summary())\n",
      "fig, axs = plt.subplots(2,2, figsize=(8,6))\n",
      "fig.suptitle('Residuals after AR(4) model')\n",
      "# Show residuals\n",
      "axs[0,0].plot(res) \n",
      "axs[0,0].set_title('Residuals')\n",
      "# Show residuals histogram\n",
      "axs[0,1].hist(res, bins=100)\n",
      "plot_acf(res, lags=20, ax=axs[0,1])\n",
      "plot_pacf(res, lags=20, ax=axs[1,1])\n",
      "plt.show()\n",
      "from scipy.stats import norm\n",
      "qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')\n",
      "plt.show()\n",
      "63/195:\n",
      "model = AutoReg(residuals, lags=4, old_names=True,trend='n')\n",
      "model_fit  = model.fit()\n",
      "coef = model_fit.params\n",
      "res = model_fit.resid\n",
      "res.index = res.index.to_timestamp()\n",
      "print(model_fit.summary())\n",
      "fig, axs = plt.subplots(2,2, figsize=(8,6))\n",
      "fig.suptitle('Residuals after AR(4) model')\n",
      "# Show residuals\n",
      "axs[0,0].plot(res) \n",
      "axs[0,0].set_title('Residuals')\n",
      "# Show residuals histogram\n",
      "axs[1,0].hist(res, bins=100)\n",
      "plot_acf(res, lags=20, ax=axs[0,1])\n",
      "plot_pacf(res, lags=20, ax=axs[1,1])\n",
      "plt.show()\n",
      "from scipy.stats import norm\n",
      "qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')\n",
      "plt.show()\n",
      "63/196:\n",
      "model = AutoReg(residuals, lags=4, old_names=True,trend='n')\n",
      "model_fit  = model.fit()\n",
      "coef = model_fit.params\n",
      "res = model_fit.resid\n",
      "res.index = res.index.to_timestamp()\n",
      "print(model_fit.summary())\n",
      "fig, axs = plt.subplots(2,2, figsize=(8,6))\n",
      "fig.suptitle('Residuals after AR(4) model')\n",
      "# Show residuals (date vs residuals)\n",
      "axs[0,0].plot(res , label='Residuals')\n",
      "axs[0,0].set_title('Residuals')\n",
      "# Show residuals histogram\n",
      "axs[1,0].hist(res, bins=100)\n",
      "plot_acf(res, lags=20, ax=axs[0,1])\n",
      "plot_pacf(res, lags=20, ax=axs[1,1])\n",
      "plt.show()\n",
      "from scipy.stats import norm\n",
      "qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')\n",
      "plt.show()\n",
      "63/197:\n",
      "from scipy import signal\n",
      "\n",
      "def apply_convolution(x, window):\n",
      "    conv = np.repeat([0., 1., 0.], window)\n",
      "    filtered = signal.convolve(x, conv, mode='same') / window\n",
      "    return filtered\n",
      "denoised = temps.apply(lambda x: apply_convolution(x, 90))\n",
      "denoised['T'][-lookback:-100].plot(figsize=(12,6))\n",
      "plt.ylabel('Temperature (deg C)')\n",
      "plt.show()\n",
      "63/198:\n",
      "from scipy import signal\n",
      "lookback = 365*5\n",
      "def apply_convolution(x, window):\n",
      "    conv = np.repeat([0., 1., 0.], window)\n",
      "    filtered = signal.convolve(x, conv, mode='same') / window\n",
      "    return filtered\n",
      "denoised = temps.apply(lambda x: apply_convolution(x, 90))\n",
      "denoised['temperature_air_mean_200'][-lookback:-100].plot(figsize=(12,6))\n",
      "plt.ylabel('Temperature (deg C)')\n",
      "plt.show()\n",
      "63/199:\n",
      "from scipy import signal\n",
      "lookback = 365*5\n",
      "def apply_convolution(x, window):\n",
      "    conv = np.repeat([0., 1., 0.], window)\n",
      "    filtered = signal.convolve(x, conv, mode='same') / window\n",
      "    return filtered\n",
      "denoised = temps_decomposition.apply(lambda x: apply_convolution(x, 90))\n",
      "denoised['temperature_air_mean_200'][-lookback:-100].plot(figsize=(12,6))\n",
      "plt.ylabel('Temperature (deg C)')\n",
      "plt.show()\n",
      "63/200:\n",
      "denoised['MA'] = denoised['temperature_air_mean_200'].rolling(window = lookback).mean()\n",
      "plt.ylabel('Temperature (deg C)')\n",
      "denoised['MA'].plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "63/201:\n",
      "denoised['MA'] = denoised['temperature_air_mean_200'].rolling(window = lookback).mean()\n",
      "plt.ylabel('Temperature (deg C)')\n",
      "denoised['MA'].plot(figsize=(8,4), color=\"tab:red\", title=\"Rolling mean over annual periods\")\n",
      "\n",
      "denoised['S'] = denoised['temperature_air_mean_200'] - denoised['MA']\n",
      "denoised['S'][-lookback:-100].plot(figsize=(12,6))\n",
      "plt.ylabel('Temperature (deg C)')\n",
      "plt.show()\n",
      "63/202:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps['T'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/203:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/204:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.rename(columns  = {'temperature_air_mean_200':'T'})\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/205:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.rename({'temperature_air_mean_200':'T'})\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/206:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "# Rename columns\n",
      "temp_t = temp_t.rename({'temperature_air_mean_200':'T'})\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/207:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "# Rename columns\n",
      "temp_t = temp_t.rename(columns={'temperature_air_mean_200': 'T'})\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/208:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "# Rename columns\n",
      "print(temp_t.head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/209:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition[['temperature_air_mean_200']].copy(deep=True)\n",
      "# Rename columns\n",
      "print(temp_t.head())\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/210:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition[['temperature_air_mean_200']].copy(deep=True)\n",
      "# Rename columns\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "temp_t = temp_t.to_frame()\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/211:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/212:\n",
      "temp_t = temps['T'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365.25 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/213:\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365.25 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/214:\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365.25 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/215:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365.25 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/216:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365.25 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/217:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/218:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365.25 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/219:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/220:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/221:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/222:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model(x, params):\n",
      "    a,b,a1,b1 = params\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    theta = np.arctan(a1/b1)\n",
      "    alpha = np.sqrt( a1**2 + b1**2)   \n",
      "    print('Parameters:\\n     a {0:0.3}\\n     b {1:0.3}\\n alpha {2:0.3}\\n theta {3:0.3}'.format(a,b,alpha,theta))\n",
      "    y_pred = a + b*x + alpha*np.sin(omega*x + theta)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t.plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/223:\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "    \n",
      "temp_t['model'] = model(temp_t.index-first_ord, params_all)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "    \n",
      "temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )\n",
      "63/224:\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t['res'] = temp_t['T']-temp_t['model']\n",
      "temp_t['res'][-5000:].plot(figsize=(12,6))\n",
      "plt.show()\n",
      "fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
      "fig.suptitle('Residuals after de-trending and removing seasonality from the DAT')\n",
      "axs[0,0].plot(temp_t['res'])\n",
      "axs[1,0].plot(temp_t['res'][-2000:])\n",
      "plot_acf(temp_t['res'], lags=40, ax=axs[0,1])\n",
      "plot_pacf(temp_t['res'], lags=40, ax=axs[1,1])\n",
      "plt.show()\n",
      "63/225:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "63/226:\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t['res'] = temp_t['T']-temp_t['model']\n",
      "temp_t['res'][-5000:].plot(figsize=(12,6))\n",
      "plt.show()\n",
      "fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
      "fig.suptitle('Residuals after de-trending and removing seasonality from the DAT')\n",
      "axs[0,0].plot(temp_t['res'])\n",
      "axs[1,0].plot(temp_t['res'][-2000:])\n",
      "plot_acf(temp_t['res'], lags=40, ax=axs[0,1])\n",
      "plot_pacf(temp_t['res'], lags=40, ax=axs[1,1])\n",
      "plt.show()\n",
      "63/227:\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t['res'] = temp_t['T']-temp_t['model_fit']\n",
      "temp_t['res'][-5000:].plot(figsize=(12,6))\n",
      "plt.show()\n",
      "fig, axs = plt.subplots(2,2, figsize=(12,8))\n",
      "fig.suptitle('Residuals after de-trending and removing seasonality from the DAT')\n",
      "axs[0,0].plot(temp_t['res'])\n",
      "axs[1,0].plot(temp_t['res'][-2000:])\n",
      "plot_acf(temp_t['res'], lags=40, ax=axs[0,1])\n",
      "plot_pacf(temp_t['res'], lags=40, ax=axs[1,1])\n",
      "plt.show()\n",
      "63/228:\n",
      "import scipy.stats as stats\n",
      "stats.probplot(temp_t['res'], dist=\"norm\", plot=plt)\n",
      "plt.title(\"Probability Plot model\")\n",
      "plt.show()\n",
      "mu, std = norm.fit(temp_t['res'])\n",
      "z = (temp_t['res'] - mu)/std\n",
      "plt.hist(temp_t['res'], density=True, alpha=0.6, bins=100, label='Temp Error')\n",
      "xmin, xmax = plt.xlim()\n",
      "ymin, ymax = plt.ylim()\n",
      "x = np.linspace(xmin, xmax, 100)\n",
      "p = norm.pdf(x, mu, std)\n",
      "data = np.random.randn(100000)\n",
      "plt.plot(x, p, 'k', linewidth=2, label='Normal Dist')\n",
      "plt.plot([std*2,std*2],[0,ymax])\n",
      "print('P(Z > 2): {:0.3}% vs Normal Distibution: {:0.3}% '.format(len(z[z >= 2])/len(z)*100, (1-norm.cdf(2))*100))\n",
      "print('SKEW    : {:0.3}'.format(stats.skew(z)))\n",
      "print('KURTOSIS: {:0.3}'.format(stats.kurtosis(z)+3))\n",
      "plt.ylabel('Density')\n",
      "plt.xlabel('Temperature Error')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/229:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')\n",
      "lookup_table_fips.columns = ['fips', 'iso', 'fips_name']\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "\n",
      "# Get variables from inventory table\n",
      "inventory = pd.read_csv(\"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\", sep='\\s+', header=None)\n",
      "print(inventory.shape)\n",
      "\n",
      "\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/230:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')\n",
      "lookup_table_fips.columns = ['fips', 'iso', 'fips_name']\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/231:\n",
      "# Get variables from inventory table\n",
      "inventory = pd.read_csv(\"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\", sep='\\s+', header=None)\n",
      "print(inventory.shape)\n",
      "63/232:\n",
      "# Merge with stations\n",
      "print(inventory.columns)\n",
      "63/233:\n",
      "# Merge with stations\n",
      "inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']\n",
      "print(inventory)\n",
      "#stations = stations.merge(inventory, on='station_id', how='left')\n",
      "63/234:\n",
      "# Merge with stations\n",
      "inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']\n",
      "# Put variables in JSON format, grouped by station_id\n",
      "inventory_2 = inventory.groupby('station_id')['variable'].apply(list).reset_index()\n",
      "print(inventory_2)\n",
      "#stations = stations.merge(inventory, on='station_id', how='left')\n",
      "63/235:\n",
      "# Merge with stations\n",
      "inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']\n",
      "# Put variables in JSON format, grouped by station_id\n",
      "inventory = inventory.groupby('station_id')['variable'].apply(list).reset_index()\n",
      "stations = stations.merge(inventory, on='station_id', how='left')\n",
      "63/236:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')\n",
      "lookup_table_fips.columns = ['fips', 'iso', 'fips_name']\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "63/237:\n",
      "# Merge with stations\n",
      "inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']\n",
      "# Put variables in JSON format, grouped by station_id\n",
      "inventory = inventory.groupby('station_id')['variable'].apply(list).reset_index()\n",
      "stations = stations.merge(inventory, on='station_id', how='left')\n",
      "63/238:\n",
      "# Get variables from inventory table\n",
      "inventory = pd.read_csv(\"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\", sep='\\s+', header=None)\n",
      "print(inventory.shape)\n",
      "63/239:\n",
      "# Merge with stations\n",
      "inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']\n",
      "# Put variables in JSON format, grouped by station_id\n",
      "inventory = inventory.groupby('station_id')['variable'].apply(list).reset_index()\n",
      "stations = stations.merge(inventory, on='station_id', how='left')\n",
      "63/240:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/241:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10, -1]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/242:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        # Display columns 1:10 and last column\n",
      "        display_cols = [1:10] + [-1]\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/243:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        # Display columns 1:10 and last column\n",
      "        display_cols = [1:10,-1]\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/244:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        # Display columns 1:10 and last column\n",
      "        display_cols = [1:10,len(stations.columns)-1]\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/245:\n",
      "# Only keep the stations that are in Europe.\n",
      "stations_eu = stations[stations['region'].isin(['Europe'])]\n",
      "# Remove Russia\n",
      "stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']\n",
      "#print(stations_eu.head())\n",
      "\n",
      "def plot_stations(stations):\n",
      "    import folium\n",
      "    from folium.plugins import MarkerCluster\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    # Create a figure to place the map in \n",
      "    fig = folium.Figure(width=800, height=600)\n",
      "    # Create a map\n",
      "    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)\n",
      "    \n",
      "    # Create a marker cluster\n",
      "    marker_cluster = MarkerCluster().add_to(m)\n",
      "\n",
      "    # Add markers to the map\n",
      "    for index, row in stations.iterrows():\n",
      "        # Join (bold) columns with values to display label\n",
      "        # Display columns 1:10 and last column\n",
      "        display_cols = [len(stations.columns)-1] + list(range(1,10))\n",
      "        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])\n",
      "        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)\n",
      "\n",
      "    # Display the map\n",
      "    m.add_to(fig)\n",
      "    return fig\n",
      "\n",
      "plot_stations(stations_eu)\n",
      "63/246:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=NoaaGhcnParameter.DAILY.WIND_SPEED,\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('SCHIPHOL')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/247:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('SCHIPHOL')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "63/248:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('SCHIPHOL')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at Schiphol airport')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/249:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('SCHIPHOL')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "print(df)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value')\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at Schiphol airport')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/250:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('SCHIPHOL')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at Schiphol airport')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/251:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at Schiphol airport')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/252:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "65/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/School/Assignments/portm/RET.csv\")\n",
      "print(df.head())\n",
      "65/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "print(df.head())\n",
      "65/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "df\n",
      "65/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "df\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "65/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "df\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "65/6:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "# Set the index to the date column\n",
      "df = df.set_index(pd.to_datetime(df['Date'], format='%Y%m%d'))\n",
      "print(df)\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "65/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "# Set the index to the date column\n",
      "df = df.set_index(pd.to_datetime(df['date'], format='%Y%m%d'))\n",
      "print(df)\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "65/8:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "# Set the index to the date column\n",
      "print(df)\n",
      "df = df.set_index(pd.to_datetime(df['date'], format='%Y%m%d'))\n",
      "print(df)\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "65/9:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "# Set the index to the date column\n",
      "df = df.set_index(pd.to_datetime(df['DATE'], format='%Y%m%d'))\n",
      "print(df)\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "65/10:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "# Set the index to the date column\n",
      "df = df.set_index(pd.to_datetime(df['DATE'], format='%Y-%m-%d'))\n",
      "print(df)\n",
      "# Sample 50 assets (columns) randomly, also include the date column\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "65/11:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "df = pd.read_csv(\"C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv\")\n",
      "# Set the index to the date column\n",
      "df = df.set_index(pd.to_datetime(df['DATE'], format='%Y-%m-%d'))\n",
      "# Sample 50 assets (columns) randomly\n",
      "df = df.sample(n=50, axis=1)\n",
      "df\n",
      "63/253:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "63/254:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/1:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from wetterdienst import Wetterdienst\n",
      "import wetterdienst as wd\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "66/3:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/4:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Max temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/5:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/6:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/7:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('MALTA')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/8:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('Berlin')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/9:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('HEATHROW')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/10:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('HEATHROW')[0]\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "\n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/11:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('MALIN HEAD')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "    \n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De Bilt (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "66/12:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('MALIN HEAD')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "    \n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De MALIN HEAD')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De MALIN HEAD (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De MALIN HEAD (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "67/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from wetterdienst import Wetterdienst\n",
      "import wetterdienst as wd\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "67/2:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('MALIN HEAD')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "    \n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De MALIN HEAD')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De MALIN HEAD (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De MALIN HEAD (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "67/3:\n",
      "# Select a station and load the data for the last 10 years\n",
      "from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter\n",
      "import datetime as dt\n",
      "\n",
      "stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],\n",
      "    start_date=dt.datetime(2015, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "print(stations_object)\n",
      "def get_data_from_stations_request(\n",
      "    stations_object: NoaaGhcnRequest,\n",
      ") -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Takes a stations request object and process queries\n",
      "\n",
      "    Args:\n",
      "        stations_object: DwdObservationRequest object that holds all required information\n",
      "        for downloading opendata dwd data\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with content from DwdObservationRequest\n",
      "\n",
      "    \"\"\"\n",
      "    observation_data = []\n",
      "    \n",
      "    for result in stations_object.values.query():\n",
      "        observation_data.append(result.df)\n",
      "\n",
      "    return pd.concat(observation_data)\n",
      "\n",
      "df = get_data_from_stations_request(stations_object)\n",
      "# Pivot the data based on parameter\n",
      "df = df.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "print(df)\n",
      "# Plot the data\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De BILT')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Select subset as subplot \n",
      "subset = df[1:365]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De BILT (subset 1)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "# Select different as subplot \n",
      "subset = df[1000:1500]\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')\n",
      "#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')\n",
      "plt.title('Temperature at De BILT (subset 2)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "67/4:\n",
      "# Get historical temperature data.\n",
      "from wetterdienst import Settings\n",
      "from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter\n",
      "\n",
      "\n",
      "# request = DwdObservationRequest(\n",
      "#    parameter=[\"climate_summary\"],\n",
      "#    resolution=\"daily\",\n",
      "#    start_date=\"2000-01-01\",  # if not given timezone defaulted to UTC\n",
      "#    end_date=\"2022-01-01\",  # if not given timezone defaulted to UTC\n",
      "# ).filter_by_station_id(station_id=(4411))\n",
      "\n",
      "# df = get_data_from_stations_request(request)\n",
      "\n",
      "request = stations_object = NoaaGhcnRequest(\n",
      "    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200],\n",
      "    start_date=dt.datetime(2000, 1, 1),\n",
      "    end_date=dt.datetime(2022, 1, 1)\n",
      ").filter_by_name('DE BILT')\n",
      "\n",
      "\n",
      "df = get_data_from_stations_request(request)\n",
      "67/5:\n",
      "#print(df.head())\n",
      "print(df['parameter'].unique())\n",
      "# Plot the temperature data\n",
      "temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]\n",
      "print(temps.head())\n",
      "# Plot the temperature data , with different colors for each parameter\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(12,6))\n",
      "for parameter in temps['parameter'].unique():\n",
      "    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)\n",
      "plt.title('Temperature')\n",
      "plt.ylabel('Temperature (K)')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Widen the dataframe to have one column per parameter\n",
      "temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()\n",
      "# Convert Kelvin to Celsius\n",
      "temps['temperature_air_mean_200'] = temps['temperature_air_mean_200'] - 273.15\n",
      "temps['temperature_air_max_200'] = temps['temperature_air_max_200'] - 273.15\n",
      "temps['temperature_air_min_200'] = temps['temperature_air_min_200'] - 273.15\n",
      "67/6:\n",
      "winter_months = [10, 11, 12, 1, 2, 3]\n",
      "summer_months = [4, 5, 6, 7, 8, 9]\n",
      "\n",
      "temps_season = temps.copy(deep=True)\n",
      "temps_season['month'] = temps_season['date'].dt.month\n",
      "temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')\n",
      "temps_season\n",
      "67/7:\n",
      "# Temperature distributions in histogram\n",
      "plt.figure(figsize=(12,6))\n",
      "for season in temps_season['season'].unique():    \n",
      "    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5, bins = 100, density = True)\n",
      "plt.title('Temperature distribution')\n",
      "plt.ylabel('Frequency')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "67/8:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = temps.set_index('date')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots)\n",
      "plt.figure(figsize=(12,6))\n",
      "plt.subplot(211)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "plt.legend(loc='best')\n",
      "plt.title('Temperature with rolling mean')\n",
      "plt.subplot(212)\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "plt.legend(loc='best')\n",
      "plt.title('Rolling Variance')\n",
      "plt.show()\n",
      "67/9:\n",
      "decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')\n",
      "trend = decompose_result.trend\n",
      "seasonal = decompose_result.seasonal\n",
      "residual = decompose_result.resid\n",
      "### Visualise All Data\n",
      "decompose_result.plot()\n",
      "### Visualise 10 years\n",
      "years_examine = 365*10\n",
      "fig, axs = plt.subplots(3, figsize=(8,6))\n",
      "fig.suptitle('Removed Trend and Seasonality')\n",
      "axs[0].plot(trend[-years_examine:])\n",
      "axs[1].plot(seasonal[-years_examine:])\n",
      "axs[1].set_ylim([-10,10])\n",
      "axs[2].plot(residual[-years_examine:])\n",
      "67/10:\n",
      "from scipy.stats import norm\n",
      "from scipy.optimize import curve_fit\n",
      "temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)\n",
      "temp_t = temp_t.to_frame()\n",
      "# Rename the series to 'T'\n",
      "temp_t.columns = ['T']\n",
      "\n",
      "def model_fit_general(x, a, b, a1, b1, theta, phi):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)\n",
      "    return y_pred\n",
      "def model_fit(x, a, b, a1, b1):\n",
      "    omega = 2*np.pi/365 #365.25\n",
      "    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)\n",
      "    return y_pred\n",
      "def RSS(y, y_pred):\n",
      "    return np.sqrt( (y - y_pred)**2 ).sum()\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "    \n",
      "params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov))\n",
      "for name, p, sd in zip( param_list, params, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "    \n",
      "temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)\n",
      "if isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.toordinal)\n",
      "params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')\n",
      "param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']\n",
      "print('\\n Model 1 \\n') \n",
      "std_dev = np.sqrt(np.diag(cov1))\n",
      "for name, p, sd in zip( param_list, params1, std_dev):\n",
      "    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))\n",
      "temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)\n",
      "if not isinstance(temp_t.index , pd.DatetimeIndex):\n",
      "    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)\n",
      "temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )\n",
      "RSS(temp_t['T'], temp_t['model_fit_general'])\n",
      "print('Residual Sum of Squares (RSS)')\n",
      "print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))\n",
      "print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))\n",
      "68/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from wetterdienst import Wetterdienst\n",
      "import wetterdienst as wd\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "68/2:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')\n",
      "lookup_table_fips.columns = ['fips', 'iso', 'fips_name']\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "69/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from wetterdienst import Wetterdienst\n",
      "import wetterdienst as wd\n",
      "# What is available for us\n",
      "Wetterdienst.discover()\n",
      "69/2:\n",
      "# We are interested in data from NOAA GHCN\n",
      "API = Wetterdienst(provider='NOAA', network = 'GHCN')\n",
      "stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\n",
      "stations['country_code'] = stations['station_id'].str[0:2]\n",
      "\n",
      "lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')\n",
      "lookup_table_fips.columns = ['fips', 'iso', 'fips_name']\n",
      "lookup_table = pd.read_csv(\"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\")\n",
      "# Rename name to country_name\n",
      "lookup_table.rename(columns={'name': 'country_name'}, inplace=True)\n",
      "# Join the two tables\n",
      "stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')\n",
      "stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')\n",
      "stations.head()\n",
      "# What country codes did not join?\n",
      "print(stations[stations['alpha-2'].isna()]['country_code'].unique())\n",
      "69/3:\n",
      "# Get variables from inventory table\n",
      "inventory = pd.read_csv(\"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\", sep='\\s+', header=None)\n",
      "print(inventory.shape)\n",
      "70/1:\n",
      "\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "iris = sns.load_dataset('iris')\n",
      "df = iris.sample(n=10, random_state=1)\n",
      "df\n",
      "70/2:\n",
      "def highlight_cells(val):\n",
      "    color = 'yellow' if val == 5.1 else '#C6E2E9' # Pastel blue\n",
      "    return 'background-color: {}'.format(color)\n",
      "\n",
      "df.style.applymap(highlight_cells)\n",
      "70/3:\n",
      "df.style.format(formatter={\"sepal_length\": \"{:.1f}\", \"sepal_width\": \"{:.1f}\",\n",
      "                           \"petal_length\": \"{:.1f}\", \"petal_width\": \"{:.1f}\"})\n",
      "\n",
      "def highlight_cells(val):\n",
      "    color = 'yellow' if val == 5.1 else '#C6E2E9' # Pastel blue\n",
      "    return 'background-color: {}'.format(color)\n",
      "\n",
      "df.style.applymap(highlight_cells)\n",
      "70/4:\n",
      "df.style.format(formatter={\"sepal_length\": \"{:.1f}\", \"sepal_width\": \"{:.1f}\",\n",
      "                           \"petal_length\": \"{:.1f}\", \"petal_width\": \"{:.1f}\"})\n",
      "\n",
      "def highlight_rows(row):\n",
      "    value = row.loc['species']\n",
      "    if value == 'versicolor':\n",
      "        color = '#FFB3BA' # Red\n",
      "    elif value == 'setosa':\n",
      "        color = '#BAFFC9' # Green\n",
      "    else:\n",
      "        color = '#BAE1FF' # Blue\n",
      "    return ['background-color: {}'.format(color) for r in row]\n",
      "\n",
      "df.style.apply(highlight_rows, axis=1)\n",
      "70/5:\n",
      "df.style.format(formatter={\"sepal_length\": \"{:.1f}\", \"sepal_width\": \"{:.1f}\",\n",
      "                           \"petal_length\": \"{:.1f}\", \"petal_width\": \"{:.1f}\"})\n",
      "\n",
      "def highlight_rows(row):\n",
      "    value = row.loc['species']\n",
      "    if value == 'versicolor':\n",
      "        color = '#FFB3BA' # Red\n",
      "    elif value == 'setosa':\n",
      "        color = '#BAFFC9' # Green\n",
      "    else:\n",
      "        color = '#BAE1FF' # Blue\n",
      "    return ['background-color: {}'.format(color) for r in row]\n",
      "\n",
      "df.style.apply(highlight_rows, axis=1)\n",
      "def format_sepal(val):\n",
      "    condition = (val >= 3.5) & (val <= 5.5)\n",
      "    font_color = 'red' if condition else 'black'\n",
      "    font_weight = 'bold' if condition else 'normal'\n",
      "    return 'color: {}; font-weight: {}'.format(font_color, font_weight)\n",
      "\n",
      "df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\n",
      "70/6:\n",
      "# Df to html\n",
      "df.to_html('df.html')\n",
      "70/7:\n",
      "# Render the dataframe as an HTML text\n",
      "df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\\\n",
      "        .render()\n",
      "70/8:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\\\n",
      "        .render()\n",
      "        \n",
      "# Directly link the <style> ... </style> tag to the HTML text\n",
      "70/9:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\\\n",
      "        .render()\n",
      "        \n",
      "# Link the stylesheet to the HTML text\n",
      "html_text = '<link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">' + html_text\n",
      "70/10:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\\\n",
      "        .render()\n",
      "        \n",
      "# Show the html text\n",
      "html_text\n",
      "70/11:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\\\n",
      "        .render()\n",
      "        \n",
      "# Show the html text\n",
      "html_text.to_html_table_string()\n",
      "70/12:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\\\n",
      "        .render()\n",
      "        \n",
      "# Show the html text\n",
      "html_text.to_html()\n",
      "70/13:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\n",
      "        \n",
      "# Show the html text\n",
      "html_text.to_html()\n",
      "70/14:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\n",
      "        \n",
      "# Show the html text\n",
      "html_text.to_html_table_string()\n",
      "70/15:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "70/16:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "html_text\n",
      "70/17:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "html_text\n",
      "# Show in html text\n",
      "print(html_text.render())\n",
      "70/18:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "html_text\n",
      "# Show in html text\n",
      "print(html_text.to_html())\n",
      "70/19:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "html_text\n",
      "# Show in html text\n",
      "html_text.to_html()\n",
      "70/20:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "html_text\n",
      "# Print the style as a dictionary\n",
      "html_text._compute().data\n",
      "70/21:\n",
      "# Render the dataframe as an HTML text\n",
      "html_text = df.style.background_gradient()\n",
      "html_text\n",
      "# Print the style object as a dictionary\n",
      "html_text._repr_html_()\n",
      "70/22:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "style\n",
      "70/23:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, based on the CSS selectors\n",
      "style_dict = {}\n",
      "for selector in style[style.find('{')+1:style.find('}')].split(';'):\n",
      "    selector = selector.split(':')\n",
      "    if len(selector) == 2:\n",
      "        style_dict[selector[0].strip()] = selector[1].strip()\n",
      "style_dict\n",
      "70/24:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'}\n",
      "style_dict = {}\n",
      "for style_row in style[style.find('{')+1:style.find('}')].split(';'):\n",
      "    if style_row.strip():\n",
      "        style_dict[style_row.split(':')[0].strip()] = style_row.split(':')[1].strip()\n",
      "        \n",
      "# Make a new dataframe with the style dictionary\n",
      "style_df = pd.DataFrame(style_dict.items(), columns=['id', 'style'])\n",
      "style_df\n",
      "70/25:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "style_dict = {}\n",
      "for s in style.split('}'):\n",
      "    if s:\n",
      "        keys = s.split('{')[0]\n",
      "        # different ids are seperated by ,\n",
      "        keys = keys.split(',')\n",
      "        value = s.split('{')[1]\n",
      "        for key in keys:\n",
      "            key = key.strip() \n",
      "            style_dict[key] = value\n",
      "    \n",
      "style_dict\n",
      "70/26:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "style_dict = {}\n",
      "for s in style.split('}'):\n",
      "    if s:\n",
      "        keys = s.split('{')[0]\n",
      "        print(keys)\n",
      "        # different ids are seperated by ,\n",
      "        keys = keys.split(',')\n",
      "        value = s.split('{')[1]\n",
      "        for key in keys:\n",
      "            key = key.strip() \n",
      "            style_dict[key] = value\n",
      "    \n",
      "style_dict\n",
      "70/27:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "style_dict = {}\n",
      "print(style)\n",
      "for s in style.split('}'):\n",
      "    if s:\n",
      "        keys = s.split('{')[0]\n",
      "        print(keys)\n",
      "        # different ids are seperated by ,\n",
      "        keys = keys.split(',')\n",
      "        value = s.split('{')[1]\n",
      "        for key in keys:\n",
      "            key = key.strip() \n",
      "            style_dict[key] = value\n",
      "    \n",
      "style_dict\n",
      "70/28:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "style_dict = {}\n",
      "#print(style)\n",
      "for s in style.split('}'):\n",
      "    print(s)\n",
      "    if s:\n",
      "        keys = s.split('{')[0]\n",
      "        print(keys)\n",
      "        # different ids are seperated by ,\n",
      "        keys = keys.split(',')\n",
      "        value = s.split('{')[1]\n",
      "        for key in keys:\n",
      "            key = key.strip() \n",
      "            style_dict[key] = value\n",
      "    \n",
      "style_dict\n",
      "70/29:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "def convert_css_to_dict(style_string):\n",
      "    style_list = style_string.strip().split(\"\\n\")\n",
      "    style_dict = {}\n",
      "    for style in style_list:\n",
      "        if \"{\" in style:\n",
      "            selector = style.split(\"{\")[0].strip()\n",
      "            values = style.split(\"{\")[1].split(\"}\")[0].strip()\n",
      "            style_dict[selector] = {}\n",
      "            values = values.split(\";\")\n",
      "            for value in values:\n",
      "                if value:\n",
      "                    prop = value.split(\":\")[0].strip()\n",
      "                    val = value.split(\":\")[1].strip()\n",
      "                    style_dict[selector][prop] = val\n",
      "    return style_dict\n",
      "\n",
      "style_dict = convert_css_to_dict(style)\n",
      "style_dict\n",
      "70/30:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "style\n",
      "70/31:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "import re\n",
      "\n",
      "def css_to_dict(style_string):\n",
      "    styles = re.findall(r'#[\\w_]+ {\\n([\\s\\S]+?)\\n}', style_string)\n",
      "    css_dict = {}\n",
      "    for style in styles:\n",
      "        style_lines = style.strip().split('\\n')\n",
      "        selector = re.search(r'#[\\w_]+', style_lines[0]).group()\n",
      "        properties = {}\n",
      "        for line in style_lines[1:]:\n",
      "            key, value = line.strip().split(': ')\n",
      "            properties[key] = value\n",
      "        css_dict[selector] = properties\n",
      "    return css_dict\n",
      "\n",
      "style_dict = css_to_dict(style)\n",
      "style_dict\n",
      "70/32:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "style\n",
      "70/33:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "lines = style.strip().split(\"\\n\")\n",
      "lines\n",
      "70/34:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style> and </style> tags\n",
      "style = style[7:-8]\n",
      "style\n",
      "70/35:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>')+1:style.rfind('<')]\n",
      "style\n",
      "70/36:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "style\n",
      "70/37:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "lines\n",
      "70/38:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[:l.find('{')]\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "    print(selector, style_css)\n",
      "70/39:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[:l.find('{')]\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "    print(selector)\n",
      "70/40:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    print(l)\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[:l.find('{')]\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "70/41:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    print(l)\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[l.find('{'):]\n",
      "    print(selector)\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "70/42:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[l.find('{'):]\n",
      "    print(selector)\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "70/43:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[:l.find('{')]\n",
      "    print(selector)\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "70/44:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for l in lines:\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[:l.find('{')]\n",
      "    print(l)\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "70/45:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"\\n\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Split the line into the selector and the style\n",
      "    selector = l[:l.find('{')]\n",
      "    print(idx)\n",
      "    print(l)\n",
      "    style_css = l[l.find('{')+1:l.find('}')]\n",
      "70/46:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    print(idx)\n",
      "    print(l)\n",
      "70/47:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    print(selectors)\n",
      "    print(idx)\n",
      "    print(l)\n",
      "70/48:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    print(selectors)\n",
      "    print(idx)\n",
      "    print(l)\n",
      "70/49:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:]\n",
      "    \n",
      "    print(selectors)\n",
      "    print(style)\n",
      "    print(idx)\n",
      "    print(l)\n",
      "70/50:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:]\n",
      "    \n",
      "    print(selectors)\n",
      "    print(style)\n",
      "70/51:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:]\n",
      "    \n",
      "    print(selectors)\n",
      "    print(_style)\n",
      "70/52:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip()[1:] for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:]\n",
      "    \n",
      "    print(selectors)\n",
      "    print(_style)\n",
      "70/53:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:]\n",
      "    \n",
      "    print(selectors)\n",
      "    print(_style)\n",
      "70/54:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:]\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "print(style_dict)\n",
      "70/55:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "print(style_dict)\n",
      "70/56:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "table\n",
      "70/57:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(key, '<td id=\"{}\" style=\"{}\">'.format(key, value))\n",
      "\n",
      "table\n",
      "70/58:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(key, f'id=\"{key}\"', value)\n",
      "\n",
      "table\n",
      "70/59:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', value)\n",
      "\n",
      "table\n",
      "70/60:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', value)\n",
      "\n",
      "new_table\n",
      "70/61:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', value)\n",
      "print(style_dict)\n",
      "new_table\n",
      "70/62:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', value)\n",
      "print(style_dict)\n",
      "table\n",
      "70/63:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    value_formatted = value.replace(':', '=')\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value_formatted}\"')\n",
      "print(style_dict)\n",
      "table\n",
      "70/64:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    value_formatted = value.replace(':', '=')\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value_formatted}\"')\n",
      "table\n",
      "70/65:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    value_formatted = value.replace(':', '=')\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value_formatted}\"')\n",
      "new_table\n",
      "70/66:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    value_formatted = value.replace(':', '=')\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value_formatted}\"')\n",
      "\n",
      "new_table.replace('\\n', '')\n",
      "70/67:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "html_gr\n",
      "# Show in html text\n",
      "html_text = html_gr.to_html()\n",
      "# Get the full <style> ... </style> block\n",
      "style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "# Get the full <table> ... </table> block\n",
      "table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "\n",
      "# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'\n",
      "\n",
      "# Drop the <style type=\"text/css\"> and </style> tags\n",
      "style = style[style.find('>\\n')+1:style.rfind('\\n<')]\n",
      "\n",
      "# Remove all \\n\n",
      "style = style.replace('\\n', '')\n",
      "lines = style.strip().split(\"}\")\n",
      "style_dict = {}\n",
      "for idx, l in enumerate(lines):\n",
      "    # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "    selectors = l[:l.find('{')]\n",
      "    # Seperate the selectors by ,\n",
      "    selectors = selectors.split(',')\n",
      "    # Trim whitespace and remove #\n",
      "    selectors = [s.replace('#','').strip() for s in selectors]\n",
      "    # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "    _style = l[l.find('{')+1:].strip()\n",
      "    \n",
      "    for s in selectors:\n",
      "        style_dict[s] = _style\n",
      "        \n",
      "#print(style_dict)\n",
      "\n",
      "# Replace everything in table with id=\"T_81cd3_row0_col0\" to <td id=\"T_81cd3_row0_col0\" style=\"background-color: #b1c2de; color: #000000;\">\n",
      "new_table = table\n",
      "for key, value in style_dict.items():\n",
      "    new_table = new_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "\n",
      "new_table.replace('\\n', '')\n",
      "71/1:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "\n",
      "# Wrap in function for html styled df\n",
      "def switch_style(df):\n",
      "    html_text = df.to_html()\n",
      "    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "    # Get the full <table> ... </table> block\n",
      "    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "    \n",
      "    # Drop the <style type=\"text/css\"> and </style> tags\n",
      "    html_style = html_style[html_style.find('>\\n')+1:html_style.rfind('\\n<')].replace('\\n', '')\n",
      "    html_table = html_table.replace('\\n', '')\n",
      "    \n",
      "    # Iterate over style\n",
      "    style_lines = html_style.strip().split(\"}\")\n",
      "    style_dict = {}\n",
      "    for idx, l in enumerate(lines):\n",
      "        # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "        selectors = l[:l.find('{')]\n",
      "        # Seperate the selectors by ,\n",
      "        selectors = selectors.split(',')\n",
      "        # Trim whitespace and remove #\n",
      "        selectors = [s.replace('#','').strip() for s in selectors]\n",
      "        # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "        _style = l[l.find('{')+1:].strip()\n",
      "        for s in selectors:\n",
      "            style_dict[s] = _style\n",
      "            \n",
      "    for key, value in style_dict.items():\n",
      "        html_table = html_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "        \n",
      "    return html_table\n",
      "    \n",
      "    \n",
      "switch_style(html_gr)\n",
      "71/2:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "\n",
      "# Wrap in function for html styled df\n",
      "def switch_style(data):\n",
      "    html_text = data.to_html()\n",
      "    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "    # Get the full <table> ... </table> block\n",
      "    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "    \n",
      "    # Drop the <style type=\"text/css\"> and </style> tags\n",
      "    html_style = html_style[html_style.find('>\\n')+1:html_style.rfind('\\n<')].replace('\\n', '')\n",
      "    html_table = html_table.replace('\\n', '')\n",
      "    \n",
      "    # Iterate over style\n",
      "    style_lines = html_style.strip().split(\"}\")\n",
      "    style_dict = {}\n",
      "    for idx, l in enumerate(lines):\n",
      "        # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "        selectors = l[:l.find('{')]\n",
      "        # Seperate the selectors by ,\n",
      "        selectors = selectors.split(',')\n",
      "        # Trim whitespace and remove #\n",
      "        selectors = [s.replace('#','').strip() for s in selectors]\n",
      "        # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "        _style = l[l.find('{')+1:].strip()\n",
      "        for s in selectors:\n",
      "            style_dict[s] = _style\n",
      "            \n",
      "    for key, value in style_dict.items():\n",
      "        html_table = html_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "        \n",
      "    return html_table\n",
      "    \n",
      "    \n",
      "switch_style(html_gr)\n",
      "71/3:\n",
      "\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "iris = sns.load_dataset('iris')\n",
      "df = iris.sample(n=10, random_state=1)\n",
      "df\n",
      "71/4:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "\n",
      "# Wrap in function for html styled df\n",
      "def switch_style(data):\n",
      "    html_text = data.to_html()\n",
      "    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "    # Get the full <table> ... </table> block\n",
      "    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "    \n",
      "    # Drop the <style type=\"text/css\"> and </style> tags\n",
      "    html_style = html_style[html_style.find('>\\n')+1:html_style.rfind('\\n<')].replace('\\n', '')\n",
      "    html_table = html_table.replace('\\n', '')\n",
      "    \n",
      "    # Iterate over style\n",
      "    style_lines = html_style.strip().split(\"}\")\n",
      "    style_dict = {}\n",
      "    for idx, l in enumerate(lines):\n",
      "        # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "        selectors = l[:l.find('{')]\n",
      "        # Seperate the selectors by ,\n",
      "        selectors = selectors.split(',')\n",
      "        # Trim whitespace and remove #\n",
      "        selectors = [s.replace('#','').strip() for s in selectors]\n",
      "        # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "        _style = l[l.find('{')+1:].strip()\n",
      "        for s in selectors:\n",
      "            style_dict[s] = _style\n",
      "            \n",
      "    for key, value in style_dict.items():\n",
      "        html_table = html_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "        \n",
      "    return html_table\n",
      "    \n",
      "    \n",
      "switch_style(html_gr)\n",
      "71/5:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "\n",
      "# Wrap in function for html styled df\n",
      "def switch_style(data):\n",
      "    html_text = data.to_html()\n",
      "    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "    # Get the full <table> ... </table> block\n",
      "    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "    \n",
      "    # Drop the <style type=\"text/css\"> and </style> tags\n",
      "    html_style = html_style[html_style.find('>\\n')+1:html_style.rfind('\\n<')].replace('\\n', '')\n",
      "    html_table = html_table.replace('\\n', '')\n",
      "    \n",
      "    # Iterate over style\n",
      "    style_lines = html_style.strip().split(\"}\")\n",
      "    style_dict = {}\n",
      "    for idx, l in enumerate(style_lines):\n",
      "        # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "        selectors = l[:l.find('{')]\n",
      "        # Seperate the selectors by ,\n",
      "        selectors = selectors.split(',')\n",
      "        # Trim whitespace and remove #\n",
      "        selectors = [s.replace('#','').strip() for s in selectors]\n",
      "        # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "        _style = l[l.find('{')+1:].strip()\n",
      "        for s in selectors:\n",
      "            style_dict[s] = _style\n",
      "            \n",
      "    for key, value in style_dict.items():\n",
      "        html_table = html_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "        \n",
      "    return html_table\n",
      "    \n",
      "    \n",
      "switch_style(html_gr)\n",
      "71/6:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "\n",
      "# Wrap in function for html styled df\n",
      "def switch_style(data):\n",
      "    html_text = data.to_html()\n",
      "    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "    # Get the full <table> ... </table> block\n",
      "    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "    \n",
      "    # Drop the <style type=\"text/css\"> and </style> tags\n",
      "    html_style = html_style[html_style.find('>\\n')+1:html_style.rfind('\\n<')].replace('\\n', '')\n",
      "    html_table = html_table.replace('\\n', '')\n",
      "    \n",
      "    # Iterate over style\n",
      "    style_lines = html_style.strip().split(\"}\")\n",
      "    style_dict = {}\n",
      "    for idx, l in enumerate(style_lines):\n",
      "        # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "        selectors = l[:l.find('{')]\n",
      "        # Seperate the selectors by ,\n",
      "        selectors = selectors.split(',')\n",
      "        # Trim whitespace and remove #\n",
      "        selectors = [s.replace('#','').strip() for s in selectors]\n",
      "        # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "        _style = l[l.find('{')+1:].strip()\n",
      "        for s in selectors:\n",
      "            style_dict[s] = _style\n",
      "            \n",
      "    for key, value in style_dict.items():\n",
      "        html_table = html_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "        \n",
      "    return html_table\n",
      "    \n",
      "    \n",
      "switch_style(html_gr)\n",
      "71/7:\n",
      "\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "iris = sns.load_dataset('iris')\n",
      "df = iris.sample(n=10, random_state=1)\n",
      "df\n",
      "71/8:\n",
      "df.style.format(formatter={\"sepal_length\": \"{:.1f}\", \"sepal_width\": \"{:.1f}\",\n",
      "                           \"petal_length\": \"{:.1f}\", \"petal_width\": \"{:.1f}\"})\n",
      "\n",
      "def highlight_rows(row):\n",
      "    value = row.loc['species']\n",
      "    if value == 'versicolor':\n",
      "        color = '#FFB3BA' # Red\n",
      "    elif value == 'setosa':\n",
      "        color = '#BAFFC9' # Green\n",
      "    else:\n",
      "        color = '#BAE1FF' # Blue\n",
      "    return ['background-color: {}'.format(color) for r in row]\n",
      "\n",
      "df.style.apply(highlight_rows, axis=1)\n",
      "def format_sepal(val):\n",
      "    condition = (val >= 3.5) & (val <= 5.5)\n",
      "    font_color = 'red' if condition else 'black'\n",
      "    font_weight = 'bold' if condition else 'normal'\n",
      "    return 'color: {}; font-weight: {}'.format(font_color, font_weight)\n",
      "\n",
      "df.style.apply(highlight_rows, axis=1)\\\n",
      "        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\n",
      "71/9:\n",
      "# Render the dataframe as an HTML text\n",
      "html_gr = df.style.background_gradient()\n",
      "\n",
      "# Wrap in function for html styled df\n",
      "def switch_style(data):\n",
      "    html_text = data.to_html()\n",
      "    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]\n",
      "    # Get the full <table> ... </table> block\n",
      "    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]\n",
      "    \n",
      "    # Drop the <style type=\"text/css\"> and </style> tags\n",
      "    html_style = html_style[html_style.find('>\\n')+1:html_style.rfind('\\n<')].replace('\\n', '')\n",
      "    html_table = html_table.replace('\\n', '')\n",
      "    \n",
      "    # Iterate over style\n",
      "    style_lines = html_style.strip().split(\"}\")\n",
      "    style_dict = {}\n",
      "    for idx, l in enumerate(style_lines):\n",
      "        # Find the selector(s), e.g. #T_81cd3_row0_col0\n",
      "        selectors = l[:l.find('{')]\n",
      "        # Seperate the selectors by ,\n",
      "        selectors = selectors.split(',')\n",
      "        # Trim whitespace and remove #\n",
      "        selectors = [s.replace('#','').strip() for s in selectors]\n",
      "        # Find the style, e.g. background-color: #b1c2de; color: #000000;\n",
      "        _style = l[l.find('{')+1:].strip()\n",
      "        for s in selectors:\n",
      "            style_dict[s] = _style\n",
      "            \n",
      "    for key, value in style_dict.items():\n",
      "        html_table = html_table.replace(f'id=\"{key}\"', f'id=\"{key}\" style=\"{value}\"')\n",
      "        \n",
      "    return html_table\n",
      "    \n",
      "    \n",
      "switch_style(html_gr)\n",
      "71/10:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "df = pd.DataFrame({'performance': np.random.randn(365), 'id': np.random.choice(['C-103', 'C-102', 'C-101'], 365), 'date': pd.date_range('1/1/2018', periods=365)})\n",
      "df\n",
      "71/11:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "# Let us create a dataframe with 3 columns: id, date and performance\n",
      "# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals\n",
      "df = pd.DataFrame({'id': ['C-103', 'C-102', 'C-101'], 'date': pd.date_range('2023-01-01', '2023-02-01', freq='H')})\n",
      "# Create a column called performance with random numbers\n",
      "df['performance'] = np.random.randint(0, 100, df.shape[0])\n",
      "\n",
      "df\n",
      "71/12:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "# Let us create a dataframe with 3 columns: id, date and performance\n",
      "# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals\n",
      "df = pd.DataFrame({'date': pd.date_range('2023-01-01', '2023-02-01', freq='H')})\n",
      "# Create a column called performance with random numbers\n",
      "df['performance'] = np.random.randint(0, 100, df.shape[0])\n",
      "\n",
      "df\n",
      "71/13:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "# Let us create a dataframe with 3 columns: id, date and performance\n",
      "# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals\n",
      "df = pd.DataFrame({'date': pd.date_range('2023-01-01', '2023-02-01', freq='H')})\n",
      "# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values\n",
      "df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])\n",
      "# Create a column called performance with random numbers\n",
      "df['performance'] = np.random.randint(65, 100, df.shape[0])\n",
      "\n",
      "df\n",
      "71/14:\n",
      "# Summarize the dataframe by id and 12h performance, 24h performance, 7d performance and 1M perfromance\n",
      "table_performance = df.groupby('id').resample('12H', on='date').performance.agg(['mean']).reset_index()\n",
      "table_performance\n",
      "71/15:\n",
      "# Summarize the dataframe by id and most recent 12h performance, 24h performance, 7d performance and 1M perfromance\n",
      "table_performance = df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(12).performance.mean()).reset_index().rename(columns={0: '12h'})\n",
      "table_performance\n",
      "71/16:\n",
      "# Summarize the dataframe by id and most recent 12h performance, most recent 24h performance, most recent 7d performance and most recent 1M perfromance\n",
      "table_performance = df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(12).performance.mean()).reset_index().rename(columns={0: '12h'})\n",
      "table_performance = pd.merge(table_performance, df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(24).performance.mean()).reset_index().rename(columns={0: '24h'}), how='left', on='id')\n",
      "table_performance = pd.merge(table_performance, df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(168).performance.mean()).reset_index().rename(columns={0: '7d'}), how='left', on='id')\n",
      "table_performance = pd.merge(table_performance, df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(720).performance.mean()).reset_index().rename(columns={0: '1M'}), how='left', on='id')\n",
      "table_performance\n",
      "71/17:\n",
      "# Summarize the dataframe by id and create an average 7-day rolling performance column\n",
      "df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(7).mean()))\n",
      "df\n",
      "71/18:\n",
      "# Summarize the dataframe by id and create an average 7-day rolling performance column\n",
      "df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(7*24).mean()))\n",
      "df\n",
      "71/19:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "# Let us create a dataframe with 3 columns: id, date and performance\n",
      "# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals\n",
      "df = pd.DataFrame({'date': pd.date_range('2022-12-01', '2023-02-01', freq='H')})\n",
      "# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values\n",
      "df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])\n",
      "# Create a column called performance with random numbers\n",
      "df['performance'] = np.random.randint(65, 100, df.shape[0])\n",
      "\n",
      "df\n",
      "71/20:\n",
      "# Summarize the dataframe by id and create an average 1-day rolling performance column\n",
      "df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(24).mean()))\n",
      "df\n",
      "\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "table_performance[table_performance.date == datetime.date.today()]\n",
      "71/21:\n",
      "# Summarize the dataframe by id and create an average 1-day rolling performance column\n",
      "df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(24).mean()))\n",
      "df\n",
      "\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "table_performance\n",
      "71/22:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "table_performance\n",
      "71/23:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "# Let us create a dataframe with 3 columns: id, date and performance\n",
      "# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals\n",
      "df = pd.DataFrame({'date': pd.date_range('2022-12-01', '2023-02-01', freq='H')})\n",
      "# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values\n",
      "df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])\n",
      "# Create a column called performance with random numbers\n",
      "df['performance'] = np.random.randint(65, 100, df.shape[0])\n",
      "\n",
      "df\n",
      "71/24:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "table_performance\n",
      "71/25:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "71/26:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "table_performance\n",
      "71/27:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(index='date', columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "71/28:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(index='date', columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance\n",
      "71/29:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance\n",
      "71/30:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance.style.background_gradient()\n",
      "71/31:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance.style.background_gradient().reset_index()\n",
      "71/32:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance.reset_index().style.background_gradient()\n",
      "71/33:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance.reset_index(drop=True).style.background_gradient()\n",
      "71/34:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(values='id', columns=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance.reset_index(drop=True).style.background_gradient()\n",
      "71/35:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "71/36:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance\n",
      "71/37:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance = table_performance.pivot(values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])\n",
      "table_performance\n",
      "71/38:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id and show the performance columns\n",
      "table_performance\n",
      "71/39:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "# Pivot on id, show the id's as columns and the performance_columns as index.\n",
      "table_performance = table_performance.pivot(index=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id')\n",
      "\n",
      "table_performance\n",
      "71/40:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id', columns='date', values='rolling_performance_12h', aggfunc='first').fillna('-')\n",
      "71/41:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id', columns='date', values='rolling_performance_12h', aggfunc='first').fillna('-')\n",
      "table_performance\n",
      "71/42:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index=['date', 'rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id', values='performance').reset_index()\n",
      "\n",
      "table_performance\n",
      "71/43:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id', values='performance').reset_index()\n",
      "\n",
      "table_performance\n",
      "71/44:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id', values='performance')\n",
      "\n",
      "table_performance\n",
      "71/45:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  columns='date',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "table_performance\n",
      "71/46:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "table_performance\n",
      "71/47:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "table_performance.style.background_gradient()\n",
      "71/48:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "table_performance.style.background_gradient()\n",
      "# In percentage\n",
      "table_performance.style.format(\"{:.2%}\")\n",
      "71/49:\n",
      "\n",
      "# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "# Let us create a dataframe with 3 columns: id, date and performance\n",
      "# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals\n",
      "df = pd.DataFrame({'date': pd.date_range('2022-12-01', '2023-02-01', freq='H')})\n",
      "# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values\n",
      "df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])\n",
      "# Create a column called performance with random numbers\n",
      "df['performance'] = np.random.randint(65, 100, df.shape[0]) / 100\n",
      "\n",
      "df\n",
      "71/50:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage\n",
      "table_performance.style.format(\"{:.2%}\")\n",
      "table_performance.style.background_gradient()\n",
      "71/51:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage\n",
      "table_performance.style.format(\"{:.2%}%\")\n",
      "table_performance.style.background_gradient()\n",
      "71/52:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "\n",
      "table_performance.style.format(\"{:.2%}\")\n",
      "table_performance.style.background_gradient()\n",
      "71/53:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "\n",
      "table_performance.style.format(\"{:.3%}\")\n",
      "table_performance.style.background_gradient()\n",
      "71/54:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "\n",
      "table_performance.style.format(\"{:.2%}\")\n",
      "#table_performance.style.background_gradient()\n",
      "71/55:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "\n",
      "table_p = table_performance.style.format(\"{:.2%}\")\n",
      "table_p =  table_p.style.background_gradient()\n",
      "table_p\n",
      "71/56:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "\n",
      "table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "71/57:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h\n",
      "71/58:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "71/59:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "71/60:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/61:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/62:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "\n",
      "# In percentage format\n",
      "71/63:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/64:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "#tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "#tb_p\n",
      "71/65:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx + 1).astype(str).str.zfill(2)\n",
      " \n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "#tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "#tb_p\n",
      "71/66:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (int(indx) + 1).astype(str).str.zfill(2)\n",
      " \n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "#tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "#tb_p\n",
      "71/67:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx..astype(int) + 1).astype(str).str.zfill(2)\n",
      " \n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "#tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "#tb_p\n",
      "71/68:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      " \n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "#tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "#tb_p\n",
      "71/69:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      " \n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/70:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      " \n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/71:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=23)]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/72:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24) & (df.date <= df.date.max())]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/73:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24) & (df.date < df.date.max())]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/74:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[(df.date >= df.date.max() - datetime.timedelta(hours=24)) and (df.date < df.date.max())]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "71/75:\n",
      "# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.\n",
      "table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),\n",
      "                                                             rolling_performance_24h=x.performance.rolling(24).mean(),\n",
      "                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),\n",
      "                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))\n",
      "\n",
      "\n",
      "\n",
      "# Fetch the most recent\n",
      "table_performance = table_performance[table_performance.date == table_performance.date.max()]\n",
      "\n",
      "# Id needs to be columns, such that C-101, C-102 and C-103 are the column names\n",
      "# On the index, we find 12h, 24h, 7d and 30d\n",
      "table_performance = table_performance.pivot_table(index='id',\n",
      "                                                  values=['rolling_performance_12h',\n",
      "                                                          'rolling_performance_24h',\n",
      "                                                          'rolling_performance_7d',\n",
      "                                                          'rolling_performance_30d'])\n",
      "\n",
      "# In percentage format\n",
      "tb_p = table_performance.style.background_gradient().format(\"{:.2%}\")\n",
      "\n",
      "\n",
      "# Fetch the last 24 hours\n",
      "table_performance_24h = df[(df.date >= df.date.max() - datetime.timedelta(hours=24)) & (df.date < df.date.max())]\n",
      "# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.\n",
      "# index name is the date\n",
      "# columns are the ids\n",
      "table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')\n",
      "# strftime 00-01\n",
      "indx = table_performance_24h.index.strftime('%H')\n",
      "indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)\n",
      "table_performance_24h.index = indx\n",
      "\n",
      "# Name of the index is the date\n",
      "table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')\n",
      "table_performance_24h\n",
      "# In percentage format\n",
      "tb_p = table_performance_24h.style.background_gradient().format(\"{:.2%}\")\n",
      "tb_p\n",
      "73/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from .secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/2:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/3:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/4:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/5:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "with server:\n",
      "    df = load_query('raw', query)\n",
      "\n",
      "df\n",
      "73/6:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/7:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "with server:\n",
      "    df = load_query('raw', query)\n",
      "\n",
      "df\n",
      "73/8:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/9:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "with server:\n",
      "    df = load_query('raw', query)\n",
      "\n",
      "df\n",
      "73/10:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "with server:\n",
      "    df = load_query('raw', query)\n",
      "\n",
      "df\n",
      "73/11:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "with server.start():\n",
      "    df = load_query('raw', query)\n",
      "\n",
      "df\n",
      "73/12:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/13:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/14:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/15:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/16:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/17:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/18:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "print(DB_USER)\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/19:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/20:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/21:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"user={user} dbname={database} password={password} host={host}\"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/22:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/23:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user = {user}\n",
      "    dbname = {database}\n",
      "    password = {password}\n",
      "    host = {host}\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "73/24:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/25:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, REMOTE_PORT),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "# Check if tunnel is open\n",
      "print(server.local_bind_port)\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "73/26:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 25),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "# Check if tunnel is open\n",
      "print(server.local_bind_port)\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user = {user}\n",
      "    dbname = {database}\n",
      "    password = {password}\n",
      "    host = {host}\n",
      "    \"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/2:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "# Check if tunnel is open\n",
      "print(server.local_bind_port)\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/3:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    \"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/4:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/5:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/6:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/7:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    port=5432\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/8:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/9:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password=bk7Iut6Me9O4gh4f6jSe\n",
      "    host={host}\n",
      "    port=5432\n",
      "    \"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/10:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/11:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password=\"bk7Iut6Me9O4gh4f6jSe\"\n",
      "    host={host}\n",
      "    port=5432\n",
      "    \"\"\"\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/12:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "74/13:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "74/14:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/2:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'grafana_reader'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/3:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/4:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/5:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'nicky'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user={user}\n",
      "    dbname={database}\n",
      "    password={password}\n",
      "    host={host}\n",
      "    \"\"\"\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/6:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/7:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'nicky'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user = {user}\n",
      "    dbname = {database}\n",
      "    password = {password}\n",
      "    host = {host}\n",
      "    \"\"\"\n",
      "    keepalive_kwargs = {\n",
      "    \"keepalives\": 1,\n",
      "    \"keepalives_idle\": 250,\n",
      "    \"keepalives_interval\": 5,\n",
      "    \"keepalives_count\": 5,\n",
      "    }\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/8:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/9:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'nicky'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    DSN = f\"\"\"\n",
      "    user = {user}\n",
      "    dbname = {database}\n",
      "    password='{password}'\n",
      "    host = {host}\n",
      "    \"\"\"\n",
      "    keepalive_kwargs = {\n",
      "    \"keepalives\": 1,\n",
      "    \"keepalives_idle\": 250,\n",
      "    \"keepalives_interval\": 5,\n",
      "    \"keepalives_count\": 5,\n",
      "    }\n",
      "    print(DSN)\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(DSN, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/10:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/11:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "REMOTE_HOST = '116.203.157.240'\n",
      "REMOTE_USER = 'root'\n",
      "REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'\n",
      "REMOTE_PORT = 22\n",
      "\n",
      "DB_USER = 'nicky'\n",
      "DB_HOST = 'localhost'\n",
      "DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'\n",
      "DB_PORT = 5432\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': 5432,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5,}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(database: str, query: str) -> pd.DataFrame:\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/12:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('raw', query)\n",
      "df\n",
      "75/13:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "    \n",
      "df = load_query('postgres', query)\n",
      "df\n",
      "75/14:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/15:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL) AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "df\n",
      "75/16:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "df\n",
      "75/17:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "print(df)\n",
      "75/18:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],\n",
      "    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']\n",
      "}\n",
      "\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_daily\n",
      "75/19:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],\n",
      "    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']\n",
      "}\n",
      "\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_daily\n",
      "75/20:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],\n",
      "    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/21:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'time_zone' : 'UTC',\n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        print(data)\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/22:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],\n",
      "    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/23:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        print(data)\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/24:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],\n",
      "    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/25:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        print(data)\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/26:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],\n",
      "    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/27:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max','windspeed_10m_min', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/28:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/29:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ', '.join(hourly_params),\n",
      "              'daily' : ', '.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        print(urllib.parse.urlencode(params, safe = ','))\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        print(data)\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/30:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/31:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        print(urllib.parse.urlencode(params, safe = ','))\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        print(data)\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/32:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        print(urllib.parse.urlencode(params, safe = ','))\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/33:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/34:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        print(urllib.parse.urlencode(params, safe = ','))\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        print(data)\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/35:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/36:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/37:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/38:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2020,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/39:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2023,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/40:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/41:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/42:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    lat = location_eindhoven[0], \n",
      "    lon=location_eindhoven[1], \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/43:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(20,10))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.plot(w_hourly['time'], w_hourly['shortwave_radiation'], label='Solar Radiation')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s), Solar Radiation (W/m2)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "75/44:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(20,10))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "75/45:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(10,5))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "75/46:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(8,2))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "75/47:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(20,3))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "75/48:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/49:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    location_eindhoven,\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/50:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = (51.441642, 5.469722)\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    [location_eindhoven],\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/51:\n",
      "TSO_COUNTRY_CODES = {\n",
      "    \"SK\"\n",
      "\"DK\"\n",
      "\"LT\"\n",
      "\"SI\"\n",
      "\"CZ\"\n",
      "\"FR\"\n",
      "\"LV\"\n",
      "\"RO\"\n",
      "\"NL\"\n",
      "\"EE\"\n",
      "\"HU\"\n",
      "\"AT\"\n",
      "\"FI\"\n",
      "\"RS\"\n",
      "\"IE\"\n",
      "\"PL\"\n",
      "\"UA\"\n",
      "\"ES\"\n",
      "\"PT\"\n",
      "\"NO\"\n",
      "\"BA\"\n",
      "\"GB\"\n",
      "\"BG\"\n",
      "\"IT\"\n",
      "\"HR\"\n",
      "\"SE\"\n",
      "\"GR\"\n",
      "\"CH\"\n",
      "\"BE\"\n",
      "\"DE\"\n",
      "\n",
      "'Germany' : 'DE',\n",
      "\n",
      "}\n",
      "\n",
      "# Locations of large European cities\n",
      "LOCATIONS = {\n",
      "    'Amsterdam' : {'location' : (52.370216, 4.895168), 'country' : 'NL'},\n",
      "    'Barcelona' : {'location' : (41.385064, 2.173403), 'country' : 'ES'},\n",
      "    'Berlin' : {'location' : (52.520008, 13.404954), 'country' : 'DE'},\n",
      "    'Frankfurt' : {'location' : (50.110922, 8.682127), 'country' : 'DE'},\n",
      "    'Arhus' : {'location' : (56.162939, 10.203921), 'country' : 'DK'},\n",
      "    'Copenhagen' : {'location' : (55.676097, 12.568337), 'country' : 'DK'},\n",
      "    'Brussels' : {'location' : (50.850340, 4.351710), 'country' : 'BE'},\n",
      "}\n",
      "75/52:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "LOCATIONS = {\n",
      "    \"Bratislava\": {\"location\": (48.148598, 17.107748), \"country\": \"SK\"},\n",
      "    \"Košice\": {\"location\": (48.720595, 21.257698), \"country\": \"SK\"},\n",
      "    \"Copenhagen\": {\"location\": (55.676097, 12.568337), \"country\": \"DK\"},\n",
      "    \"Aarhus\": {\"location\": (56.162939, 10.203921), \"country\": \"DK\"},\n",
      "    \"Vilnius\": {\"location\": (54.687157, 25.279652), \"country\": \"LT\"},\n",
      "    \"Kaunas\": {\"location\": (54.898521, 23.903597), \"country\": \"LT\"},\n",
      "    \"Ljubljana\": {\"location\": (46.056946, 14.505751), \"country\": \"SI\"},\n",
      "    \"Maribor\": {\"location\": (46.554650, 15.646049), \"country\": \"SI\"},\n",
      "    \"Prague\": {\"location\": (50.075539, 14.437800), \"country\": \"CZ\"},\n",
      "    \"Brno\": {\"location\": (49.195060, 16.606837), \"country\": \"CZ\"},\n",
      "    \"Paris\": {\"location\": (48.856614, 2.352222), \"country\": \"FR\"},\n",
      "    \"Marseille\": {\"location\": (43.296482, 5.369780), \"country\": \"FR\"},\n",
      "    \"Riga\": {\"location\": (56.949649, 24.105186), \"country\": \"LV\"},\n",
      "    \"Daugavpils\": {\"location\": (55.874296, 26.536963), \"country\": \"LV\"},\n",
      "    \"Bucharest\": {\"location\": (44.426765, 26.102537), \"country\": \"RO\"},\n",
      "    \"Cluj-Napoca\": {\"location\": (46.771210, 23.623635), \"country\": \"RO\"},\n",
      "    \"Amsterdam\": {\"location\": (52.370216, 4.895168), \"country\": \"NL\"},\n",
      "    \"Rotterdam\": {\"location\": (51.920179, 4.481774), \"country\": \"NL\"},\n",
      "    \"Tallinn\": {\"location\": (59.436962, 24.753574), \"country\": \"EE\"},\n",
      "    \"Tartu\": {\"location\": (58.380624, 26.725056), \"country\": \"EE\"},\n",
      "    \"Budapest\": {\"location\": (47.497913, 19.040236), \"country\": \"HU\"},\n",
      "    \"Debrecen\": {\"location\": (47.531604, 21.627312), \"country\": \"HU\"},\n",
      "    \"Vienna\": {\"location\": (48.208176, 16.373819), \"country\": \"AT\"},\n",
      "    \"Graz\": {\"location\": (47.070714, 15.439504), \"country\": \"AT\"},\n",
      "    \"Helsinki\": {\"location\": (60.169856, 24.938379), \"country\": \"FI\"},\n",
      "    \"Turku\": {\"location\": (60.451810, 22.266630), \"country\": \"FI\"},\n",
      "    \"Warsaw\": {\"location\": (52.229676, 21.012229), \"country\": \"PL\"},\n",
      "    \"Lodz\": {\"location\": (51.759250, 19.455983), \"country\": \"PL\"},\n",
      "    \"Madrid\": {\"location\": (40.416775, -3.703790), \"country\": \"ES\"},\n",
      "    \"Barcelona\": {\"location\": (41.385064, 2.173403), \"country\": \"ES\"},\n",
      "    \"Lisbon\": {\"location\": (38.722252, -9.139337), \"country\": \"PT\"},\n",
      "    \"Porto\": {\"location\": (41.157944, -8.629105), \"country\": \"PT\"},\n",
      "    \"Oslo\": {\"location\": (59.913869, 10.752245), \"country\": \"NO\"},\n",
      "    \"Bergen\": {\"location\": (60.392050, 5.322050), \"country\": \"NO\"},\n",
      "    \"Sofia\": {\"location\": (42.697708, 23.321868), \"country\": \"BG\"},\n",
      "    \"Plovdiv\": {\"location\": (42.135407, 24.745290), \"country\": \"BG\"},\n",
      "    \"Rome\": {\"location\": (41.902782, 12.496366), \"country\": \"IT\"},\n",
      "    \"Milan\": {\"location\": (45.464204, 9.189982), \"country\": \"IT\"},\n",
      "    \"Zagreb\": {\"location\": (45.815011, 15.981919), \"country\": \"HR\"},\n",
      "    \"Split\": {\"location\": (43.508132, 16.440193), \"country\": \"HR\"},\n",
      "    \"Stockholm\": {\"location\": (59.329323, 18.068581), \"country\": \"SE\"},\n",
      "    \"Gothenburg\": {\"location\": (57.708870, 11.974560), \"country\": \"SE\"},\n",
      "    \"Athens\": {\"location\": (37.983810, 23.727539), \"country\": \"GR\"},\n",
      "    \"Thessaloniki\": {\"location\": (40.640060, 22.944420), \"country\": \"GR\"},\n",
      "    \"Zürich\": {\"location\": (47.376887, 8.541694), \"country\": \"CH\"},\n",
      "    \"Geneva\": {\"location\": (46.204391, 6.143158), \"country\": \"CH\"},\n",
      "    \"Brussels\": {\"location\": (50.850340, 4.351710), \"country\": \"BE\"},\n",
      "    \"Antwerp\": {\"location\": (51.219448, 4.402464), \"country\": \"BE\"},\n",
      "    \"Berlin\": {\"location\": (52.520007, 13.404954), \"country\": \"DE\"},\n",
      "    \"Hamburg\": {\"location\": (53.551086, 9.993682), \"country\": \"DE\"}\n",
      "}\n",
      "75/53:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "LOCATIONS = {\n",
      "    \"Bratislava\": {\"location\": (48.148598, 17.107748), \"country\": \"SK\"},\n",
      "    \"Košice\": {\"location\": (48.720595, 21.257698), \"country\": \"SK\"},\n",
      "    \"Copenhagen\": {\"location\": (55.676097, 12.568337), \"country\": \"DK\"},\n",
      "    \"Aarhus\": {\"location\": (56.162939, 10.203921), \"country\": \"DK\"},\n",
      "    \"Vilnius\": {\"location\": (54.687157, 25.279652), \"country\": \"LT\"},\n",
      "    \"Kaunas\": {\"location\": (54.898521, 23.903597), \"country\": \"LT\"},\n",
      "    \"Ljubljana\": {\"location\": (46.056946, 14.505751), \"country\": \"SI\"},\n",
      "    \"Maribor\": {\"location\": (46.554650, 15.646049), \"country\": \"SI\"},\n",
      "    \"Prague\": {\"location\": (50.075539, 14.437800), \"country\": \"CZ\"},\n",
      "    \"Brno\": {\"location\": (49.195060, 16.606837), \"country\": \"CZ\"},\n",
      "    \"Paris\": {\"location\": (48.856614, 2.352222), \"country\": \"FR\"},\n",
      "    \"Marseille\": {\"location\": (43.296482, 5.369780), \"country\": \"FR\"},\n",
      "    \"Riga\": {\"location\": (56.949649, 24.105186), \"country\": \"LV\"},\n",
      "    \"Daugavpils\": {\"location\": (55.874296, 26.536963), \"country\": \"LV\"},\n",
      "    \"Bucharest\": {\"location\": (44.426765, 26.102537), \"country\": \"RO\"},\n",
      "    \"Cluj-Napoca\": {\"location\": (46.771210, 23.623635), \"country\": \"RO\"},\n",
      "    \"Amsterdam\": {\"location\": (52.370216, 4.895168), \"country\": \"NL\"},\n",
      "    \"Rotterdam\": {\"location\": (51.920179, 4.481774), \"country\": \"NL\"},\n",
      "    \"Tallinn\": {\"location\": (59.436962, 24.753574), \"country\": \"EE\"},\n",
      "    \"Tartu\": {\"location\": (58.380624, 26.725056), \"country\": \"EE\"},\n",
      "    \"Budapest\": {\"location\": (47.497913, 19.040236), \"country\": \"HU\"},\n",
      "    \"Debrecen\": {\"location\": (47.531604, 21.627312), \"country\": \"HU\"},\n",
      "    \"Vienna\": {\"location\": (48.208176, 16.373819), \"country\": \"AT\"},\n",
      "    \"Graz\": {\"location\": (47.070714, 15.439504), \"country\": \"AT\"},\n",
      "    \"Helsinki\": {\"location\": (60.169856, 24.938379), \"country\": \"FI\"},\n",
      "    \"Turku\": {\"location\": (60.451810, 22.266630), \"country\": \"FI\"},\n",
      "    \"Warsaw\": {\"location\": (52.229676, 21.012229), \"country\": \"PL\"},\n",
      "    \"Lodz\": {\"location\": (51.759250, 19.455983), \"country\": \"PL\"},\n",
      "    \"Madrid\": {\"location\": (40.416775, -3.703790), \"country\": \"ES\"},\n",
      "    \"Barcelona\": {\"location\": (41.385064, 2.173403), \"country\": \"ES\"},\n",
      "    \"Lisbon\": {\"location\": (38.722252, -9.139337), \"country\": \"PT\"},\n",
      "    \"Porto\": {\"location\": (41.157944, -8.629105), \"country\": \"PT\"},\n",
      "    \"Oslo\": {\"location\": (59.913869, 10.752245), \"country\": \"NO\"},\n",
      "    \"Bergen\": {\"location\": (60.392050, 5.322050), \"country\": \"NO\"},\n",
      "    \"Sofia\": {\"location\": (42.697708, 23.321868), \"country\": \"BG\"},\n",
      "    \"Plovdiv\": {\"location\": (42.135407, 24.745290), \"country\": \"BG\"},\n",
      "    \"Rome\": {\"location\": (41.902782, 12.496366), \"country\": \"IT\"},\n",
      "    \"Milan\": {\"location\": (45.464204, 9.189982), \"country\": \"IT\"},\n",
      "    \"Zagreb\": {\"location\": (45.815011, 15.981919), \"country\": \"HR\"},\n",
      "    \"Split\": {\"location\": (43.508132, 16.440193), \"country\": \"HR\"},\n",
      "    \"Stockholm\": {\"location\": (59.329323, 18.068581), \"country\": \"SE\"},\n",
      "    \"Gothenburg\": {\"location\": (57.708870, 11.974560), \"country\": \"SE\"},\n",
      "    \"Athens\": {\"location\": (37.983810, 23.727539), \"country\": \"GR\"},\n",
      "    \"Thessaloniki\": {\"location\": (40.640060, 22.944420), \"country\": \"GR\"},\n",
      "    \"Zürich\": {\"location\": (47.376887, 8.541694), \"country\": \"CH\"},\n",
      "    \"Geneva\": {\"location\": (46.204391, 6.143158), \"country\": \"CH\"},\n",
      "    \"Brussels\": {\"location\": (50.850340, 4.351710), \"country\": \"BE\"},\n",
      "    \"Antwerp\": {\"location\": (51.219448, 4.402464), \"country\": \"BE\"},\n",
      "    \"Berlin\": {\"location\": (52.520007, 13.404954), \"country\": \"DE\"},\n",
      "    \"Hamburg\": {\"location\": (53.551086, 9.993682), \"country\": \"DE\"}\n",
      "}\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "75/54:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "LOCATIONS = {\n",
      "    \"Bratislava\": {\"location\": (48.148598, 17.107748), \"country\": \"SK\"},\n",
      "    \"Košice\": {\"location\": (48.720595, 21.257698), \"country\": \"SK\"},\n",
      "    \"Copenhagen\": {\"location\": (55.676097, 12.568337), \"country\": \"DK\"},\n",
      "    \"Aarhus\": {\"location\": (56.162939, 10.203921), \"country\": \"DK\"},\n",
      "    \"Vilnius\": {\"location\": (54.687157, 25.279652), \"country\": \"LT\"},\n",
      "    \"Kaunas\": {\"location\": (54.898521, 23.903597), \"country\": \"LT\"},\n",
      "    \"Ljubljana\": {\"location\": (46.056946, 14.505751), \"country\": \"SI\"},\n",
      "    \"Maribor\": {\"location\": (46.554650, 15.646049), \"country\": \"SI\"},\n",
      "    \"Prague\": {\"location\": (50.075539, 14.437800), \"country\": \"CZ\"},\n",
      "    \"Brno\": {\"location\": (49.195060, 16.606837), \"country\": \"CZ\"},\n",
      "    \"Paris\": {\"location\": (48.856614, 2.352222), \"country\": \"FR\"},\n",
      "    \"Marseille\": {\"location\": (43.296482, 5.369780), \"country\": \"FR\"},\n",
      "    \"Riga\": {\"location\": (56.949649, 24.105186), \"country\": \"LV\"},\n",
      "    \"Daugavpils\": {\"location\": (55.874296, 26.536963), \"country\": \"LV\"},\n",
      "    \"Bucharest\": {\"location\": (44.426765, 26.102537), \"country\": \"RO\"},\n",
      "    \"Cluj-Napoca\": {\"location\": (46.771210, 23.623635), \"country\": \"RO\"},\n",
      "    \"Amsterdam\": {\"location\": (52.370216, 4.895168), \"country\": \"NL\"},\n",
      "    \"Rotterdam\": {\"location\": (51.920179, 4.481774), \"country\": \"NL\"},\n",
      "    \"Tallinn\": {\"location\": (59.436962, 24.753574), \"country\": \"EE\"},\n",
      "    \"Tartu\": {\"location\": (58.380624, 26.725056), \"country\": \"EE\"},\n",
      "    \"Budapest\": {\"location\": (47.497913, 19.040236), \"country\": \"HU\"},\n",
      "    \"Debrecen\": {\"location\": (47.531604, 21.627312), \"country\": \"HU\"},\n",
      "    \"Vienna\": {\"location\": (48.208176, 16.373819), \"country\": \"AT\"},\n",
      "    \"Graz\": {\"location\": (47.070714, 15.439504), \"country\": \"AT\"},\n",
      "    \"Helsinki\": {\"location\": (60.169856, 24.938379), \"country\": \"FI\"},\n",
      "    \"Turku\": {\"location\": (60.451810, 22.266630), \"country\": \"FI\"},\n",
      "    \"Warsaw\": {\"location\": (52.229676, 21.012229), \"country\": \"PL\"},\n",
      "    \"Lodz\": {\"location\": (51.759250, 19.455983), \"country\": \"PL\"},\n",
      "    \"Madrid\": {\"location\": (40.416775, -3.703790), \"country\": \"ES\"},\n",
      "    \"Barcelona\": {\"location\": (41.385064, 2.173403), \"country\": \"ES\"},\n",
      "    \"Lisbon\": {\"location\": (38.722252, -9.139337), \"country\": \"PT\"},\n",
      "    \"Porto\": {\"location\": (41.157944, -8.629105), \"country\": \"PT\"},\n",
      "    \"Oslo\": {\"location\": (59.913869, 10.752245), \"country\": \"NO\"},\n",
      "    \"Bergen\": {\"location\": (60.392050, 5.322050), \"country\": \"NO\"},\n",
      "    \"Sofia\": {\"location\": (42.697708, 23.321868), \"country\": \"BG\"},\n",
      "    \"Plovdiv\": {\"location\": (42.135407, 24.745290), \"country\": \"BG\"},\n",
      "    \"Rome\": {\"location\": (41.902782, 12.496366), \"country\": \"IT\"},\n",
      "    \"Milan\": {\"location\": (45.464204, 9.189982), \"country\": \"IT\"},\n",
      "    \"Zagreb\": {\"location\": (45.815011, 15.981919), \"country\": \"HR\"},\n",
      "    \"Split\": {\"location\": (43.508132, 16.440193), \"country\": \"HR\"},\n",
      "    \"Stockholm\": {\"location\": (59.329323, 18.068581), \"country\": \"SE\"},\n",
      "    \"Gothenburg\": {\"location\": (57.708870, 11.974560), \"country\": \"SE\"},\n",
      "    \"Athens\": {\"location\": (37.983810, 23.727539), \"country\": \"GR\"},\n",
      "    \"Thessaloniki\": {\"location\": (40.640060, 22.944420), \"country\": \"GR\"},\n",
      "    \"Zürich\": {\"location\": (47.376887, 8.541694), \"country\": \"CH\"},\n",
      "    \"Geneva\": {\"location\": (46.204391, 6.143158), \"country\": \"CH\"},\n",
      "    \"Brussels\": {\"location\": (50.850340, 4.351710), \"country\": \"BE\"},\n",
      "    \"Antwerp\": {\"location\": (51.219448, 4.402464), \"country\": \"BE\"},\n",
      "    \"Berlin\": {\"location\": (52.520007, 13.404954), \"country\": \"DE\"},\n",
      "    \"Hamburg\": {\"location\": (53.551086, 9.993682), \"country\": \"DE\"}\n",
      "}\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2022,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "75/55:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/56:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    [city['location'] for city in LOCATIONS],\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/57:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    [city_value['location'] for city_name, city_value in LOCATIONS.items()],\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/58:\n",
      "# Add city name and country to weather dataframe (based on lat/lon)\n",
      "for _df in [w_daily, w_hourly]:\n",
      "    # Select city based on lat/lon\n",
      "    _df['city'] = _df.apply(lambda x: [city_name for city_name, city_value in LOCATIONS.items() if city_value['location'] == (x['lat'], x['lon'])][0], axis=1)\n",
      "    # Select country based on city\n",
      "    _df['country'] = _df.apply(lambda x: LOCATIONS[x['city']]['country'], axis=1)\n",
      "\n",
      "w_daily\n",
      "75/59:\n",
      "# Add city name and country to weather dataframe (based on lat/lon)\n",
      "for _df in [w_daily, w_hourly]:\n",
      "    # Select city based on lat/lon\n",
      "    _df['city'] = _df.apply(lambda x: [city_name for city_name, city_value in LOCATIONS.items() if city_value['location'] == (x['latitude'], x['longitude'])][0], axis=1)\n",
      "    # Select country based on city\n",
      "    _df['country'] = _df.apply(lambda x: LOCATIONS[x['city']]['country'], axis=1)\n",
      "\n",
      "w_daily\n",
      "75/60:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    [location_eindhoven],\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/61:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection: {}\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres: {}\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "75/62:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    [location_eindhoven],\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "75/63:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\"}\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2022,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "75/64:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\"}\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2022,12,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "75/65:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    [city_value['location'] for city_name, city_value in LOCATIONS.items()],\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/66:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/67: w_hourly\n",
      "75/68:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(15, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    ax.plot(w_hourly['temperature_2m'].loc[city], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/69:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(15, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    ax.plot(w_hourly['temperature_2m'][w_hourly['city'] == city], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/70:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    ax.plot(w_hourly['temperature_2m'][w_hourly['city'] == city], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/71:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    ax.plot(w_hourly[w_hourly['city']['temperature_2m'] == city], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/72:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    ax.plot(w_hourly[w_hourly['city'] == city]['temperature_2m'], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/73:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    ax.plot(w_hourly[w_hourly['city'] == city['city']]['temperature_2m'], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/74:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_daily[w_daily['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/75:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(10, 5))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/76:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(20, 3))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "ax.legend()\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/77:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(20, 3))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "\n",
      "# Horizontal legend\n",
      "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/78:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(20, 3))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "\n",
      "# Horizontal legend\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=5)\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/79:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(20, 3))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "\n",
      "# Horizontal legend\n",
      "ax.legend(loc='upper center', fancybox=True, shadow=True, ncol=5)\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/80:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(20, 3))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "\n",
      "# Horizontal legend outside the plot\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
      "#ax.legend(loc='upper center', fancybox=True, shadow=True, ncol=5)\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/81:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2022,12,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "75/82:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/83: w_hourly\n",
      "75/84:\n",
      "# Visualize the different temperatures in the different cities\n",
      "fig, ax = plt.subplots(figsize=(20, 3))\n",
      "for i, city in enumerate(LOCATIONS):\n",
      "    _df = w_hourly[w_hourly['city'] == city['city']]\n",
      "    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])\n",
      "\n",
      "# Horizontal legend outside the plot\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)\n",
      "ax.set_ylabel('Temperature (C)')\n",
      "ax.set_title('Temperature in different cities')\n",
      "plt.show()\n",
      "75/85:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_daily, col=\"region\", col_wrap=3, height=3, aspect=1.5)\n",
      "graph.map(plt.plot, \"date\", \"temperature_2m_max\", color=\"red\")\n",
      "graph.map(plt.plot, \"date\", \"temperature_2m_min\", color=\"blue\")\n",
      "graph.set_axis_labels(\"Date\", \"Temperature (C)\")\n",
      "graph.set_titles(\"{col_name}\")\n",
      "graph.fig.suptitle(\"Daily temperature in different cities\", y=1.05)\n",
      "graph.fig.tight_layout()\n",
      "graph.fig.subplots_adjust(top=0.9)\n",
      "graph\n",
      "75/86:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_daily, col=\"region\", col_wrap=3, height=3, aspect=1.5)\n",
      "graph.map(plt.plot, \"date\", \"temperature_2m_max\", color=\"red\")\n",
      "graph.map(plt.plot, \"date\", \"temperature_2m_min\", color=\"blue\")\n",
      "graph.set_axis_labels(\"Date\", \"Temperature (C)\")\n",
      "graph.set_titles(\"{col_name}\")\n",
      "graph.fig.suptitle(\"Daily temperature in different cities\", y=1.05)\n",
      "graph.fig.tight_layout()\n",
      "graph.fig.subplots_adjust(top=0.9)\n",
      "graph\n",
      "75/87:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_daily, col=\"region\", col_wrap=3, height=3, aspect=1.5)\n",
      "graph.map(plt.plot, \"date\", \"temperature_2m_max\", color=\"red\")\n",
      "graph.map(plt.plot, \"date\", \"temperature_2m_min\", color=\"blue\")\n",
      "graph.set_axis_labels(\"Date\", \"Temperature (C)\")\n",
      "graph.fig.suptitle(\"Daily temperature in different cities\", y=1.05)\n",
      "graph.fig.tight_layout()\n",
      "graph.fig.subplots_adjust(top=0.9)\n",
      "graph\n",
      "75/88:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_daily, col=\"region\", col_wrap=3, height=3, aspect=1.5)\n",
      "graph.map(plt.plot, \"time\", \"temperature_2m_max\", color=\"red\")\n",
      "graph.map(plt.plot, \"time\", \"temperature_2m_min\", color=\"blue\")\n",
      "graph.set_axis_labels(\"Date\", \"Temperature (C)\")\n",
      "graph.set_titles(\"{col_name}\")\n",
      "graph.fig.suptitle(\"Daily temperature in different cities\", y=1.05)\n",
      "graph.fig.tight_layout()\n",
      "graph.fig.subplots_adjust(top=0.9)\n",
      "graph\n",
      "75/89:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_hourly, col=\"region\", hue = 'city', col_wrap=3, height=3, aspect=1.5)\n",
      "# map the above form facetgrid with some attributes\n",
      "graph.map(plt.scatter, \"temperature_2m\", \"tip\", edgecolor =\"w\").add_legend()\n",
      "plt.show()\n",
      "75/90:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_hourly, col=\"region\", hue = 'city', col_wrap=3, height=3, aspect=1.5)\n",
      "# map the above form facetgrid with some attributes\n",
      "graph.map(plt.scatter, \"temperature_2m\", edgecolor =\"w\").add_legend()\n",
      "plt.show()\n",
      "75/91:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_hourly, col=\"region\", hue = 'city', col_wrap=3, height=3, aspect=1.5)\n",
      "# map the above form facetgrid with some attributes\n",
      "graph.map(plt.scatter, 'time',\"temperature_2m\", edgecolor =\"w\").add_legend()\n",
      "plt.show()\n",
      "75/92:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_hourly, col=\"region\", hue = 'city', col_wrap=3, height=3, aspect=1.5)\n",
      "# map the above form facetgrid with some attributes\n",
      "graph.map(plt.plot, 'time',\"temperature_2m\", edgecolor =\"w\").add_legend()\n",
      "plt.show()\n",
      "75/93:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "graph = sns.FacetGrid(w_hourly, col=\"region\", hue = 'city', col_wrap=3, height=3, aspect=1.5)\n",
      "# map the above form facetgrid with some attributes\n",
      "graph.map(plt.plot, 'time',\"temperature_2m\").add_legend()\n",
      "plt.show()\n",
      "75/94:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set_style(\"ticks\")\n",
      "g = sns.FacetGrid(w_hourly,x=\"time\",y=\"temperature_2m\", col=\"region\", hue = 'city', col_wrap=3,\n",
      "                dodge=False, \n",
      "                aspect = 1,\n",
      "                sharey = True,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               ).despine(left=True)\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(w_hourly['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['serial'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/95:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set_style(\"ticks\")\n",
      "g = sns.catplot(w_hourly,x=\"time\",y=\"temperature_2m\", col=\"region\", hue = 'city', col_wrap=3,\n",
      "                dodge=False, \n",
      "                aspect = 1,\n",
      "                sharey = True,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               ).despine(left=True)\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(w_hourly['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['serial'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/96:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set_style(\"ticks\")\n",
      "g = sns.catplot(w_hourly,x=\"time\",y=\"temperature_2m\", col=\"region\", hue = 'city', col_wrap=3,\n",
      "                dodge=False, \n",
      "                aspect = 1,\n",
      "                sharey = True,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               ).despine(left=True)\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(w_hourly['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['region'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/97:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "g = sns.catplot(w_hourly,x=\"time\",y=\"temperature_2m\", col=\"region\", hue = 'city', col_wrap=3,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               ).despine(left=True)\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(w_hourly['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['city'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/98:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "g = sns.FacetGrid(w_daily, col=\"region\", hue = 'city', col_wrap=3,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               )\n",
      "g.map(plt.plot, \"time\", \"temperature_2m_max\")\n",
      "\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(w_daily['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), w_daily.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['city'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/99:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df, col=\"region\", hue='city', col_wrap=3,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               )\n",
      "g.map(plt.plot, \"time\", \"temperature_2m_max\")\n",
      "\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(df['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), df.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['city'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/100:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df, col=\"region\", hue='city', col_wrap=3,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               )\n",
      "g.map(plt.plot, \"time\", \"temperature_2m_max\")\n",
      "\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(df['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), df.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['city'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "        print(idx)\n",
      "       axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/102:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df, col=\"region\", hue='city', col_wrap=3,\n",
      "                legend_out = False,               # REMOVE MASTER LEGEND\n",
      "               )\n",
      "g.map(plt.plot, \"time\", \"temperature_2m_max\")\n",
      "\n",
      "# MASTER SERIES OF serial\n",
      "ser_vals = pd.Series(df['city'].sort_values().unique())\n",
      "\n",
      "for axes, (i, d) in zip(g.axes.ravel(), df.groupby(['region'])):\n",
      "    handles, labels = axes.get_legend_handles_labels()\n",
      "\n",
      "    # SUBSET MASTER SERIES OF serial\n",
      "    vals = ser_vals[ser_vals.isin(d['city'].unique())]    \n",
      "    idx = vals.index.tolist()\n",
      "\n",
      "    if len(idx) > 0:\n",
      "        print(idx)\n",
      "        axes.legend(handles = [handles[i] for i in idx], \n",
      "                   labels = vals.tolist())\n",
      "\n",
      "plt.show()\n",
      "75/103:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df, col=\"region\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"region\", \"city\"]):\n",
      "    ax = g.axes[g.row_names.index(row_var), g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "plt.show()\n",
      "75/104:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df, col=\"region\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"region\"]):\n",
      "    ax = g.axes[g.row_names.index(row_var), g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "plt.show()\n",
      "75/105:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df,col=\"region\")\n",
      "for (col_var), facet_df in df.groupby([\"region\"]):\n",
      "    ax = g.axes[g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/106:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df,col=\"region\")\n",
      "for (col_var), facet_df in df.groupby([\"region\"]):\n",
      "    ax = g.axes['time',g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/107:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "g = sns.FacetGrid(df,col=\"region\")\n",
      "for (col_var), facet_df in df.groupby([\"region\"]):\n",
      "    ax = g.axes[None,g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/108:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "\n",
      "g = sns.FacetGrid(df, row='region',col=\"region\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"region\", \"region\"]):\n",
      "    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/109:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "df['area'] = 'EU'\n",
      "g = sns.FacetGrid(df, row='region',col=\"area\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"region\", \"area\"]):\n",
      "    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    #sns.swarmplot(x=\"time\", y=\"total_bill\", hue=\"size\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/110:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "df['area'] = 'EU'\n",
      "g = sns.FacetGrid(df, row='region',col=\"area\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"region\", \"area\"]):\n",
      "    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/111:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "df['area'] = 'EU'\n",
      "g = sns.FacetGrid(df, row='area',col=\"region\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"area\", \"region\"]):\n",
      "    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/112:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "df['area'] = 'EU'\n",
      "g = sns.FacetGrid(df, row='area',col=\"region\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"area\", \"region\"]):\n",
      "    ax = g.axes[g.row_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/113:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "df['area'] = 'EU'\n",
      "g = sns.FacetGrid(df, row='region',col=\"area\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"region\", \"area\"]):\n",
      "    ax = g.axes[g.row_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/114:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "\n",
      "\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharex=True, sharey=True)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/115:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(5, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/116:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/117:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/118:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 4, title=\"\")\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/119:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title=\"\", nrows= 4)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/120:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title=\"\", nrow = 4)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/121:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot. 4 rows\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title=\"\", ncol=4)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/122:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot.\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title=\"\", ncols = 2)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/123:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "# Visualize the different temperatures in the different cities. \n",
      "# Matplotlib\n",
      "facet_plots = []\n",
      "fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)\n",
      "for i, (region, region_df) in enumerate(df.groupby('region')):\n",
      "    ax = axes[i]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\",hue='city', data=region_df, ax=ax)\n",
      "    # Attach the legend to the right of the plot, outside of the plot.\n",
      "    ax.legend(bbox_to_anchor=(1.05, 1), loc=0, borderaxespad=0., title=\"\", ncols = 2)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "    ax.set_title(region)\n",
      "    facet_plots.append(ax)\n",
      "75/124:\n",
      "# Visualize the different temperatures in the different cities. Facet the plot by region\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)\n",
      "df['area'] = 'EU'\n",
      "g = sns.FacetGrid(df, row='area',col=\"region\")\n",
      "for (row_var, col_var), facet_df in df.groupby([\"area\", \"region\"]):\n",
      "    ax = g.axes[g.row_names.index(row_var),g.col_names.index(col_var)]\n",
      "    sns.lineplot(x=\"time\", y=\"temperature_2m_max\", data=facet_df, ax=ax)\n",
      "    ax.set(xlabel=\"\", ylabel=\"\")\n",
      "\n",
      "g.set_axis_labels(\"time\", \"Temperature (C)\")\n",
      "g.set_titles()\n",
      "75/125:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "\n",
      "facet_plots = []\n",
      "for region in set([l['region'] for l in LOCATIONS]):\n",
      "    g = sns.FacetGrid(\n",
      "            w_daily[w_daily['region'] == region], \n",
      "            col='region', \n",
      "            hue='city', \n",
      "            col_wrap=3, \n",
      "            height=3, \n",
      "            aspect=1.5\n",
      "        )\n",
      "    g.map(sns.lineplot, 'date', 'temperature_2m_max')\n",
      "    # Add a legend\n",
      "    g.add_legend()\n",
      "    facet_plots.append(g)\n",
      "    \n",
      "plt.show()\n",
      "75/126:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "\n",
      "facet_plots = []\n",
      "for region in set([l['region'] for l in LOCATIONS]):\n",
      "    g = sns.FacetGrid(\n",
      "            w_daily[w_daily['region'] == region], \n",
      "            col='region', \n",
      "            hue='city', \n",
      "            col_wrap=3, \n",
      "            height=3, \n",
      "            aspect=1.5\n",
      "        )\n",
      "    g.map(sns.lineplot, 'time', 'temperature_2m_max')\n",
      "    # Add a legend\n",
      "    g.add_legend()\n",
      "    facet_plots.append(g)\n",
      "    \n",
      "plt.show()\n",
      "75/127:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20))\n",
      "for i, region in enumerate(REGIONS):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='date', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('Date')\n",
      " \n",
      "plt.show()\n",
      "75/128:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20))\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='date', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('Date')\n",
      " \n",
      "plt.show()\n",
      "75/129:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20))\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('Date')\n",
      " \n",
      "plt.show()\n",
      "75/130:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      " \n",
      "plt.show()\n",
      "75/131:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      " \n",
      "plt.show()\n",
      "75/132:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "    fig.subplots_adjust(hspace=0.5)\n",
      " \n",
      "plt.show()\n",
      "75/133:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      " \n",
      "plt.show()\n",
      "75/134:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      " \n",
      "plt.show()\n",
      "75/135:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/136:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2022,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "75/137:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/138:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/139:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(5, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/140:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 5), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/141:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 15), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/142:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/143:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    axes[i].lines[0].set_linewidth(1)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/144:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    axes[i].lines[0].set_linewidth(0.75)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/145:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    axes[i].lines[0].set_linewidth(0.75)\n",
      "    # Smaller alpha\n",
      "    axes[i].lines[0].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/146:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    axes[i].lines[0].set_linewidth(0.5)\n",
      "    # Smaller alpha\n",
      "    axes[i].lines[0].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/147:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    axes[i].lines[0].set_linewidth(0.5)\n",
      "    # Make sure the hue is seethrough\n",
      "    axes[i].lines[0].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/148:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    axes[i].lines[i].set_linewidth(0.5) for i in range(1, len(axes[i].lines))\n",
      "    # Make sure the hue is seethrough\n",
      "    axes[i].lines[i].set_alpha(0.5) for i in range(1, len(axes[i].lines))\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/149:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    [axes[i].lines[i].set_linewidth(0.5) for i in range(1, len(axes[i].lines))]\n",
      "    # Make sure the hue is seethrough\n",
      "    [axes[i].lines[i].set_alpha(0.5) for i in range(1, len(axes[i].lines))]\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/150:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for i in range(1, len(axes[i].lines)):\n",
      "        axes[i].lines[i].set_linewidth(0.5)\n",
      "        axes[i].lines[i].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/151:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for i in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[i].set_linewidth(0.5)\n",
      "        axes[i].lines[i].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/152:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[j].lines)):\n",
      "        axes[i].lines[j].set_linewidth(0.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/153:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(0.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/154:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1)\n",
      "        axes[i].lines[j].set_alpha(0.75)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/155:\n",
      "import seaborn as sns\n",
      "\n",
      "# Plot the daily temperature in matplotlib by region, hue by city\n",
      "# Multiplot 7 pltos\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/156:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_daily.copy()\n",
      "df['label'] = df['city'] + '(' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(w_daily['region'].unique()):\n",
      "    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/157:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_daily.copy()\n",
      "df['label'] = df['city'] + '(' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/158:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_daily.copy()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/159:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_daily.copy()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/160:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/161:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('W', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/162:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('W', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/163:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('W', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/164:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=1)\n",
      " \n",
      "plt.show()\n",
      "75/165:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      " \n",
      "plt.show()\n",
      "75/166:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=20)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "75/167:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=20)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "75/168:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=20)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/169:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/170:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/171:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/172:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/173:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    title = old_legend.get_title().get_text()\n",
      "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 'upper right', bbox_to_anchor=(1.05, 1), ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/174:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    title = old_legend.get_title().get_text()\n",
      "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 'upper right', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    \n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/175:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    title = old_legend.get_title().get_text()\n",
      "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], loc=2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/176:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    title = old_legend.get_title().get_text()\n",
      "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/177:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    title = old_legend.get_title().get_text()\n",
      "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., name = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/178:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    title = old_legend.get_title().get_text()\n",
      "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/179:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/180:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/181:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "75/182:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "75/183:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Wind Speed (m/s)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/184:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Wind Speed (m/s)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/185:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/186:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/187:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/188:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "df['windspeed_10m'] = np.log(df['windspeed_10m'])\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/189:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'])\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/190:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'] if df['windspeed_10m'] > 0 else 0.0001)\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/191:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m']) if df['windspeed_10m'] > 0 else 0\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/192:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/193:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')s\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/194:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/195:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/196:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5, stack = True)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/197:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5,  multiple = 'stack')\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/198:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "#df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5,  multiple = 'stack')\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/199:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "#df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.75)\n",
      "plt.show()\n",
      "75/200:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "#df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "75/201:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Wind Speed (m/s)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "75/202:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "75/203:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ({country_list})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ({country_list})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys())\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys())\n",
      "75/204:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ({country_list})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ({country_list})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys().to_list())\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys().to_list())\n",
      "75/205:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ({country_list})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ({country_list})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/206:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ({','.join(country_list)})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ({','.join(country_list)})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/207:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ({','.join(country_list)})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ({','.join(country_list)})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/208:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ({\"','\".join(country_list)})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ({\"','\".join(country_list)})\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/209:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/210:\n",
      "import seaborn as sns\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['time', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='time').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'time']).sum().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    sns.lineplot(data=df[df['country_code'] == country], x='time', y='generation', hue='type', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/211:\n",
      "import seaborn as sns\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='time').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/212:\n",
      "import seaborn as sns\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/213:\n",
      "import seaborn as sns\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Area plot with fill\n",
      "    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], palette='Set1', fill=True, alpha = 0.5, multiple = 'stack')\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/214:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Area plot with fill\n",
      "    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], palette='Set1', alpha = 0.5, multiple = 'stack')\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/215:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Area plot with fill\n",
      "    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], palette='Set1', alpha = 0.5, multiple = 'stack')\n",
      "    # Area fill\n",
      "    axes[i].fill_between(df[df['country_code'] == country]['timestamp'], df[df['country_code'] == country]['generation'], alpha=0.5)\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/216:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Area plot with fill\n",
      "    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], alpha = 0.5, multiple = 'stack')\n",
      "    # Area fill\n",
      "    axes[i].fill_between(df[df['country_code'] == country]['timestamp'], df[df['country_code'] == country]['generation'], alpha=0.5)\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/217:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Multiple stack\n",
      "    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], alpha = 0.5, )\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/218:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Multiple stack\n",
      "    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i] )\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/219:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_generation_production.copy()\n",
      "# Pivot wide to long\n",
      "df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')\n",
      "# groupby \n",
      "df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()\n",
      "# Sum all price areas\n",
      "df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()\n",
      "\n",
      "country_length = len(df['country_code'].unique())\n",
      "fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)\n",
      "for i, country in enumerate(df['country_code'].unique()):\n",
      "    # Multiple stack\n",
      "    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type' )\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(country)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Generation (MW)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/220:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='generation', hue='price_area', ax=axes[i] )\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes[i].lines)):\n",
      "    axes[i].lines[j].set_linewidth(1.5)\n",
      "    axes[i].lines[j].set_alpha(0.5)\n",
      "axes[i].set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes[i].set_ylabel('Price (EUR/MWh)')\n",
      "axes[i].set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes[i].tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/221:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='generation', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes[i].lines)):\n",
      "    axes[i].lines[j].set_linewidth(1.5)\n",
      "    axes[i].lines[j].set_alpha(0.5)\n",
      "axes[i].set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes[i].set_ylabel('Price (EUR/MWh)')\n",
      "axes[i].set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes[i].tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/222:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes[i].lines)):\n",
      "    axes[i].lines[j].set_linewidth(1.5)\n",
      "    axes[i].lines[j].set_alpha(0.5)\n",
      "axes[i].set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes[i].set_ylabel('Price (EUR/MWh)')\n",
      "axes[i].set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes[i].tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/223:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "print(df)\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes[i].lines)):\n",
      "    axes[i].lines[j].set_linewidth(1.5)\n",
      "    axes[i].lines[j].set_alpha(0.5)\n",
      "axes[i].set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes[i].set_ylabel('Price (EUR/MWh)')\n",
      "axes[i].set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes[i].tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/224:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_day_ahead_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/225:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "print(df)\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes[i].lines)):\n",
      "    axes[i].lines[j].set_linewidth(1.5)\n",
      "    axes[i].lines[j].set_alpha(0.5)\n",
      "axes[i].set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes[i].set_ylabel('Price (EUR/MWh)')\n",
      "axes[i].set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes[i].tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/226:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/227:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "print(df)\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes[i].lines)):\n",
      "    axes[i].lines[j].set_linewidth(1.5)\n",
      "    axes[i].lines[j].set_alpha(0.5)\n",
      "axes[i].set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes[i].set_ylabel('Price (EUR/MWh)')\n",
      "axes[i].set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes[i].tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/228:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='time').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/229:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/230:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(5, 20), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/231:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 10), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/232:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(10, 5), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/233:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(20, 5), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/234:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(12, 5), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "75/235:\n",
      "# Get the data for France\n",
      "country_of_interest = 'FR'\n",
      "# Get the data for the price area of interest\n",
      "da_prices_country = hourly_entsoe_day_ahead_prices[hourly_entsoe_day_ahead_prices['country_code'] == country_of_interest]\n",
      "production_country = hourly_entsoe_generation_production[hourly_entsoe_generation_production['country_code'] == country_of_interest]\n",
      "weather_country = w_hourly[w_hourly['country'] == country_of_interest]\n",
      "\n",
      "# Visualize the wind speed, wind power production and day ahead prices\n",
      "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
      "# Plot the wind speed\n",
      "sns.lineplot(data=weather_country, x='time', y='windspeed_10m', ax=axes[0])\n",
      "axes[0].set_title('Wind Speed')\n",
      "axes[0].set_ylabel('Wind Speed (m/s)')\n",
      "# Plot the wind power production\n",
      "sns.lineplot(data=production_country, x='time', y='wind_onshore_generation_actual', ax=axes[1])\n",
      "axes[1].set_title('Wind Power Production')\n",
      "axes[1].set_ylabel('Wind Power Production (MW)')\n",
      "# Plot the day ahead prices\n",
      "sns.lineplot(data=da_prices_country, x='timestamp', y='price', ax=axes[2])\n",
      "axes[2].set_title('Day Ahead Prices')\n",
      "75/236:\n",
      "hourly_entsoe_day_ahead_prices.columns\n",
      "w_hourly.columns\n",
      "hourly_entsoe_generation_production.columns\n",
      "75/237:\n",
      "print(hourly_entsoe_day_ahead_prices.columns)\n",
      "print(w_hourly.columns)\n",
      "print(hourly_entsoe_generation_production.columns)\n",
      "75/238:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/239:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "df = df[df['country_code'] == 'DE']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/240:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "df = df[df['country_code'] == 'NL']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/241:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "df = df[df['country_code'] == ['NL','DE']]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/242:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "df = df[df['country_code'].isin['NL','DE']]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/243:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "df = df[df['country_code'] in ['NL','DE']]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/244:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'month', 'country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'month', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'month', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'month', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'month', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'month', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/245:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'hour', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/246:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g['wind_mean'] = g['wind_mean'].astype(float)\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'hour', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/247:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'hour', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/248:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', grouped['wind_mean'] - grouped['wind_std'], grouped['wind_mean'] + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/249:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Flatten the column index\n",
      "grouped.columns = grouped.columns.get_level_values(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', grouped['wind_mean'] - grouped['wind_std'], grouped['wind_mean'] + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/250:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped.columns)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', grouped['wind_mean'] - grouped['wind_std'], grouped['wind_mean'] + grouped['wind_std'], alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/251:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped.columns)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/252:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped.columns)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/253:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped.columns)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/254:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped.columns)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Show lines\n",
      "g.map(plt.axhline, y=0, ls=\":\", c=\".5\")\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/255:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/256:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped)\n",
      "print(grouped['hour'].unique())\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/257:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "print(grouped)\n",
      "print(grouped['hour'].unique())\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/258:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/259:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate mean and std\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std'],\n",
      "    'solar': ['mean', 'std'],\n",
      "    'other_renewable': ['mean', 'std'],\n",
      "    'fossil_gas': ['mean', 'std'],\n",
      "    'price': ['mean', 'std']\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Generate the mean - std and mean + std columns\n",
      "for col in grouped.columns:\n",
      "    if 'mean' in col:\n",
      "        grouped[col.replace('mean', 'min')] = grouped[col] - grouped[col.replace('mean', 'std')]\n",
      "        grouped[col.replace('mean', 'max')] = grouped[col] + grouped[col.replace('mean', 'std')]\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_min', 'wind_max', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/260:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "#  The lines represent the median, the dark shading represents the inner 50% of observations (25th to 75th percentile) and the light shading represents the outer 50% of observations (0th to 100th percentile) of the daily averaged value for that same day in each of the 39 years of record\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['mean', 'std', 'min', 'max', '25%', '75%'],\n",
      "    'solar': ['mean', 'std', 'min', 'max', '25%', '75%'],\n",
      "    'other_renewable': ['mean', 'std', 'min', 'max', '25%', '75%'],\n",
      "    'fossil_gas': ['mean', 'std', 'min', 'max', '25%', '75%'],\n",
      "    'price': ['mean', 'std', 'min', 'max', '25%', '75%']\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',\n",
      "                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']\n",
      "\n",
      "# Generate the mean - std and mean + std columns\n",
      "for col in grouped.columns:\n",
      "    if 'mean' in col:\n",
      "        grouped[col.replace('mean', 'min')] = grouped[col] - grouped[col.replace('mean', 'std')]\n",
      "        grouped[col.replace('mean', 'max')] = grouped[col] + grouped[col.replace('mean', 'std')]\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_min', 'wind_max', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/261:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "print(grouped.head())\n",
      "\n",
      "# Generate the mean - std and mean + std columns\n",
      "for col in grouped.columns:\n",
      "    if 'mean' in col:\n",
      "        grouped[col.replace('mean', 'min')] = grouped[col] - grouped[col.replace('mean', 'std')]\n",
      "        grouped[col.replace('mean', 'max')] = grouped[col] + grouped[col.replace('mean', 'std')]\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_mean', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_min', 'wind_max', alpha=0.1, color='blue')\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/262:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/263:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "print(grouped.columns)\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/264:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns if col[1] != '']\n",
      "print(grouped.columns)\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/265:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns if col[1] != '']\n",
      "print(grouped.columns)\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/266:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "print(grouped.columns)\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code_', col='quarter_', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/267:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/268:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshoremedian', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wwind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/269:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.5, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wwind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/270:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.5)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.9)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/271:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_mean', color='yellow')\n",
      "#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'renewable_mean', color='green')\n",
      "#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'fossil_mean', color='red')\n",
      "#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_mean', color='black')\n",
      "#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')\n",
      "\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/272:\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "g.map(plt.twinx)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/273:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "75/274:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices,hourly_entsoe_day_ahead_forecast, on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "g.map(plt.twinx)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/275:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast, on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "g.map(plt.twinx)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/276:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "g.map(plt.twinx)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/277:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/278:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/279:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['month','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'month'})\n",
      "grouped_months['quarter'] = None\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/280:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['month','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'month'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'hour', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/281:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['month','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/282:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
      "\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['month','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/283:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['month','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/284:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/285:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months])\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/286:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/287:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/288:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "print(grouped)\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/289:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Replace NaN values with 0\n",
      "df = df.fillna(0)\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):\n",
      "    # Return the 25th percentile as a float value\n",
      "    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "75/290:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "77/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "77/2:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(20,3))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "77/3:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "77/4:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "77/5:\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "77/6:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "79/1:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "79/2:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "79/3:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "79/4:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "79/5:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "79/6:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "79/7:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "80/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "80/2:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "80/3:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "80/4:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "80/5:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "print(grouped.columns)\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "81/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "81/2:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "81/3:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "81/4:\n",
      "# Print numpy version\n",
      "print(np.__version__)\n",
      "import seaborn as sns\n",
      "print(sns.__version__)\n",
      "81/5:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "print(grouped.columns)\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "81/6:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Convert median,std,min,max,q25,q75 columns to float64\n",
      "for col in grouped.columns[3:]:\n",
      "    grouped[col] = grouped[col].astype('float64')\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "81/7:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Convert median,std,min,max,q25,q75 columns to float64\n",
      "for col in grouped.columns[3:]:\n",
      "    grouped[col] = grouped[col].astype('float')\n",
      "# Replace NaN values with 0\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "81/8:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "81/9:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "81/10:\n",
      "# Print numpy version\n",
      "print(np.__version__)\n",
      "import seaborn as sns\n",
      "print(sns.__version__)\n",
      "82/1:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "82/2:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "82/3:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "82/4:\n",
      "# Print numpy version\n",
      "print(np.__version__)\n",
      "import seaborn as sns\n",
      "print(sns.__version__)\n",
      "82/5:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Convert median,std,min,max,q25,q75 columns to float64\n",
      "for col in grouped.columns[3:]:\n",
      "    grouped[col] = grouped[col].astype('float')\n",
      "# Replace NaN values with 0 np.nan\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/6:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Convert median,std,min,max,q25,q75 columns to float64\n",
      "for col in grouped.columns[3:]:\n",
      "    grouped[col] = grouped[col].astype('float')\n",
      "# Replace NaN values with 0 np.nan\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/7:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 0\n",
      "\n",
      "# Concatenate the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Convert median,std,min,max,q25,q75 columns to float64\n",
      "for col in grouped.columns[3:]:\n",
      "    grouped[col] = grouped[col].astype('float')\n",
      "# Replace NaN values with 0 np.nan\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_months, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/8:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Bind the two dataframes\n",
      "grouped = pd.concat([grouped_quarters, grouped_months], axis=0)\n",
      "# Convert median,std,min,max,q25,q75 columns to float64\n",
      "for col in grouped.columns[3:]:\n",
      "    grouped[col] = grouped[col].astype('float')\n",
      "# Replace NaN values with 0 np.nan\n",
      "grouped = grouped.fillna(0)\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/9:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE'])]\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/10:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/11:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "#df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/12:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/13:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/14:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "#for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "#    df[col] = df[col] / df['forecasted_load']\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/15:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    #df[col] = df[col] / df['forecasted_load']\n",
      "    # Take log\n",
      "    df[col] = np.log(df[col])\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "\n",
      "\n",
      "# Group the data by quarter, month, and country_code, and calculate median and std\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
      "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "82/16:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "82/17:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly.copy()\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "grouped_months['season'] = 'complete'\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "82/18:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly.copy()\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "grouped_months['season'] = 'complete'\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='orange')\n",
      "82/19:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly.copy()\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "grouped_months['season'] = 'complete'\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='orange')\n",
      "82/20: g.fig\n",
      "82/21:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2019-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#c5d3ff')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#c5d3ff')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#c5d3ff')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts\n",
      "g.fig.legend(labels=['Temperature 2m (C)', 'Wind speed 10m (m/s)', 'Wind gusts 10m (m/s)'])\n",
      "# Add x axis label\n",
      "g.fig.text(0.5, 0.01, 'Hour of day', ha='center', va='center')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/22:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2019-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#c5d3ff')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#c5d3ff')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#c5d3ff')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts\n",
      "g.fig.legend(labels=['Temperature 2m (C)', 'Wind speed 10m (m/s)', 'Wind gusts 10m (m/s)'])\n",
      "# Add x axis label\n",
      "g.fig.text(0.5, 0.01, 'Hour of day', ha='center', va='center')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "\n",
      "g.fig\n",
      "82/23:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2022-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts on the right side in every subplot\n",
      "for ax in g.axes.flat:\n",
      "    ax.legend(labels=['Temperature 2m (C)', '25\\% - 75\\%', 'Min - Max',\n",
      "                      'Wind speed 10m (m/s)', '25\\% - 75\\%', 'Min - Max',\n",
      "                      'Wind gusts 10m (m/s)', '25\\% - 75\\%', 'Min - Max'\n",
      "                      ], loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=3)\n",
      "    \n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "\n",
      "g.fig\n",
      "82/24:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2022-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "                      'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "                      'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max'\n",
      "                      ]\n",
      "g.fig.legend(labels=labels, loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=3)\n",
      "# Add margin titles for every row and column\n",
      "g.set_titles(row_template='{row_name}', col_template='{col_name}')\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "\n",
      "g.fig\n",
      "82/25:\n",
      "# Add legend below\n",
      "g.fig.legend(labels=labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
      "g.fig\n",
      "82/26:\n",
      "# Add legend below\n",
      "g.fig.legend(labels=labels, loc='lower center', ncol=3)\n",
      "g.fig\n",
      "82/27:\n",
      "# Add legend below\n",
      "g.fig.legend(labels=labels, loc='upper center', ncol=3)\n",
      "g.fig\n",
      "82/28:\n",
      "# Add legend upper center outside of plot area\n",
      "g.fig.legend(labels=labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.1))\n",
      "g.fig\n",
      "82/29:\n",
      "# Add legend upper center outside of plot area\n",
      "g.fig.legend(labels=labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0))\n",
      "g.fig\n",
      "82/30:\n",
      "# Add legend upper center outside of plot area\n",
      "g.fig.legend(labels=labels)\n",
      "g.fig\n",
      "82/31:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2022-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/32:\n",
      "g.fig.legend(labels=labels, ncol = 3)\n",
      "g.fig\n",
      "82/33:\n",
      "g.fig.legend(labels=labels, ncol = 3)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.title('Weather')\n",
      "g.fig.tight_layout()\n",
      "g.fig\n",
      "82/34:\n",
      "g.fig.legend(labels=labels, ncol = 3)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.suptitle('Weather Temporal', fontsize=16)\n",
      "g.fig.tight_layout()\n",
      "g.fig\n",
      "82/35:\n",
      "g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{row_key}-{col_key}\")\n",
      "g.fig\n",
      "82/36:\n",
      "g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{row_key}: {col_key}\", fontsize=12)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.5)\n",
      "g.fig\n",
      "82/37:\n",
      "g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[row_key]}: {col_key}\", fontsize=12)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "82/38:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2022-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=False, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/39:\n",
      "# Weather Temporal\n",
      "\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.catplot(grouped_quarters, col='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/40:\n",
      "g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[row_key]}: {col_key}\", fontsize=12)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "82/41:\n",
      "g.fig.legend(labels=labels, ncol = 3)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[row_key]}: {col_key}\", fontsize=12)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "82/42:\n",
      "# Weather Temporal\n",
      "\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.catplot(grouped, col='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/43:\n",
      "# Weather Temporal\n",
      "\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/44: g.fig\n",
      "82/45:\n",
      "# Weather Temporal\n",
      "\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/46: g.fig\n",
      "82/47: print(grouped['date'].head())\n",
      "82/48: print(df['date'].head())\n",
      "82/49: print(grouped)\n",
      "82/50:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "grouped['t'] = 't'\n",
      "g = sns.FacetGrid(grouped, row='country',col = 't' sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/51:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "grouped['t'] = 't'\n",
      "g = sns.FacetGrid(grouped, row='country',col = 't', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "82/52: g.fig\n",
      "82/53:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "grouped['t'] = 't'\n",
      "g = sns.FacetGrid(grouped, row='country',col = 't', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "#g.set(xticks=[0,4,8,12,16,20])\n",
      "82/54:\n",
      "\n",
      "g.fig\n",
      "82/55:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(15, 10)\n",
      "g.fig\n",
      "82/56:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 15)\n",
      "g.fig\n",
      "82/57:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 30)\n",
      "g.fig\n",
      "82/58:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.fig\n",
      "82/59:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Convert the timestamp to month e.g. January, February, March\n",
      "g.set_xticklabels(rotation=45, horizontalalignment='right')\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    # Convert the timestamp to month e.g. January, February, March\n",
      "    ax.set_xticklabels([dt.fromtimestamp(x).strftime('%B') for x in ax.get_xticks()])\n",
      "82/60:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Convert the timestamp to month e.g. January, February, March\n",
      "g.set_xticklabels(rotation=45, horizontalalignment='right')\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    # Convert the timestamp to month e.g. January, February, March\n",
      "    ax.set_xticklabels([x.strftime('%B') for x in ax.get_xticks()])\n",
      "82/61:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Convert the timestamp to month e.g. January, February, March\n",
      "g.set_xticklabels(rotation=45, horizontalalignment='right')\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    # Convert the timestamp to month e.g. January, February, March\n",
      "    ax.set_xticklabels([x.strftime('%B') for x in ax.get_xticks()])\n",
      "82/62:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Convert the timestamp to month e.g. January, February, March\n",
      "g.set_xticklabels(rotation=45, horizontalalignment='right')\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    # Convert the timestamp to month e.g. January, February, March\n",
      "    ax.set_xticklabels([pd.to_datetime(x).strftime('%B') for x in ax.get_xticks()])\n",
      "82/63:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "82/64:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "82/65:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([pd.to_datetime(x).strftime('%B') for x in g.get_xticks()])\n",
      "g.fig\n",
      "82/66:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([dt.strftime('%B') for dt in g.axes_dict[0,0].get_xticks()])\n",
      "g.fig\n",
      "82/67:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([dt.strftime('%B') for dt in g.axes_dict[0].get_xticks()])\n",
      "g.fig\n",
      "82/68:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,13)])\n",
      "g.fig\n",
      "82/69:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,12)])\n",
      "g.fig\n",
      "82/70:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,7)])\n",
      "g.fig\n",
      "82/71:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# SEt tick labels to January, February, March, etc.\n",
      "g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,8)])\n",
      "g.fig\n",
      "82/72:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2000,x,1).strftime('%B') for x in range(12)])\n",
      "g.fig\n",
      "82/73:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2000,x,1).strftime('%B') for x in range(1,13)])\n",
      "g.fig\n",
      "82/74:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2000,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/75:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/76:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Convert the timestamp to month e.g. January, February, March\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    # Convert the timestamp to month e.g. January, February, March\n",
      "    ax.set_xticklabels([pd.to_datetime(x).strftime('%B') for x in ax.get_xticks()])\n",
      "82/77:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/78:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/79:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1).strftime('%B') for x in range(1,13)])\n",
      "g.fig\n",
      "82/80:\n",
      "# Weather Temporal\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "82/81:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/82:\n",
      "# Weather Temporal\n",
      "df = w_hourly\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "82/83:\n",
      "# Weather Temporal\n",
      "df = w_hourly\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "82/84:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/85:\n",
      "# Weather Temporal\n",
      "df = w_hourly\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "82/86:\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "g.fig\n",
      "82/87:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "82/88:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "# Add the standard deviation as a black line\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "82/89:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "# Add the standard deviation as a black line\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "82/90:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_100m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_100m_q25', 'windspeed_100m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_100m_min', 'windspeed_100m_max', alpha=0.1, color='#7877E6')\n",
      "# Add the standard deviation as a black line\n",
      "g.map(plt.plot, 'x_label', 'windspeed_100m_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "82/91:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "82/92:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "83/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "83/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "84/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "84/2:\n",
      "# --- Request Blends ----\n",
      "data = client.base_request(endpoint=\"api/blends\")\n",
      "print(data.json())\n",
      "#df = pd.DataFrame.from_records(data.json())\n",
      "87/1:\n",
      "# --- Request Crudes ----\n",
      "df = client.get_crudes()\n",
      "print(df)\n",
      "#df = pd.DataFrame.from_records(data.json())\n",
      "87/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "87/3:\n",
      "# --- Request Crudes ----\n",
      "df = client.get_crudes()\n",
      "print(df)\n",
      "#df = pd.DataFrame.from_records(data.json())\n",
      "87/4: df\n",
      "87/5: df.columns\n",
      "87/6: df['ID']\n",
      "87/7: df.columns\n",
      "87/8: df['Name']\n",
      "87/9: df.columns\n",
      "87/10:\n",
      "df.columns\n",
      "df['intCrudeID']\n",
      "87/11: crude = client.get_crude(\"A1RTP140\")\n",
      "87/12:\n",
      "df.columns\n",
      "df[['intCrudeID','Library']]\n",
      "87/13: crude = client.get_crude(code=\"A1RTP140\",library='CHEVRON')\n",
      "87/14: crude = client.get_crude(code=\"A1RTP140\",libary_name='CHEVRON')\n",
      "88/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "88/2:\n",
      "df.columns\n",
      "df[['intCrudeID','Library']]\n",
      "88/3: crude = client.get_crude(libary_name='CHEVRON',code=\"A1RTP140\")\n",
      "88/4: crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "88/5: crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "89/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "89/2: crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "89/3: crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "89/4:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "89/5: crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "90/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "90/2: crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "90/3:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crude\n",
      "91/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "91/2:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crude\n",
      "91/3:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crude\n",
      "91/4:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "91/5:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crude\n",
      "92/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "92/2:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crude\n",
      "92/3:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude)\n",
      "92/4:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude.attrs)\n",
      "92/5:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude.attrs.cuts)\n",
      "92/6:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude.attrs.Cuts)\n",
      "92/7:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude.attrs.CutsProperties)\n",
      "93/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "93/2:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude.attrs.CutsProperties)\n",
      "93/3:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude)\n",
      "94/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "94/2:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude)\n",
      "95/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "95/2:\n",
      "crude = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(crude)\n",
      "95/3:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "print(summary)\n",
      "95/4:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "summary\n",
      "96/1:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "summary\n",
      "96/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "96/3:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "summary\n",
      "96/4:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "summary, cuts\n",
      "96/5:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      " cuts\n",
      "96/6:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "cuts\n",
      "97/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "98/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "98/2: client.get_specifications()\n",
      "99/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "99/2: client.get_specifications()\n",
      "100/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "100/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "100/3: client.get_specifications()\n",
      "101/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "101/2: client.get_default_specifications()\n",
      "102/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "102/2: client.get_default_specifications()\n",
      "102/3: df_spec = client.get_default_specifications()\n",
      "102/4: df_spec['product_name'].unique()\n",
      "102/5:\n",
      "df_spec = client.get_default_specifications()\n",
      "df_spec\n",
      "102/6:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "102/7:\n",
      "df_recut = client.get_slate_fraction(cut_id=35331, slate_id=25917, tabular = True)\n",
      "# To tabular\n",
      "df_recut\n",
      "103/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "103/2:\n",
      "df_recut = client.get_slate_fraction(cut_id=35331, slate_id=25917, tabular = True)\n",
      "# To tabular\n",
      "df_recut\n",
      "103/3:\n",
      "df_recut = client.get_slate_fraction(cutset_id=35331, slate_id=25917, tabular = True)\n",
      "# To tabular\n",
      "df_recut\n",
      "105/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "105/2:\n",
      "# How to put into your own custom specs? Especially how do you combine the product with the product defined in ur cut_id?\n",
      "df_spec = client.get_default_specifications()\n",
      "df_spec\n",
      "105/3:\n",
      "df_recut = client.get_slate_fraction(cutset_id=35331, slate_id=25917, tabular = True)\n",
      "# To tabular\n",
      "df_recut\n",
      "106/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "106/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)\n",
      "# To tabular\n",
      "df_recut\n",
      "107/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "107/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)\n",
      "# To tabular\n",
      "df_recut\n",
      "108/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "108/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)\n",
      "# To tabular\n",
      "df_recut\n",
      "109/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "109/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)\n",
      "# To tabular\n",
      "df_recut\n",
      "109/3:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "109/4: df_recut.reset_index()\n",
      "110/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "110/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "111/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "111/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "112/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "112/2:\n",
      "# How to put into your own custom specs? Especially how do you combine the product with the product defined in ur cut_id?\n",
      "df_spec = client.get_default_specifications()\n",
      "df_spec\n",
      "112/3:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "113/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "113/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "114/1:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "114/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "114/3:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "115/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "115/2:\n",
      "df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)\n",
      "# To tabular\n",
      "df_recut\n",
      "115/3:\n",
      "# Get RON on row level\n",
      "df_recut[df_recut['property'] == 'RON']\n",
      "115/4:\n",
      "# Fetch the RON on the index\n",
      "df_recut.loc['RON']\n",
      "115/5:\n",
      "# Fetch all possible properties from df_recut\n",
      "df_recut['property'].unique()\n",
      "115/6:\n",
      "# Fetch all possible properties from df_recut. All unique indices are the properties\n",
      "df_recut.index.unique()\n",
      "115/7:\n",
      "# Fetch all possible properties from df_recut. All unique indices are the properties\n",
      "print(df_recut.index.unique())\n",
      "115/8:\n",
      "# Fetch all possible properties from df_recut. All unique indices are the properties\n",
      "print(df_recut.index.unique())\n",
      "# Fetch RONC\n",
      "df_recut.loc['RONC']\n",
      "115/9:\n",
      "# Fetch all crude data\n",
      "crudes = client.get_crudes()\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crudes\n",
      "115/10:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "115/11:\n",
      "# Fetch all crude data\n",
      "crudes = client.get_crudes()\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "crudes\n",
      "115/12:\n",
      "crudes = client.get_crudes()\n",
      "print(crudes.columns)\n",
      "crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])\n",
      "crudes\n",
      "115/13:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "cuts\n",
      "115/14:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "summary\n",
      "115/15:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\")\n",
      "cuts\n",
      "115/16:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "from hcomet.templates import *\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "115/17:\n",
      "# Transform the crude into a blend with 100% of the crude\n",
      "from hcomet import templates\n",
      "crudes = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend = templates.blend_template(crudes)\n",
      "116/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "116/2:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\", tabular = True)\n",
      "cuts\n",
      "117/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "117/2:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\", tabular = True)\n",
      "cuts\n",
      "118/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "118/2:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\", tabular = True)\n",
      "cuts\n",
      "119/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "119/2:\n",
      "summary, cuts = client.get_crude(library_name='CHEVRON',code=\"A1RTP140\", tabular = True)\n",
      "cuts\n",
      "119/3:\n",
      "from hcomet import templates\n",
      "crudes = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "# Transform the crude into a blend with 100% of the crude\n",
      "blend_name = 'test_blend'\n",
      "blend_template = templates.blend_template(crudes, blend_name)\n",
      "# Get the new cuts\n",
      "cutset_name = 'test_cutset'\n",
      "cutset_template = templates.cutset_template(cutset, cutset_name)\n",
      "\n",
      "# Create the blend\n",
      "res = client.post_blend(blend_template, blend_name)\n",
      "print(res)\n",
      "# Create the cut\n",
      "res = client.put_cutset(cutset_template, cutset_name)\n",
      "print(res)\n",
      "119/4:\n",
      "from hcomet import templates\n",
      "crudes = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "# Transform the crude into a blend with 100% of the crude\n",
      "blend_name = 'test_blend'\n",
      "blend_template = templates.blend_template(crudes, blend_name)\n",
      "# Get the new cuts\n",
      "cutset_name = 'test_cutset'\n",
      "cutset_template = templates.cutset_template(cutset, cutset_name)\n",
      "\n",
      "# Create the blend\n",
      "res = client.post_blend(blend_template)\n",
      "print(res)\n",
      "# Create the cut\n",
      "res = client.post_blend(cutset_template)\n",
      "print(res)\n",
      "119/5:\n",
      "from hcomet import templates\n",
      "crudes = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "# Transform the crude into a blend with 100% of the crude\n",
      "blend_name = 'test_blend'\n",
      "blend_template = templates.blend_template(crudes, blend_name)\n",
      "# Get the new cuts\n",
      "cutset_name = 'test_cutset'\n",
      "cutset_template = templates.cutset_template(cutset, cutset_name)\n",
      "\n",
      "# Create the blend\n",
      "res = client.post_blend(blend_template)\n",
      "print(res)\n",
      "# Create the cut\n",
      "res = client.post_blend(cutset_template)\n",
      "print(res.text)\n",
      "119/6:\n",
      "from hcomet import templates\n",
      "crudes = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "# Transform the crude into a blend with 100% of the crude\n",
      "blend_name = 'test_blend'\n",
      "blend_template = templates.blend_template(crudes, blend_name)\n",
      "# Get the new cuts\n",
      "cutset_name = 'test_cutset'\n",
      "cutset_template = templates.cutset_template(cutset, cutset_name)\n",
      "\n",
      "# Create the blend\n",
      "res = client.post_blend(blend_template)\n",
      "print(res.text)\n",
      "# Create the cut\n",
      "res = client.post_cutset(cutset_template)\n",
      "print(res.text)\n",
      "119/7:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "120/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "120/2:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "120/3:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "120/4:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends())\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "120/5:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends())\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "121/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "121/2:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends())\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "122/2:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends())\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/3:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends().head())\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/4:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends().head())\n",
      "print(client.get_cutsets().head())\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/5:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends().head())\n",
      "print(client.get_cutsets().columns)\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/6:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends().head())\n",
      "print(client.get_cutsets()['name','id'])\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/7:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends().head())\n",
      "print(client.get_cutsets()[['name','id']])\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/8:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(client.get_blends()[['name','id']])\n",
      "print(client.get_cutsets()[['name','id']])\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/9:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "print(blend_id, cutset_id)\n",
      "122/10:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Existing blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Existing cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "# Recut blend based on cutset\n",
      "df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)\n",
      "\n",
      "# Delete blend and cutset\n",
      "client.delete_blend(blend_id)\n",
      "client.delete_cutset(cutset_id)\n",
      "122/11:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Existing blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Existing cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "# Recut blend based on cutset\n",
      "df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)\n",
      "\n",
      "# Delete blend and cutset\n",
      "client.delete_blend(blend_id)\n",
      "client.delete_cutset(cutset_id)\n",
      "\n",
      "df_recut\n",
      "123/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "123/2:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Existing blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Existing cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "# Recut blend based on cutset\n",
      "df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)\n",
      "\n",
      "# Delete blend and cutset\n",
      "client.delete_blend(blend_id)\n",
      "client.delete_cutset(cutset_id)\n",
      "\n",
      "df_recut\n",
      "124/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "124/2:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Existing blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Existing cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "# Recut blend based on cutset\n",
      "df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)\n",
      "\n",
      "# Delete blend and cutset\n",
      "client.delete_blend(blend_id)\n",
      "client.delete_cutset(cutset_id)\n",
      "\n",
      "df_recut\n",
      "125/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "125/2:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Existing blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Existing cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "# Recut blend based on cutset\n",
      "df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)\n",
      "\n",
      "# Delete blend and cutset\n",
      "client.delete_blend(blend_id)\n",
      "client.delete_cutset(cutset_id)\n",
      "\n",
      "df_recut\n",
      "125/3:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "125/4:\n",
      "from hcomet import templates\n",
      "blend = [\n",
      "    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },\n",
      "]\n",
      "blend_name = 'test_blend'\n",
      "\n",
      "cutset = [\n",
      "        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},\n",
      "        {\"Name\" : 'LN', \"IBP\" : 15, \"EBP\" : 85},\n",
      "        {\"Name\" : 'HN', \"IBP\" : 85, \"EBP\" : 175},\n",
      "        {\"Name\" : 'JET', \"IBP\" : 175, \"EBP\" : 272},\n",
      "        {\"Name\" : 'GO', \"IBP\" : 272, \"EBP\" : 325},\n",
      "        {\"Name\" : 'RES', \"IBP\" : 250, \"EBP\" : 9999},\n",
      "    ]\n",
      "cutset_name = 'test_cutset'\n",
      "\n",
      "print(f\"\"\"Existing blends:\\n{client.get_blends()[['name','id']]}\"\"\")\n",
      "print(f\"\"\"Existing cutsets:\\n{client.get_cutsets()[['name','id']]}\"\"\")\n",
      "\n",
      "# Submit blend and cutset\n",
      "blend_id = client.submit_blend(blend, blend_name)\n",
      "cutset_id = client.submit_cutset(cutset, cutset_name)\n",
      "\n",
      "# Recut blend based on cutset\n",
      "df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)\n",
      "\n",
      "# Delete blend and cutset\n",
      "client.delete_blend(blend_id)\n",
      "client.delete_cutset(cutset_id)\n",
      "\n",
      "df_recut\n",
      "125/5:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "125/6:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "125/7:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "126/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "126/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "127/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "127/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "127/3:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "128/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "128/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "128/3:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "129/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "129/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "130/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "130/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "131/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "131/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "132/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "132/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "133/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "133/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "134/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "134/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "135/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "135/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "136/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "136/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "client.post_cru_files(LIBRARY,FILES)\n",
      "136/3:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "137/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "137/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\"]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "138/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "138/2:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "138/3:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980 \"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "138/4:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "138/5:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "138/6:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"flash_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "138/7:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "139/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "139/2:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"FLASH_200980\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "139/3:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "140/1:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "140/2:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "140/3:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "141/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "141/2:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "142/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "142/2:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "142/3:\n",
      "FILES = [\n",
      "        #\"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "142/4:\n",
      "FILES = [\n",
      "        \"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru\",\n",
      "        \"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru\",\n",
      "        #\"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru\",\n",
      "        ]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "142/5:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/AUS00 FLASH AUS2209A.cru\"]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "143/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "143/2:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/AUS00 FLASH AUS2209A.cru\"]\n",
      "# Our library name (FLASH_200980 doesn't work...)\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "143/3:\n",
      "crudes = client.get_crudes()\n",
      "print(crudes.columns)\n",
      "crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])\n",
      "crudes\n",
      "143/4:\n",
      "assay_names = [file.split(\"/\")[-1] for file in FILES]\n",
      "assay_names\n",
      "143/5:\n",
      "assay_names = [file.split(\"/\")[-1] for file in FILES]\n",
      "mask = crudes['name'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/6:\n",
      "assay_names = [file.split(\"/\")[-1] for file in FILES]\n",
      "mask = crudes['id'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/7:\n",
      "assay_names = [file.split(\"/\")[-1] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['id'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/8:\n",
      "assay_names = [file.split(\"/\")[-1].str.remove('.cru') for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['id'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/9:\n",
      "assay_names = [file.split(\"/\")[-1].remove('.cru') for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['id'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/10:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['id'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/11:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['name'].isin(assay_names)\n",
      "crudes[mask]\n",
      "143/12:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['library']==LIBRARY\n",
      "crudes[mask]\n",
      "143/13:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['library']==LIBRARY\n",
      "print(crudes[mask])\n",
      "143/14:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask])\n",
      "144/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "144/2:\n",
      "crudes = client.get_crudes()\n",
      "print(crudes.columns)\n",
      "crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])\n",
      "crudes\n",
      "145/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "145/2:\n",
      "crudes = client.get_crudes()\n",
      "print(crudes.columns)\n",
      "crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])\n",
      "crudes\n",
      "146/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "146/2:\n",
      "crudes = client.get_crudes()\n",
      "print(crudes.columns)\n",
      "crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])\n",
      "crudes\n",
      "146/3:\n",
      "crudes = client.get_crudes()\n",
      "print(crudes.columns)\n",
      "#crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])\n",
      "crudes\n",
      "146/4:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask])\n",
      "146/5:\n",
      "FILES = [\"C:/Users/Nicky/Downloads/AUS00 FLASH AUS2209A.cru\"]\n",
      "# TODO: Our library name (FLASH_200980 doesn't work...). Therefore, we need to request support from Haverly\n",
      "LIBRARY = \"VITOL\"\n",
      "res = client.post_cru_files(LIBRARY,FILES)\n",
      "print(res.text)\n",
      "146/6:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask])\n",
      "146/7:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['int_crude_id'])\n",
      "146/8:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['id'])\n",
      "146/9:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['name'])\n",
      "146/10:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['int_crude_id'])\n",
      "146/11:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['code'])\n",
      "146/12:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['cru_file_source'])\n",
      "146/13:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "print(crudes[mask]['properties'])\n",
      "146/14:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "crudes[mask]['properties'].values[0]\n",
      "146/15:\n",
      "assay_names = [file.split(\"/\")[-1].split(\".\")[0] for file in FILES]\n",
      "print(assay_names)\n",
      "mask = crudes['int_crude_id'].isin(assay_names)\n",
      "pd.DataFrame(crudes[mask]['properties'].values[0])\n",
      "146/16: slates_df = client.get_slates()\n",
      "147/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "147/2:\n",
      "slates_df = client.get_slates()\n",
      "slates_df\n",
      "147/3:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "blends_df\n",
      "147/4:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "print(f\"\"\"Blends: {blends_df.head()}\"\"\")\n",
      "\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}: {blend}\"\"\")\n",
      "147/5:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "print(f\"\"\"Blends: {blends_df.head()}\"\"\")\n",
      "\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}: {blend}\"\"\")\n",
      "148/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "148/2:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "print(f\"\"\"Blends: {blends_df.head()}\"\"\")\n",
      "\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}: {blend}\"\"\")\n",
      "148/3:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend}\"\"\")\n",
      "148/4:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "\n",
      "# --- Blends ---\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend}\"\"\")\n",
      "# --- Slates ---\n",
      "print(f\"\"\"Blends:\\n{slates_df.head()}\"\"\")\n",
      "#blend_id = \"46846\" # test_vpr\n",
      "#blend = client.get_blend(blend_id)\n",
      "#print(f\"\"\"Blend {blend_id}:\\n{blend}\"\"\")\n",
      "148/5:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "\n",
      "# --- Blends ---\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend}\"\"\")\n",
      "# --- Slates ---\n",
      "print(f\"\"\"Slates:\\n{slates_df.head()}\"\"\")\n",
      "slate_id = \"25955\" # China_Coking_XMB\n",
      "slate = client.get_slate(slate_id)\n",
      "print(f\"\"\"Slate {slate_id}:\\n{slate}\"\"\")\n",
      "148/6:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "\n",
      "# --- Blends ---\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend}\"\"\")\n",
      "# --- Slates ---\n",
      "print(f\"\"\"Slates:\\n{slates_df.head()}\"\"\")\n",
      "slate_id = \"26005\" # China_Coking_XMB\n",
      "slate = client.get_slate(slate_id)\n",
      "print(f\"\"\"Slate {slate_id}:\\n{slate}\"\"\")\n",
      "149/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "149/2:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "\n",
      "# --- Blends ---\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend}\"\"\")\n",
      "# --- Slates ---\n",
      "print(f\"\"\"Slates:\\n{slates_df.head()}\"\"\")\n",
      "slate_id = \"26005\" # China_Coking_XMB\n",
      "slate = client.get_slate(slate_id)\n",
      "print(f\"\"\"Slate {slate_id}:\\n{slate}\"\"\")\n",
      "149/3:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "\n",
      "# --- Blends ---\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend.head()}\"\"\")\n",
      "# --- Slates ---\n",
      "print(f\"\"\"Slates:\\n{slates_df.head()}\"\"\")\n",
      "slate_id = \"26005\" # China_Coking_XMB\n",
      "slate = client.get_slate(slate_id)\n",
      "print(f\"\"\"Slate {slate_id}:\\n{slate.head()}\"\"\")\n",
      "150/1:\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "150/2:\n",
      "slates_df = client.get_slates()\n",
      "blends_df = client.get_blends()\n",
      "\n",
      "# --- Blends ---\n",
      "print(f\"\"\"Blends:\\n{blends_df.head()}\"\"\")\n",
      "blend_id = \"46846\" # test_vpr\n",
      "blend = client.get_blend(blend_id)\n",
      "print(f\"\"\"Blend {blend_id}:\\n{blend.head()}\"\"\")\n",
      "# --- Slates ---\n",
      "print(f\"\"\"Slates:\\n{slates_df.head()}\"\"\")\n",
      "slate_id = \"26005\" # China_Coking_XMB\n",
      "slate = client.get_slate(slate_id)\n",
      "print(f\"\"\"Slate {slate_id}:\\n{slate.head()}\"\"\")\n",
      "150/3:\n",
      "# --- Imports----\n",
      "import sys\n",
      "import pandas as pd\n",
      "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
      "from hcomet.hcomet import HcometClient\n",
      "from secret import *\n",
      "import psycopg2\n",
      "\n",
      "# --- Set up proxies ----\n",
      "#proxies = {\"http\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\", \"https\": f\"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080\"}\n",
      "proxies = {}\n",
      "# --- Set up Client ----\n",
      "client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)\n",
      "# --- Database retrieval functions ----\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "151/1:\n",
      "g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[row_key]}: {col_key}\", fontsize=12)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "151/2:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2022-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=False, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "151/3:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "151/4:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    [location_eindhoven],\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "151/5:\n",
      "# Create a graph of the weather data using matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots(figsize=(20,3))\n",
      "ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')\n",
      "ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')\n",
      "ax.set_xlabel('Time')\n",
      "ax.set_ylabel('Temperature (C), Wind Speed (m/s)')\n",
      "ax.set_title('Weather Data for Eindhoven')\n",
      "ax.legend()\n",
      "plt.show()\n",
      "151/6:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "151/7:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "151/8:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    #df[col] = df[col] / df['forecasted_load']\n",
      "    # Take log\n",
      "    df[col] = np.log(df[col])\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['season'] = df['timestamp'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "151/9:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "151/10:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "151/11:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Wind Speed (m/s)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "151/12:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "151/13:\n",
      "# Investigate the distribution of wind speed for each city\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Wind Speed (m/s)')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "    # Xaxis max of 50 m/s\n",
      "    axes[i].set_xlim(0, 50)\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "151/14:\n",
      "# Check whether the data is lognormally distributed, take the log of the data and plot the distribution\n",
      "df = w_hourly.copy()\n",
      "#df = df[df['country'] == 'NL']\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "# Take log of wind speed (when wind speed is 0, take the log of 0.01)\n",
      "df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))\n",
      "\n",
      "def move_legend(ax, new_loc, **kws):\n",
      "    old_legend = ax.legend_\n",
      "    handles = old_legend.legendHandles\n",
      "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
      "    _title = old_legend.get_title().get_text()\n",
      "    # Check if title in **kws\n",
      "    if 'title' in kws:\n",
      "        _title = kws['title']\n",
      "        del kws['title']\n",
      "    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)\n",
      "    \n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    \n",
      "    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)\n",
      "    axes[i].set_title(region)\n",
      "    axes[i].set_ylabel('Frequency')\n",
      "    # Show xlabel for all plots\n",
      "    # Show the xticks for all plots\n",
      "    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
      "    axes[i].set_xlabel('Logarithmic Wind Speed')\n",
      "    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values\n",
      "    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')\n",
      "# Add legend outside the plot\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "151/15:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "151/16:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "df = hourly_entsoe_day_ahead_prices.copy()\n",
      "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
      "\n",
      "fig, axes = plt.subplots(1, 1, figsize=(12, 5), sharex=False)\n",
      "i=0\n",
      "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
      "# Smaller line width\n",
      "for j in range(0, len(axes.lines)):\n",
      "    axes.lines[j].set_linewidth(1.5)\n",
      "    axes.lines[j].set_alpha(0.5)\n",
      "axes.set_title('Day Ahead Prices')\n",
      "# Set axis labels to '' to avoid overlapping\n",
      "axes.set_ylabel('Price (EUR/MWh)')\n",
      "axes.set_xlabel('')\n",
      "# Rotate xticks\n",
      "axes.tick_params(axis='x', rotation=45)\n",
      "# Legends outside the plot\n",
      "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "# Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "151/17:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    #df[col] = df[col] / df['forecasted_load']\n",
      "    # Take log\n",
      "    df[col] = np.log(df[col])\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['season'] = df['timestamp'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "151/18:\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Merge the three dataframes\n",
      "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
      "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
      "# Select only NL and DE\n",
      "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]\n",
      "\n",
      "# Sum over price_area\n",
      "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
      "\n",
      "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
      "for col in hourly_entsoe_generation_production.columns[3:]:\n",
      "    #df[col] = df[col] / df['forecasted_load']\n",
      "    # Take log\n",
      "    df[col] = np.log(df[col])\n",
      "\n",
      "# Create new columns for quarter and month\n",
      "df['season'] = df['timestamp'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
      "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['timestamp'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "grouped_months = df.groupby(['date','country_code']).agg({\n",
      "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "grouped_months = grouped_months.reset_index()\n",
      "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
      "grouped_months['quarter'] = 5\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country_code', col='season', margin_titles=True, sharex=False, sharey=False)\n",
      "\n",
      "# Plot wind, solar, renewable, fossil, and price\n",
      "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
      "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
      "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
      "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
      " \n",
      "g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
      "g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
      "\n",
      "# Double axis for price\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig\n",
      "151/19:\n",
      "# Weather Temporal\n",
      "import seaborn as sns\n",
      "from matplotlib import pyplot as plt\n",
      "df = w_hourly[w_hourly['time'] > '2022-01-01']\n",
      "\n",
      "# Average over country\n",
      "df = df.groupby(['time', 'country']).mean().reset_index()\n",
      "\n",
      "# Create new columns for season and month (EU in same hemisphere)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# 25th Percentile\n",
      "def q25(x):    \n",
      "    return x.quantile(0.25)\n",
      "# 75th Percentile\n",
      "def q75(x):\n",
      "    return x.quantile(0.75)\n",
      "\n",
      "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
      "grouped_quarters = df.groupby(['season', 'hour','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],\n",
      "})\n",
      "\n",
      "# Reset index and rename columns\n",
      "grouped_quarters = grouped_quarters.reset_index()\n",
      "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})\n",
      "\n",
      "# Set up the plot\n",
      "g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=False, sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "# Show 0 to 24 in x axis in 0,4,8,12,16,20,24\n",
      "g.set(xticks=[0,4,8,12,16,20])\n",
      "151/20:\n",
      "g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Set plot title, axis labels, and legend\n",
      "g.fig.tight_layout()\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[row_key]}: {col_key}\", fontsize=12)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.25)\n",
      "g.fig\n",
      "151/21:\n",
      "# Weather Temporal\n",
      "df = w_hourly\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x, roll_length=24):\n",
      "    return x.rolling(24).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/22:\n",
      "# Weather Temporal\n",
      "df = w_hourly\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x, roll_length=24):\n",
      "    return x.rolling(24).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/23:\n",
      "# Weather Temporal\n",
      "df = w_hourly\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x, roll_length=24*7):\n",
      "    return x.rolling(24).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/24:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x, roll_length=24*7):\n",
      "    return x.rolling(24).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/25:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/26:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std().mean()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/27:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/28:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "#g.fig.set_size_inches(20, 40)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/29:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "\n",
      "# Extend the graphs\n",
      "g.fig.set_size_inches(20)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/30:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/31:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "print(grouped)\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/32:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "print(grouped.tail())\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/33:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(50000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std().mean()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/34:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "print(grouped.tail())\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/35:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(50000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/36:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(50000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24).std()\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/37:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(50000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24).std().iloc[-1]\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/38:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "print(grouped.tail())\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/39:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(50000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/40:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "print(grouped.tail())\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/41:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(500000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/42:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "print(grouped.tail())\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/43:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "df['hour'] = pd.DatetimeIndex(df['time']).hour\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x):\n",
      "    return x.rolling(24*7).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/44:\n",
      "g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/45:\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2 )\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "\n",
      "# Remove temperature_2m_median \n",
      "\n",
      "g.fig\n",
      "151/46:\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2 )\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.4)\n",
      "# Convert to 12 ticks in x axis\n",
      "g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])\n",
      "151/47:\n",
      "# Manipulate\n",
      "# Add average temperature to daily, max + min / 2\n",
      "w_daily['temperature_2m_avg'] = (w_daily['temperature_2m_max'] + w_daily['temperature_2m_min']) / 2\n",
      "151/48:\n",
      "# Weather Temporal \n",
      "df = w_daily.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x, window=7):\n",
      "    return x.rolling(window).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/49:\n",
      "# Weather Temporal \n",
      "df = w_daily.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x, window=7):\n",
      "    return x.rolling(window).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/50:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x, window=7):\n",
      "    return x.rolling(window).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m_avg': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/51:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x, window=7):\n",
      "    return x.rolling(window).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/52: print(df.columns)\n",
      "151/53: print(grouped.columns)\n",
      "151/54:\n",
      "# Weather Temporal \n",
      "df = w_hourly.head(5000)\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x, window=7):\n",
      "    return x.rolling(window).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "def rolling_week_std(x):\n",
      "    return rolling_std(x, window=7)\n",
      "\n",
      "def rolling_month_std(x):\n",
      "    return rolling_std(x, window=30)\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75,  rolling_week_std, rolling_month_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/55:\n",
      "# Weather Temporal \n",
      "df = w_hourly\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "# Group the data by date\n",
      "df['date'] = df['time'].dt.date\n",
      "# Change year of date to 2020\n",
      "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
      "# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std\n",
      "# Rolling std (not aggregate function)\n",
      "def rolling_std(x, window=7):\n",
      "    return x.rolling(window).std().iloc[-1] # -1 to get the last value\n",
      "\n",
      "def rolling_week_std(x):\n",
      "    return rolling_std(x, window=7)\n",
      "\n",
      "def rolling_month_std(x):\n",
      "    return rolling_std(x, window=30)\n",
      "\n",
      "\n",
      "grouped = df.groupby(['date','country']).agg({\n",
      "    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75,  rolling_week_std, rolling_month_std],\n",
      "    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],\n",
      "    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],\n",
      "    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],\n",
      "})\n",
      "# Reset index and rename columns\n",
      "grouped = grouped.reset_index()\n",
      "grouped.columns = ['_'.join(col) for col in grouped.columns]\n",
      "# Rename country_code_ to country_code\n",
      "grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})\n",
      "# Convert date to datetime\n",
      "#grouped['x_label'] = pd.to_datetime(grouped['x_label'])\n",
      "\n",
      "# Set up the plot (for each country)\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "# Wind gusts (#0D5060)\n",
      "#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')\n",
      "#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')\n",
      "\n",
      "# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots\n",
      "labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',\n",
      "        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',\n",
      "        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']\n",
      "# Remove y and x names\n",
      "g.set_axis_labels('', '')\n",
      "151/56:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2 )\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_week_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(20, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.5)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%b\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[row_key]}: {col_key}\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "151/57:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=3)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_week_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(vspace=0.35)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[col_key]}\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Crease size between plots on same row\n",
      "151/58:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=3)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_rolling_week_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[col_key]}\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Crease size between plots on same row\n",
      "151/59:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=3)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"{TSO_COUNTRIES[col_key]}\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Crease size between plots on same row\n",
      "151/60:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.35)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"\"\"{TSO_COUNTRIES[f\"{row_key}{col_key}\"]}\"\"\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Crease size between plots on same row\n",
      "151/61:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.6)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"\"\"{TSO_COUNTRIES[f\"{row_key}{col_key}\"]}\"\"\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels[0:3], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Decrease the space between the plots horizontally\n",
      "g.fig.subplots_adjust(wspace=0.1)\n",
      "151/62:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=0.6)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"\"\"{TSO_COUNTRIES[f\"{row_key}{col_key}\"]}\"\"\", fontsize=12)\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels[0:3], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Decrease the space between the plots horizontally\n",
      "g.fig.subplots_adjust(wspace=0.1)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=1)\n",
      "151/63:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')\n",
      "g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')\n",
      "\n",
      "#g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"\"\"{TSO_COUNTRIES[f\"{row_key}{col_key}\"]}\"\"\")\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels[0:3], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Decrease the space between the plots horizontally\n",
      "g.fig.subplots_adjust(wspace=0.1)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=1)\n",
      "151/64:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10mq75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "#g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"\"\"{TSO_COUNTRIES[f\"{row_key}{col_key}\"]}\"\"\")\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels[3:6], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Decrease the space between the plots horizontally\n",
      "g.fig.subplots_adjust(wspace=0.1)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=1)\n",
      "151/65:\n",
      "from matplotlib.dates import DateFormatter\n",
      "g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=4)\n",
      "\n",
      "g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')\n",
      "g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')\n",
      "\n",
      "#g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')\n",
      "# Extend the graphs horizontally\n",
      "g.fig.set_size_inches(30, 10)\n",
      "g.set_xticklabels(rotation=0)\n",
      "# Add custom formatter (January, February, etc.)\n",
      "formatter = DateFormatter(\"%B\")\n",
      "for (row_key, col_key),ax in g.axes_dict.items():\n",
      "    ax.xaxis.set_major_formatter(formatter)\n",
      "    ax.set_title(f\"\"\"{TSO_COUNTRIES[f\"{row_key}{col_key}\"]}\"\"\")\n",
      "    \n",
      "# Drop xaxis label\n",
      "g.set_axis_labels('', '')\n",
      "# Add legend\n",
      "g.fig.legend(labels=labels[3:6], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)\n",
      "# Decrease the space between the plots horizontally\n",
      "g.fig.subplots_adjust(wspace=0.1)\n",
      "# Increase the space between the plots vertically\n",
      "g.fig.subplots_adjust(hspace=1)\n",
      "151/66:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median'],\n",
      "    'windspeed_10m_max': ['median'],\n",
      "    'windgusts_10m_max': ['median'],\n",
      "})\n",
      "\n",
      "# Rename columns\n",
      "summary_df.columns = ['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename columns\n",
      "summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "pivot_summary\n",
      "151/67:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median'],\n",
      "    'windspeed_10m_max': ['median'],\n",
      "    'windgusts_10m_max': ['median'],\n",
      "})\n",
      "\n",
      "# Rename columns\n",
      "summary_df.columns = ['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename columns\n",
      "summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "pivot_summary\n",
      "151/68:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median'],\n",
      "    'windspeed_10m_max': ['median'],\n",
      "    'windgusts_10m_max': ['median'],\n",
      "})\n",
      "\n",
      "# Rename columns\n",
      "summary_df.columns = ['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename columns\n",
      "summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "pivot_summary\n",
      "151/69:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median'],\n",
      "    'windspeed_10m_max': ['median'],\n",
      "    'windgusts_10m_max': ['median'],\n",
      "})\n",
      "\n",
      "# Rename columns\n",
      "summary_df.columns = [' Median: Average Temperature (2m)', 'Median: Max Windspeed (10m)', 'Median: Max Windgust (10m)']\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename columns\n",
      "summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "pivot_summary\n",
      "151/70:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median'],\n",
      "    'windspeed_10m_max': ['median'],\n",
      "    'windgusts_10m_max': ['median'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename columns\n",
      "summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "pivot_summary.to_latex()\n",
      "151/71:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median'],\n",
      "    'windspeed_10m_max': ['median'],\n",
      "    'windgusts_10m_max': ['median'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename columns\n",
      "summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "print(pivot_summary.to_latex())\n",
      "153/1:\n",
      "import requests\n",
      "import pandas as pd\n",
      "\n",
      "companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')\n",
      "companies\n",
      "153/2:\n",
      "def google_search(company):\n",
      "    url = 'https://www.google.com/search?q=' + company\n",
      "    response = requests.get(url)\n",
      "    return response\n",
      "\n",
      "def get_results(company):\n",
      "    # Extract results from Google search\n",
      "    response = google_search(company)\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    search_results = soup.find('div', {'class': 'g'})\n",
      "    print(search_results)\n",
      "    # Extract the telephone number (if any)\n",
      "    telephone_number = None\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'^tel:', tag['href']):\n",
      "            telephone_number = tag['href'][4:]\n",
      "            break\n",
      "\n",
      "    # Extract the company social media links (if any)\n",
      "    social_media_links = {}\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'twitter.com', tag['href']):\n",
      "            social_media_links['twitter'] = tag['href']\n",
      "        elif re.search(r'linkedin.com', tag['href']):\n",
      "            social_media_links['linkedin'] = tag['href']\n",
      "        elif re.search(r'facebook.com', tag['href']):\n",
      "            social_media_links['facebook'] = tag['href']\n",
      "\n",
      "    # Extract all links from the search result page\n",
      "    all_links = []\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        all_links.append(tag['href'])\n",
      "    \n",
      "     # Return as a dictionary\n",
      "    time.sleep(random.randint(1, 3))\n",
      "    return {\n",
      "        'company': company,\n",
      "        'telephone_number': telephone_number,\n",
      "        'social_media_links': social_media_links,\n",
      "        'all_links': all_links\n",
      "    }\n",
      "153/3:\n",
      "# Get results for all companies\n",
      "results = []\n",
      "for idx, row in companies.iterrows():\n",
      "    results.append(get_results(row['company']))\n",
      "153/4:\n",
      "# Get results for all companies\n",
      "results = []\n",
      "for idx, row in companies.iterrows():\n",
      "    results.append(get_results(row['name']))\n",
      "153/5:\n",
      "import requests\n",
      "import pandas as pd\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "import time, random\n",
      "companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')\n",
      "153/6:\n",
      "def google_search(company):\n",
      "    url = 'https://www.google.com/search?q=' + company\n",
      "    response = requests.get(url)\n",
      "    return response\n",
      "\n",
      "def get_results(company):\n",
      "    # Extract results from Google search\n",
      "    response = google_search(company)\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    search_results = soup.find('div', {'class': 'g'})\n",
      "    print(search_results)\n",
      "    # Extract the telephone number (if any)\n",
      "    telephone_number = None\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'^tel:', tag['href']):\n",
      "            telephone_number = tag['href'][4:]\n",
      "            break\n",
      "\n",
      "    # Extract the company social media links (if any)\n",
      "    social_media_links = {}\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'twitter.com', tag['href']):\n",
      "            social_media_links['twitter'] = tag['href']\n",
      "        elif re.search(r'linkedin.com', tag['href']):\n",
      "            social_media_links['linkedin'] = tag['href']\n",
      "        elif re.search(r'facebook.com', tag['href']):\n",
      "            social_media_links['facebook'] = tag['href']\n",
      "\n",
      "    # Extract all links from the search result page\n",
      "    all_links = []\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        all_links.append(tag['href'])\n",
      "    \n",
      "     # Return as a dictionary\n",
      "    time.sleep(random.randint(1, 3))\n",
      "    return {\n",
      "        'company': company,\n",
      "        'telephone_number': telephone_number,\n",
      "        'social_media_links': social_media_links,\n",
      "        'all_links': all_links\n",
      "    }\n",
      "153/7:\n",
      "# Get results for all companies\n",
      "results = []\n",
      "for idx, row in companies.iterrows():\n",
      "    results.append(get_results(row['name']))\n",
      "153/8:\n",
      "def google_search(company):\n",
      "    url = 'https://www.google.com/search?q=' + company\n",
      "    response = requests.get(url)\n",
      "    return response\n",
      "\n",
      "def get_results(company):\n",
      "    # Extract results from Google search\n",
      "    response = google_search(company)\n",
      "    print(response.text)\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    search_results = soup.find('div', {'class': 'g'})\n",
      "    print(search_results)\n",
      "    # Extract the telephone number (if any)\n",
      "    telephone_number = None\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'^tel:', tag['href']):\n",
      "            telephone_number = tag['href'][4:]\n",
      "            break\n",
      "\n",
      "    # Extract the company social media links (if any)\n",
      "    social_media_links = {}\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'twitter.com', tag['href']):\n",
      "            social_media_links['twitter'] = tag['href']\n",
      "        elif re.search(r'linkedin.com', tag['href']):\n",
      "            social_media_links['linkedin'] = tag['href']\n",
      "        elif re.search(r'facebook.com', tag['href']):\n",
      "            social_media_links['facebook'] = tag['href']\n",
      "\n",
      "    # Extract all links from the search result page\n",
      "    all_links = []\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        all_links.append(tag['href'])\n",
      "    \n",
      "     # Return as a dictionary\n",
      "    time.sleep(random.randint(1, 3))\n",
      "    return {\n",
      "        'company': company,\n",
      "        'telephone_number': telephone_number,\n",
      "        'social_media_links': social_media_links,\n",
      "        'all_links': all_links\n",
      "    }\n",
      "153/9:\n",
      "# Get results for all companies\n",
      "results = []\n",
      "for idx, row in companies.iterrows():\n",
      "    print(row['name'])\n",
      "    results.append(get_results(row['name']))\n",
      "153/10:\n",
      "def google_search(company):\n",
      "    url = 'https://www.google.com/search?q=' + company\n",
      "    cookies = {\n",
      "        'CONSENT': 'YES+cb.20220415-07-p0.en+FX+135',\n",
      "    }\n",
      "    response = requests.get(url, cookies=cookies)    \n",
      "    return response\n",
      "\n",
      "def get_results(company):\n",
      "    # Extract results from Google search\n",
      "    response = google_search(company)\n",
      "    print(response.text)\n",
      "    soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "    search_results = soup.find('div', {'class': 'g'})\n",
      "    print(search_results)\n",
      "    # Extract the telephone number (if any)\n",
      "    telephone_number = None\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'^tel:', tag['href']):\n",
      "            telephone_number = tag['href'][4:]\n",
      "            break\n",
      "\n",
      "    # Extract the company social media links (if any)\n",
      "    social_media_links = {}\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        if re.search(r'twitter.com', tag['href']):\n",
      "            social_media_links['twitter'] = tag['href']\n",
      "        elif re.search(r'linkedin.com', tag['href']):\n",
      "            social_media_links['linkedin'] = tag['href']\n",
      "        elif re.search(r'facebook.com', tag['href']):\n",
      "            social_media_links['facebook'] = tag['href']\n",
      "\n",
      "    # Extract all links from the search result page\n",
      "    all_links = []\n",
      "    for tag in search_results.find_all('a', href=True):\n",
      "        all_links.append(tag['href'])\n",
      "    \n",
      "     # Return as a dictionary\n",
      "    time.sleep(random.randint(1, 3))\n",
      "    return {\n",
      "        'company': company,\n",
      "        'telephone_number': telephone_number,\n",
      "        'social_media_links': social_media_links,\n",
      "        'all_links': all_links\n",
      "    }\n",
      "153/11:\n",
      "# Get results for all companies\n",
      "results = []\n",
      "for idx, row in companies.iterrows():\n",
      "    print(row['name'])\n",
      "    results.append(get_results(row['name']))\n",
      "153/12:\n",
      "import requests\n",
      "import pandas as pd\n",
      "from googlesearch import search\n",
      "companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')\n",
      "154/1:\n",
      "import requests\n",
      "import pandas as pd\n",
      "from googlesearch import search\n",
      "companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')\n",
      "155/1:\n",
      "import requests\n",
      "import pandas as pd\n",
      "from googlesearch import search\n",
      "companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')\n",
      "155/2:\n",
      "# Get results for all companies\n",
      "companies\n",
      "155/3:\n",
      "results = search(\"Club Noah B.V.\", num_results = 10)\n",
      "results\n",
      "155/4:\n",
      "results = search(\"Club Noah B.V.\", num_results = 10, advanced = True)\n",
      "results\n",
      "155/5:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/6:\n",
      "results = search(\"Club Noah B.V.\", num_results = 100, advanced = True)\n",
      "results\n",
      "155/7:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/8:\n",
      "results = search(\"Puur Bouwadvies\", num_results = 15, advanced = True)\n",
      "results\n",
      "155/9:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/10:\n",
      "results = search(\"Puur Bouwadvies\", num_results = 15, advanced = True)\n",
      "results\n",
      "155/11:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/12:\n",
      "results = search(\"Sonnem Holding B.V.\", num_results = 15, advanced = True)\n",
      "results\n",
      "155/13:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/14:\n",
      "results = search(\"Sonnem Holding\", num_results = 15, advanced = True)\n",
      "results\n",
      "155/15:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/16:\n",
      "results = search(\"Val The Label\", num_results = 15, advanced = True)\n",
      "results\n",
      "155/17:\n",
      "for result in results:\n",
      "    print(result)\n",
      "155/18:\n",
      "results = search(\"Kyos Energy\", num_results = 15, advanced = True)\n",
      "results\n",
      "155/19:\n",
      "for result in results:\n",
      "    print(result)\n",
      "152/1:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])\n",
      "152/2: w_daily\n",
      "152/3:\n",
      "# Dump the IPYNB environment for reproducibility\n",
      "!jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=-1 --output-dir=../output/ ../notebooks/01-eda.ipynb\n",
      "152/4:\n",
      "# Dump the IPYNB environment for reproducibility\n",
      "import dill as pickle\n",
      "pickle.dump_session('notebook_env.db')\n",
      "152/5:\n",
      "# Load the IPYNB environment for reproducibility\n",
      "import dill as pickle\n",
      "pickle.load_session('notebook_env.db')\n",
      "152/6:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/7:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/8:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "152/9:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/10:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(2000,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/11:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/12:\n",
      "# Manipulate\n",
      "# Add average temperature to daily, max + min / 2\n",
      "w_daily['temperature_2m_avg'] = (w_daily['temperature_2m_max'] + w_daily['temperature_2m_min']) / 2\n",
      "152/13:\n",
      "# Energy\n",
      "\n",
      "# Connect to remote database\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "# Create data retrieval functions\n",
      "def get_hourly_entsoe_generation_production(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(biomass) AS biomass,\n",
      "    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,\n",
      "    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,\n",
      "    AVG(fossil_gas) AS fossil_gas,\n",
      "    AVG(fossil_hard_coal) AS fossil_hard_coal,\n",
      "    AVG(fossil_oil) AS fossil_oil,\n",
      "    AVG(geothermal) AS geothermal,\n",
      "    AVG(hydro_pumped_storage) AS hydro_pumped_storage,\n",
      "    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,\n",
      "    AVG(hydro_water_reservoir) AS hydro_water_reservoir,\n",
      "    AVG(nuclear) AS nuclear,\n",
      "    AVG(other) AS other,\n",
      "    AVG(other_renewable) AS other_renewable,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(waste) AS waste, \n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore\n",
      "    FROM scraper.entsoe_generation_production\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(solar) AS solar,\n",
      "    AVG(wind_offshore) AS wind_offshore,\n",
      "    AVG(wind_onshore) AS wind_onshore,\n",
      "    AVG(forecasted_load) AS forecasted_load,\n",
      "    AVG(actual_aggregated) AS actual_aggregated\n",
      "    FROM scraper.entsoe_da_forecast\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "\n",
      "def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):\n",
      "    query = f\"\"\"\n",
      "    SELECT \n",
      "    date_trunc('hour', datetime_start_utc) AS \"timestamp\", \n",
      "    price_area, \n",
      "    country_code,\n",
      "    AVG(price) AS price\n",
      "    FROM scraper.entsoe_da_prices\n",
      "    WHERE price_area IN ('{\"','\".join(country_list)}')\n",
      "    AND \"datetime_start_utc\" BETWEEN '{start_date}' AND '{end_date}'\n",
      "    GROUP BY \"timestamp\",\"price_area\", \"country_code\"\n",
      "    ORDER BY \"timestamp\"\n",
      "    \"\"\"\n",
      "    server.start()\n",
      "    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "    df = load_query(connection, query)\n",
      "    server.stop()\n",
      "    return df\n",
      "\n",
      "hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))\n",
      "152/14:\n",
      "# Dump the IPYNB environment for reproducibility\n",
      "import dill as pickle\n",
      "pickle.dump_session('notebook_env.db')\n",
      "152/15:\n",
      "lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall'))\n",
      "# Resample the weather data (hourly, daily) to the desired frequency\n",
      "del server\n",
      "152/16:\n",
      "# Dump the IPYNB environment for reproducibility\n",
      "import dill as pickle\n",
      "pickle.dump_session('notebook_env.db')\n",
      "152/17:\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])\n",
      "152/18:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])\n",
      "152/19:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])\n",
      "152/20:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "    break\n",
      "152/21:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "    break\n",
      "df\n",
      "152/22:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "for city in w_daily['city'].unique():\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "    break\n",
      "df\n",
      "152/23:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','London','Brussels']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/24:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/25:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/26:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/27:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.5)\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/28:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.25)\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/29:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.25)\n",
      "    plt.legend(loc='bottom right')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/30:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.25)\n",
      "    plt.legend(loc='bottom right') \n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/31:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.25)\n",
      "    plt.legend(loc='bottom right')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/32:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.25)\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/33:\n",
      "import matplotlib.pyplot as plt\n",
      "from statsmodels.graphics.api import qqplot\n",
      "from statsmodels.tsa.stattools import adfuller\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
      "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults\n",
      "\n",
      "# Let the index be the date\n",
      "temps_decomposition = w_daily.set_index('time')\n",
      "temps_decomposition.sort_index(inplace=True)\n",
      "# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)\n",
      "\n",
      "def plot_rolling_mean_and_variance(df):\n",
      "    plt.figure(figsize=(12,6))\n",
      "    plt.subplot(211)\n",
      "    plt.plot(df['temperature_2m_avg'], label='Original')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')\n",
      "\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'{df[\"city\"].unique()[0]}: Temperature with rolling mean ')\n",
      "    plt.subplot(212)\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')\n",
      "    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')\n",
      "    # Increase size between subplots\n",
      "    plt.subplots_adjust(hspace=0.25)\n",
      "    plt.legend(loc='best')\n",
      "    plt.title(f'Rolling Variance')\n",
      "    plt.show()\n",
      "\n",
      "# Plot for all cities\n",
      "print(temps_decomposition['city'].unique())\n",
      "cities = ['Amsterdam','Debrecen','Berlin','Brussels', 'Paris', 'Vienna']\n",
      "for city in cities:\n",
      "    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)\n",
      "    plot_rolling_mean_and_variance(df)\n",
      "152/34:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])\n",
      "# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var\n",
      "summary_df = summary_df.pivot_table(index=['Country', 'City', 'Season'], columns=['variable', 'statistic'], values='value').reset_index()\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/35:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var\n",
      "summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable', 'statistic'], values='value').reset_index()\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/36:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var\n",
      "summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])\n",
      "summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
      "\n",
      "#summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable', 'statistic'], values='value').reset_index()\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/37:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var\n",
      "summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable'], values='value').reset_index()\n",
      "summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])\n",
      "summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
      "\n",
      "#\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/38:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "print(summary_df.head())\n",
      "# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var\n",
      "summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable'], values='value').reset_index()\n",
      "summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])\n",
      "summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
      "\n",
      "#\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/39:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name='variable', value_name='value')\n",
      "print(summary_df.head())\n",
      "summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])\n",
      "summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
      "\n",
      "#\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/40:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'])\n",
      "print(summary_df.head())\n",
      "summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])\n",
      "summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
      "\n",
      "#\n",
      "summary_df\n",
      "# Pivot\n",
      "#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])\n",
      "\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/41:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# To latex\n",
      "print(pivot_summary.to_latex())\n",
      "152/42:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# To latex\n",
      "print(summary_df.to_latex())\n",
      "152/43:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "pivot_summary\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/44:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Pivot\n",
      "print(summary_df.head())\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "#print(pivot_summary.to_latex())\n",
      "152/45:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Pivot the multiindex columns to single index long format columns using variable, statistic\n",
      "#summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Pivot\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "print(summary_df.to_latex())\n",
      "152/46:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "print(pivot_summary.to_latex())\n",
      "152/47:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "print(pivot_summary)\n",
      "152/48:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'season'])\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "pivot_summary\n",
      "152/49:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "pivot_summary\n",
      "152/50:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'season', 'statistic'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "pivot_summary\n",
      "152/51:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'season', 'statistic'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "print(pivot_summary.to_latex())\n",
      "152/52:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# To latex\n",
      "print(pivot_summary.to_latex())\n",
      "152/53:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/54:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/55:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', axis = 1 , subset = [variable,statistic])\n",
      "\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/56:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', axis = 1 , subset = [variable,statistic])\n",
      "\n",
      "#pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/57:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic])\n",
      "\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/58:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic])\n",
      "\n",
      "pivot_summary.style\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/59:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic])\n",
      "\n",
      "# Plot the styled dataframe\n",
      "pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/60:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic][0])\n",
      "\n",
      "# Plot the styled dataframe\n",
      "pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/61:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = [(variable,statistic)])\n",
      "\n",
      "# Plot the styled dataframe\n",
      "pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/62:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = subset)\n",
      "\n",
      "# Plot the styled dataframe\n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/63:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "#print(summary_df.head())\n",
      "#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])\n",
      "# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)\n",
      "# Important to differ it for each variable\n",
      "for variable in pivot_summary.columns.levels[0]:\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        # Apply background gradient\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)\n",
      "        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format(\"{:.3f}\")\n",
      "        subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "        print(subset)\n",
      "        pivot_summary.style.background_gradient(cmap='Blues', subset = subset)\n",
      "\n",
      "# Plot the styled dataframe\n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/64:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "for variable, cmap in gradients.items():\n",
      "    idx = pd.IndexSlice\n",
      "    subset = idx[:, :, idx[variable, :, :]]\n",
      "    df_subset = pivot_summary.loc[:, subset]\n",
      "    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)\n",
      "    pivot_summary.loc[:, subset] = df_style\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/65:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "for variable, cmap in gradients.items():\n",
      "    idx = pd.IndexSlice\n",
      "    subset = idx[:, :, idx[variable, :, :]]\n",
      "    df_subset = pivot_summary.loc[:, subset]\n",
      "    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)\n",
      "    pivot_summary.loc[:, subset] = df_style\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/66:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    df_subset = pivot_summary.loc[:, subset]\n",
      "    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)\n",
      "    pivot_summary.loc[:, subset] = df_style\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/67:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_dfs = []\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    df_subset = pivot_summary.loc[:, subset]\n",
      "    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)\n",
      "    styled_dfs.append(df_style)\n",
      "    #pivot_summary.loc[:, subset] = df_style\n",
      "styled_df = pd.concat(styled_dfs, axis=1)\n",
      "\n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_df\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/68:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    pivot_summary.loc[:, subset].style.background_gradient(cmap=cmap, axis=1)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/69:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    print(pivot_summary.loc[:, subset].style.background_gradient(cmap=cmap, axis=1))\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/70:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    pivot_summary.style.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/71:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/72:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    pivot_summary = pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/73:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_df = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    styled_df = pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_df\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/74:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    styled_pivot_summary = styled_pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/75:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/76:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    statistics = pivot_summary.columns.levels[1]\n",
      "    seasons = pivot_summary.columns.levels[2]\n",
      "    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]\n",
      "    #df_subset = pivot_summary.loc[:, subset]\n",
      "    styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/77:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'Blues'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/78:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'Blues',\n",
      "    'windgusts_10m_max': 'ocean'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/79:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'ocean'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/80:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# To latex\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/81:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/82:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format\n",
      "        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): \"{:0.3f}\".format for season in seasons})\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/83:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format\n",
      "        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): \"{:0.3f}\".format for season in seasons})\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex()\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/84:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format\n",
      "        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): \"{:0.3f}\".format for season in seasons})\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex())\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/85:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format\n",
      "        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): \"{:0.3f}\".format for season in seasons})\n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/86:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;index\"))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/87:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\"))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/88:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrule = True))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/89:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/90:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone',\n",
      "    'windgusts_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/91:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'YlOrRd',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/92:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'berlin',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/93:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'magma',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/94:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'PuRd',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/95:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'summer',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/96:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'bone',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/97:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'bone',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=-cmap, axis=1, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/98:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'bone',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/99:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/100:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below column headers\n",
      "pivot_summary.style.set_table_styles([{'selector': 'th', 'props': [('border-bottom', '2px solid black')]}])\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/101:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below each multiiindex\n",
      "styled_pivot_summary = styled_pivot_summary.set_properties(**{'border-bottom': '1px solid black'})\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/102:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below each multiiindex (e.g. below median, var, winter)\n",
      "styled_pivot_summary = styled_pivot_summary.set_properties(**{'border-bottom': '1px solid black'})\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"all;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/103:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add lines below each multiiindex (e.g. below median, var, winter)     {'selector': 'th.level0', 'props': 'border-bottom: 1px solid black;'},\n",
      "styled_pivot_summary = styled_pivot_summary.set_table_styles([{'selector': 'th.level0', 'props': 'border-bottom: 1px solid black;'}, {'selector': 'th.level1', 'props': 'border-bottom: 1px solid black;'}, {'selector': 'th.level2', 'props': 'border-bottom: 1px solid black;'}])\n",
      " \n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/104:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add lines below each multiiindex (e.g. below median, var, winter)     {'selector': 'th.level', 'props': 'border-bottom: 1px solid black;'}\n",
      "styled_pivot_summary = styled_pivot_summary.set_table_styles([{'selector': 'th.level', 'props': 'border-bottom: 1px solid black;'}])\n",
      " \n",
      " \n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/105:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below each multiiindex\n",
      "\n",
      "pivot_summary.style.set_table_styles([{'selector': 'th', 'props': [('border-bottom', '2px solid black')]}])\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/106:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below each multiiindex\n",
      "\n",
      "pivot_summary.style.set_table_styles([{'selector': 'th.level0', 'props': [('border-bottom', '2px solid black')]}])\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/107:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below each multiiindex\n",
      "\n",
      "pivot_summary.style.set_table_styles([{'selector': 'th.level1', 'props': [('border-bottom', '2px solid black')]}])\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = True, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/108:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Add clines below each multiiindex\n",
      "\n",
      "styled_pivot_summary = pivot_summary.style.set_table_styles([{'selector': 'th.level1', 'props': [('border-bottom', '2px solid black')]}])\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/109:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/110:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Function to apply custom styles to column headers\n",
      "def style_column_headers(column, level):\n",
      "    if column.name[level] in column.name:\n",
      "        return ['border-bottom: 1px solid black;' for _ in column]\n",
      "    return ['' for _ in column]\n",
      "\n",
      "# Apply custom styles to column headers\n",
      "for level in range(pivot_summary.columns.nlevels):\n",
      "    styled_pivot_summary = styled_pivot_summary.apply(style_column_headers, axis=0, level=level)\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/111:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "# Function to apply custom styles to column headers\n",
      "def style_column_headers(column, level):\n",
      "    if column.name[level] in column.name:\n",
      "        return ['border-bottom: 5px solid black;' for _ in column]\n",
      "    return ['' for _ in column]\n",
      "\n",
      "# Apply custom styles to column headers\n",
      "for level in range(pivot_summary.columns.nlevels):\n",
      "    styled_pivot_summary = styled_pivot_summary.apply(style_column_headers, axis=0, level=level)\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/112:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "def custom_to_latex(df):\n",
      "    latex_output = df.to_latex(multirow=True, escape=False, convert_css=True, clines = \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c')\n",
      "    latex_lines = latex_output.splitlines()\n",
      "    new_lines = []\n",
      "    found_header = False\n",
      "\n",
      "    for line in latex_lines:\n",
      "        new_lines.append(line)\n",
      "\n",
      "        # Find the header line and add the \\cline commands\n",
      "        if not found_header and '\\\\toprule' in line:\n",
      "            found_header = True\n",
      "            for i in range(1, df.columns.nlevels + 1):\n",
      "                new_lines.append(f\"\\\\cline{{{i + 1}-{i + df.shape[1]}}}\")\n",
      "\n",
      "    return '\\n'.join(new_lines)\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(custom_to_latex(styled_pivot_summary))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/113:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "def custom_to_latex(df):\n",
      "    latex_output = df.to_latex(escape=False, convert_css=True, clines = \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c')\n",
      "    latex_lines = latex_output.splitlines()\n",
      "    new_lines = []\n",
      "    found_header = False\n",
      "\n",
      "    for line in latex_lines:\n",
      "        new_lines.append(line)\n",
      "\n",
      "        # Find the header line and add the \\cline commands\n",
      "        if not found_header and '\\\\toprule' in line:\n",
      "            found_header = True\n",
      "            for i in range(1, df.columns.nlevels + 1):\n",
      "                new_lines.append(f\"\\\\cline{{{i + 1}-{i + df.shape[1]}}}\")\n",
      "\n",
      "    return '\\n'.join(new_lines)\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(custom_to_latex(styled_pivot_summary))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/114:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "def custom_to_latex(df):\n",
      "    latex_output = df.to_latex(convert_css=True, clines = \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c')\n",
      "    latex_lines = latex_output.splitlines()\n",
      "    new_lines = []\n",
      "    found_header = False\n",
      "\n",
      "    for line in latex_lines:\n",
      "        new_lines.append(line)\n",
      "\n",
      "        # Find the header line and add the \\cline commands\n",
      "        if not found_header and '\\\\toprule' in line:\n",
      "            found_header = True\n",
      "            for i in range(1, df.columns.nlevels + 1):\n",
      "                new_lines.append(f\"\\\\cline{{{i + 1}-{i + df.shape[1]}}}\")\n",
      "\n",
      "    return '\\n'.join(new_lines)\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(custom_to_latex(styled_pivot_summary))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/115:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/116:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "# Define a custom formatting function\n",
      "def custom_formatting(value, median, variance):\n",
      "    return f\"{median:.3f}\\n({variance:.3f})\"\n",
      "\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        \n",
      "        # Apply background gradient\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        \n",
      "        # Apply custom formatting\n",
      "        if statistic == 'median':\n",
      "            for season in seasons:\n",
      "                median_subset = (variable, 'median', season)\n",
      "                var_subset = (variable, 'var', season)\n",
      "                styled_pivot_summary = styled_pivot_summary.applymap(\n",
      "                    lambda x: custom_formatting(x, pivot_summary.loc[:, median_subset], pivot_summary.loc[:, var_subset]),\n",
      "                    subset=[median_subset]\n",
      "                )\n",
      "\n",
      "# Hide the variance values\n",
      "styled_pivot_summary = styled_pivot_summary.hide_columns([(var[0], 'var', var[2]) for var in styled_pivot_summary.columns if var[1] == 'var'])\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/117:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "def custom_formatting(median, variance):\n",
      "    return f\"{median:.3f}\\n({variance:.3f})\"\n",
      "\n",
      "formatted_pivot = pivot_summary.apply(lambda x: custom_formatting(x[(x.name[0], 'median', x.name[2])], x[(x.name[0], 'var', x.name[2])]), axis=1)\n",
      "formatted_pivot.columns = formatted_pivot.columns.droplevel(1)\n",
      "\n",
      "styled_formatted_pivot = formatted_pivot.style\n",
      "\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, season) for season in seasons]\n",
      "        styled_formatted_pivot = styled_formatted_pivot.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "\n",
      "print(styled_formatted_pivot.to_latex(convert_css=True, clines=\"skip-last;data\", hrules=False, multicol_align='c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/118:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/119:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'count': 'temperature_2m_avg_count'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/120:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/121:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "print(summary_df.columns)\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/122:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "summary_df.columns = ['_'.join(col).strip() for col in summary_df.columns.values]\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/123:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "summary_df.columns = ['_'.join(col).strip() for col in summary_df.columns.values]\n",
      "print(summary_df.columns)\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/124:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "summary_df.columns = ['_'.join(col).strip('_') for col in summary_df.columns.values]\n",
      "# Remove leading and trailing underscores\n",
      "print(summary_df.columns)\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/125:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var', 'count'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations\n",
      "     \n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "summary_df.columns = ['_'.join(col).strip('_') for col in summary_df.columns.values]\n",
      "# Remove leading and trailing underscores\n",
      "print(summary_df.columns)\n",
      "# Rename the count column\n",
      "summary_df.rename(columns={'temperature_2m_avg_count': 'count'}, inplace=True)\n",
      "\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/126:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations that ends up in the first level of the index\n",
      "    'time': 'count',\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "print(summary_df.columns)\n",
      "\n",
      "# Remove leading and trailing underscores\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/127:\n",
      "# Provide a summary table:\n",
      "# First column index is the variable, e.g. temperature_2m\n",
      "# Second column index is the season\n",
      "# First row index is the country, e.g. AT\n",
      "# Second row index is the city, e.g. Vienna\n",
      "df = w_daily \n",
      "df['date'] = df['time'].dt.date\n",
      "df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))\n",
      "\n",
      "summary_df = df.groupby(['country', 'city', 'season']).agg({\n",
      "    'temperature_2m_avg': ['median', 'var'],\n",
      "    'windspeed_10m_max': ['median', 'var'],\n",
      "    # Add a count of the number of observations that ends up in the first level of the index\n",
      "    'time': 'count',\n",
      "    #'windgusts_10m_max': ['median', 'var'],\n",
      "})\n",
      "\n",
      "# Reset index\n",
      "summary_df = summary_df.reset_index()\n",
      "# Last column becomes count\n",
      "summary_df.columns[-1] = 'count'\n",
      "print(summary_df.head(10))\n",
      "# Remove leading and trailing underscores\n",
      "# Melt to normal format again\n",
      "summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')\n",
      "# Sort by country and city\n",
      "summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])\n",
      "# Pivot\n",
      "pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')\n",
      "\n",
      "# Define the background gradients for each variable\n",
      "gradients = {\n",
      "    'temperature_2m_avg': 'coolwarm',\n",
      "    'windspeed_10m_max': 'bone'\n",
      "}\n",
      "#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]\n",
      "# Loop over the gradients and apply them to the appropriate subsets of the DataFrame\n",
      "styled_pivot_summary = pivot_summary.style\n",
      "for variable, cmap in gradients.items():\n",
      "    for statistic in pivot_summary.columns.levels[1]:\n",
      "        seasons = pivot_summary.columns.levels[2]\n",
      "        subset = [(variable, statistic, season) for season in seasons]\n",
      "        #df_subset = pivot_summary.loc[:, subset]\n",
      "        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)\n",
      "        # Float format 3 decimals\n",
      "        \n",
      "    \n",
      "#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])\n",
      "styled_pivot_summary = styled_pivot_summary.format(\"{:.3f}\")\n",
      "\n",
      "\n",
      "# Styler to latex\n",
      "print(styled_pivot_summary.to_latex(convert_css=True, clines= \"skip-last;data\", hrules = False, multicol_align = 'c', multirow_align='c'))\n",
      "\n",
      "#print(pivot_summary.to_latex(float_format=\"{{:0.3f}}\".format))\n",
      "152/128:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "152/129:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    [location_eindhoven],\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "152/130:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    #'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1960,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/131:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/132:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : [],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1960,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/133:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/134:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')\n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = pd.DataFrame()\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = pd.DataFrame()      \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "152/135:\n",
      "server = SSHTunnelForwarder(\n",
      "    (REMOTE_HOST, 22),\n",
      "    ssh_username=REMOTE_USER,\n",
      "    ssh_password=REMOTE_PASS,\n",
      "    remote_bind_address=('localhost', DB_PORT)\n",
      ")\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT datetime_start_utc AS \"time\", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore\n",
      "FROM scraper.entsoe_generation_production\n",
      "WHERE price_area IN ('NL') AND \"datetime_start_utc\" BETWEEN '2023-01-01' AND '2023-12-31'\n",
      "GROUP BY \"time\",\"price_area\"\n",
      "ORDER BY \"time\"\n",
      "\"\"\"\n",
      "server.start()\n",
      "connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)\n",
      "df = load_query(connection, query)\n",
      "server.stop()\n",
      "\n",
      "#print(df.head(10))\n",
      "\n",
      "location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}\n",
      "w_variables = {\n",
      "    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']\n",
      "}\n",
      "\n",
      "print(w_variables)\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    dt.date(2020,1,1), \n",
      "    dt.date(2022,12,31), \n",
      "    [location_eindhoven],\n",
      "    weather_variables=w_variables)\n",
      "\n",
      "w_hourly\n",
      "152/136:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : [],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1960,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/137:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/138:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max','temperature_2m_min'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1960,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/139:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/140:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max','temperature_2m_min'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1960,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/141:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/142:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max','temperature_2m_min'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1945,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/143:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/144:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "def convert_time(time_str):\n",
      "    try:\n",
      "        return pd.to_datetime(time_str, format='%Y-%m-%dT%H:%M')\n",
      "    except ValueError:\n",
      "        return pd.to_datetime(time_str.replace(\"-\", \"\"), format='%Y-%m-%dT%H:%M')\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            #daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M') \n",
      "            daily['time'] = daily['time'].apply(convert_time)\n",
      "            \n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = hourly['time'].apply(convert_time)\n",
      "\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "    Failed to retrieve weather data from OpenMeteo API for (48.148598-17.107748) between 1945-01-01-2023-01-01: time data \"1945-01-01T-23:00\" at position 1 doesn't match format specified\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "152/145:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "def convert_time(time_str):\n",
      "    try:\n",
      "        return pd.to_datetime(time_str, format='%Y-%m-%dT%H:%M')\n",
      "    except ValueError:\n",
      "        return pd.to_datetime(time_str.replace(\"-\", \"\"), format='%Y-%m-%dT%H:%M')\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            #daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M') \n",
      "            daily['time'] = daily['time'].apply(convert_time)\n",
      "            \n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = hourly['time'].apply(convert_time)\n",
      "\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "    \n",
      "    Failed to retrieve weather data from OpenMeteo API for (48.148598-17.107748) between 1945-01-01-2023-01-01: time data \"1945-01-01T-23:00\" at position 1 doesn't match format specified\n",
      "    \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "152/146:\n",
      "# --- Imports ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import urllib.parse\n",
      "import datetime as dt\n",
      "import requests\n",
      "import psycopg2\n",
      "import sys\n",
      "\n",
      "from secret import *\n",
      "from sshtunnel import SSHTunnelForwarder\n",
      "\n",
      "# --- Constants ---\n",
      "# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)\n",
      "hourly_weather_variables = {\n",
      "    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],\n",
      "    'humidity' : ['relativehumidity_2m'],\n",
      "    'pressure' : ['pressure_msl', 'surface_pressure'],\n",
      "    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],\n",
      "    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],\n",
      "    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],\n",
      "    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],\n",
      "    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],\n",
      "    'weather' : ['weathercode'],\n",
      "    'snow' : ['snow_depth', 'freezinglevel_height'],\n",
      "    'visibility' : ['visibility'],\n",
      "    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']\n",
      "}\n",
      "daily_weather_variables = {\n",
      "    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],\n",
      "    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],\n",
      "    'weather' : ['weathercode', 'sunrise', 'sunset'],\n",
      "    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],\n",
      "    'radiation' : ['shortwave_radiation_sum'],\n",
      "    'energy' : ['et0_fao_evapotranspiration'],\n",
      "    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']\n",
      "}\n",
      "# --- Weather Retrieval Functions ---\n",
      "def convert_time(time_str):\n",
      "    try:\n",
      "        return pd.to_datetime(time_str, format='%Y-%m-%dT%H:%M')\n",
      "    except ValueError:\n",
      "        return pd.to_datetime(time_str.replace(\"-\", \"\"), format='%Y-%m-%dT%H:%M')\n",
      "\n",
      "# Base OpenMeteo API call\n",
      "def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:\n",
      "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
      "    hourly_params = weather_variables.get('hourly', [])\n",
      "    daily_params = weather_variables.get('daily', [])\n",
      "    params = {'latitude' : lat, \n",
      "              'longitude' : lat, \n",
      "              'start_date' : start.strftime('%Y-%m-%d'), \n",
      "              'end_date' : end.strftime('%Y-%m-%d'), \n",
      "              'hourly' : ','.join(hourly_params),\n",
      "              'daily' : ','.join(daily_params),\n",
      "              'timezone' : 'UTC'\n",
      "              }\n",
      "    try:\n",
      "        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))\n",
      "        data = response.json()\n",
      "        # Daily Data\n",
      "        if 'daily' in data:\n",
      "            daily = pd.DataFrame(data['daily'])\n",
      "            #daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M') \n",
      "            daily['time'] = daily['time'].apply(convert_time)\n",
      "            \n",
      "            daily['latitude'] = lat\n",
      "            daily['longitude'] = lon\n",
      "        else:\n",
      "            daily = None\n",
      "        # Hourly Data\n",
      "        if 'hourly' in data:\n",
      "            hourly = pd.DataFrame(data['hourly'])\n",
      "            hourly['time'] = hourly['time'].apply(convert_time)\n",
      "\n",
      "            hourly['latitude'] = lat\n",
      "            hourly['longitude'] = lon\n",
      "        else:\n",
      "            hourly = None \n",
      "        \n",
      "        return daily, hourly\n",
      "    except Exception as e:\n",
      "        print(f\"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}\")\n",
      "        \n",
      "# Request an array of locations using OpenMeteo API\n",
      "def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:\n",
      "    daily, hourly = [], []\n",
      "    for location in locations_array:\n",
      "        lat, lon = location['coordinates']\n",
      "        # Get rest of location data (e.g. name, elevation, etc.)\n",
      "        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}\n",
      "        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)\n",
      "        if daily_data is not None:\n",
      "            daily_data = daily_data.assign(**location_metadata)\n",
      "            daily.append(daily_data)\n",
      "        if hourly_data is not None:\n",
      "            hourly_data = hourly_data.assign(**location_metadata)\n",
      "            hourly.append(hourly_data)\n",
      "    return pd.concat(daily), pd.concat(hourly)\n",
      "\n",
      "# --- Database Retrieval Functions (SSH) ---\n",
      "def load_connection(user: str,password: str,host: str,database: str, port):\n",
      "    pg_connection_dict = {\n",
      "    'dbname': database,\n",
      "    'user': user,\n",
      "    'password': password,\n",
      "    'port': port,\n",
      "    'host': host\n",
      "    }\n",
      "    keepalive_kwargs = {\"keepalives\": 1,\"keepalives_idle\": 250,\"keepalives_interval\": 5,\"keepalives_count\": 5}\n",
      "    try:\n",
      "        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)\n",
      "        return connection\n",
      "    except(Exception, EnvironmentError) as e:\n",
      "        raise Exception (\"Error in connection, }\".format(e))\n",
      "        \n",
      "def load_query(connection, query) -> pd.DataFrame:\n",
      "    try:\n",
      "        cursor = connection.cursor()\n",
      "        cursor.execute(query) \n",
      "        record = cursor.fetchall()\n",
      "        # Convert to pandas\n",
      "        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])\n",
      "        return df_record\n",
      "\n",
      "    except(Exception,EnvironmentError) as e:\n",
      "        raise Exception (\"Error while fetching data from postgres, }\".format(e))\n",
      "    finally:\n",
      "        # Close connection\n",
      "        if(connection):\n",
      "            cursor.close()\n",
      "152/147:\n",
      "TSO_COUNTRIES = {\n",
      "\"SK\" : \"Slovakia\",\n",
      "\"DK\" : \"Denmark\",\n",
      "\"LT\" : \"Lithuania\",\n",
      "\"SI\" : \"Slovenia\",\n",
      "\"CZ\" : \"Czech Republic\",\n",
      "\"FR\" : \"France\",\n",
      "\"LV\" : \"Latvia\",\n",
      "\"RO\" : \"Romania\",\n",
      "\"NL\" : \"Netherlands\",\n",
      "\"EE\" : \"Estonia\",\n",
      "\"HU\" : \"Hungary\",\n",
      "\"AT\" : \"Austria\",\n",
      "\"FI\" : \"Finland\",\n",
      "\"PL\" : \"Poland\",\n",
      "\"ES\" : \"Spain\",\n",
      "\"PT\" : \"Portugal\",\n",
      "\"NO\" : \"Norway\",\n",
      "\"BG\" : \"Bulgaria\",\n",
      "\"IT\" : \"Italy\",\n",
      "\"HR\" : \"Croatia\",\n",
      "\"SE\" : \"Sweden\",\n",
      "\"GR\" : \"Greece\",\n",
      "\"CH\" : \"Switzerland\",\n",
      "\"BE\" : \"Belgium\",\n",
      "\"DE\" : \"Germany\"\n",
      "}\n",
      "\n",
      "# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia\n",
      "LOCATIONS = [\n",
      "    {\"city\" : \"Bratislava\", \"coordinates\": (48.148598, 17.107748), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Košice\", \"coordinates\": (48.720595, 21.257698), \"country\": \"SK\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Copenhagen\", \"coordinates\": (55.676097, 12.568337), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Aarhus\", \"coordinates\": (56.162939, 10.203921), \"country\": \"DK\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Vilnius\", \"coordinates\": (54.687157, 25.279652), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Kaunas\", \"coordinates\": (54.898521, 23.903597), \"country\": \"LT\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Ljubljana\", \"coordinates\": (46.056946, 14.505751), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Maribor\", \"coordinates\": (46.554650, 15.646049), \"country\": \"SI\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Prague\", \"coordinates\": (50.075539, 14.437800), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brno\", \"coordinates\": (49.195060, 16.606837), \"country\": \"CZ\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Paris\", \"coordinates\": (48.856614, 2.352222), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Marseille\", \"coordinates\": (43.296482, 5.369780), \"country\": \"FR\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Riga\", \"coordinates\": (56.949649, 24.105186), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Daugavpils\", \"coordinates\": (55.874296, 26.536963), \"country\": \"LV\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Bucharest\", \"coordinates\": (44.426765, 26.102537), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Cluj-Napoca\", \"coordinates\": (46.771210, 23.623635), \"country\": \"RO\", \"region\": \"Eastern Europe\"},\n",
      "    {\"city\" : \"Amsterdam\", \"coordinates\": (52.370216, 4.895168), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Rotterdam\", \"coordinates\": (51.920179, 4.481774), \"country\": \"NL\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Tallinn\", \"coordinates\": (59.436962, 24.753574), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Tartu\", \"coordinates\": (58.380624, 26.725056), \"country\": \"EE\", \"region\": \"Baltic\"},\n",
      "    {\"city\" : \"Budapest\", \"coordinates\": (47.497913, 19.040236), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Debrecen\", \"coordinates\": (47.531604, 21.627312), \"country\": \"HU\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Vienna\", \"coordinates\": (48.208176, 16.373819), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Graz\", \"coordinates\": (47.070714, 15.439504), \"country\": \"AT\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Helsinki\", \"coordinates\": (60.169856, 24.938379), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Turku\", \"coordinates\": (60.451810, 22.266630), \"country\": \"FI\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Warsaw\", \"coordinates\": (52.229676, 21.012229), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Lodz\", \"coordinates\": (51.759250, 19.455983), \"country\": \"PL\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Madrid\", \"coordinates\": (40.416775, -3.703790), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Barcelona\", \"coordinates\": (41.385064, 2.173403), \"country\": \"ES\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Lisbon\", \"coordinates\": (38.722252, -9.139337), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Porto\", \"coordinates\": (41.157944, -8.629105), \"country\": \"PT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Oslo\", \"coordinates\": (59.913869, 10.752245), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Bergen\", \"coordinates\": (60.392050, 5.322050), \"country\": \"NO\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Sofia\", \"coordinates\": (42.697708, 23.321868), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Plovdiv\", \"coordinates\": (42.135407, 24.745290), \"country\": \"BG\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Rome\", \"coordinates\": (41.902782, 12.496366), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Milan\", \"coordinates\": (45.464204, 9.189982), \"country\": \"IT\", \"region\": \"Mediterranean\"},\n",
      "    {\"city\" : \"Zagreb\", \"coordinates\": (45.815011, 15.981919), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Split\", \"coordinates\": (43.508132, 16.440193), \"country\": \"HR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Stockholm\", \"coordinates\": (59.329323, 18.068581), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Gothenburg\", \"coordinates\": (57.708870, 11.974560), \"country\": \"SE\", \"region\": \"Scandinavia\"},\n",
      "    {\"city\" : \"Athens\", \"coordinates\": (37.983810, 23.727539), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Thessaloniki\", \"coordinates\": (40.640060, 22.944420), \"country\": \"GR\", \"region\": \"Balkans\"},\n",
      "    {\"city\" : \"Zürich\", \"coordinates\": (47.376887, 8.541694), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Geneva\", \"coordinates\": (46.204391, 6.143158), \"country\": \"CH\", \"region\": \"Central Europe\"},\n",
      "    {\"city\" : \"Brussels\", \"coordinates\": (50.850340, 4.351710), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Antwerp\", \"coordinates\": (51.219448, 4.402464), \"country\": \"BE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Berlin\", \"coordinates\": (52.520007, 13.404954), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "    {\"city\" : \"Hamburg\", \"coordinates\": (53.551086, 9.993682), \"country\": \"DE\", \"region\": \"North Western Europe\"},\n",
      "]\n",
      "\n",
      "W_VARIABLES = {\n",
      "    'daily' : ['temperature_2m_max','temperature_2m_min'],\n",
      "    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']\n",
      "}\n",
      "\n",
      "START_DATE = dt.date(1950,1,1)\n",
      "END_DATE = dt.date(2023,1,1)\n",
      "152/148:\n",
      "# Weather\n",
      "w_daily, w_hourly = obtain_openmeteo_historical_array(\n",
      "    START_DATE, \n",
      "    END_DATE, \n",
      "    LOCATIONS,\n",
      "    weather_variables=W_VARIABLES)\n",
      "\n",
      "w_daily\n",
      "152/149:\n",
      "# Dump the IPYNB environment for reproducibility\n",
      "import dill as pickle\n",
      "pickle.dump_session('notebook_env.db')\n",
      "152/150: del server\n",
      "152/151: del server\n",
      "152/152:\n",
      "# Dump the IPYNB environment for reproducibility\n",
      "import dill as pickle\n",
      "pickle.dump_session('notebook_env.db')\n",
      "152/153:\n",
      "del connection\n",
      "del server\n",
      "152/154:\n",
      "import seaborn as sns\n",
      "\n",
      "df = w_hourly.copy()\n",
      "# groupby region, city, and country and resample to weekly\n",
      "df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()\n",
      "df['label'] = df['city'] + ' (' + df['country'] + ')'\n",
      "\n",
      "fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)\n",
      "for i, region in enumerate(df['region'].unique()):\n",
      "    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])\n",
      "    # Smaller line width\n",
      "    for j in range(0, len(axes[i].lines)):\n",
      "        axes[i].lines[j].set_linewidth(1.5)\n",
      "        axes[i].lines[j].set_alpha(0.5)\n",
      "    axes[i].set_title(region)\n",
      "    # Set axis labels to '' to avoid overlapping\n",
      "    axes[i].set_ylabel('Temperature (C)')\n",
      "    axes[i].set_xlabel('')\n",
      "    # Rotate xticks\n",
      "    axes[i].tick_params(axis='x', rotation=45)\n",
      "    # Legends outside the plot\n",
      "    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
      "    # Create more space between the plots\n",
      "fig.subplots_adjust(hspace=0.5)\n",
      "plt.show()\n",
      "   1: _ih[-15:]\n",
      "   2: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = hourly_entsoe_day_ahead_prices.copy()\n",
    "df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 5), sharex=False)\n",
    "i=0\n",
    "sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)\n",
    "# Smaller line width\n",
    "for j in range(0, len(axes.lines)):\n",
    "    axes.lines[j].set_linewidth(1.5)\n",
    "    axes.lines[j].set_alpha(0.5)\n",
    "axes.set_title('Day Ahead Prices')\n",
    "# Set axis labels to '' to avoid overlapping\n",
    "axes.set_ylabel('Price (EUR/MWh)')\n",
    "axes.set_xlabel('')\n",
    "# Rotate xticks\n",
    "axes.tick_params(axis='x', rotation=45)\n",
    "# Legends outside the plot\n",
    "axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)\n",
    "# Create more space between the plots\n",
    "fig.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4: Temporaral variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.1\n",
      "0.11.2\n"
     ]
    }
   ],
   "source": [
    "# Print numpy version\n",
    "print(np.__version__)\n",
    "import seaborn as sns\n",
    "print(sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x300 with 4 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Merge the three dataframes\n",
    "df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])\n",
    "df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])\n",
    "# Select only NL and DE\n",
    "df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU'])]\n",
    "\n",
    "# Sum over price_area\n",
    "df = df.groupby(['timestamp', 'country_code']).sum().reset_index()\n",
    "\n",
    "# Divide the columns from hourly_entsoe_generation_production with the forecasted_load\n",
    "for col in hourly_entsoe_generation_production.columns[3:]:\n",
    "    df[col] = df[col] / df['forecasted_load']\n",
    "\n",
    "# Create new columns for quarter and month\n",
    "df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter\n",
    "df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
    "# Group the data by date\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "# Change year of date to 2020\n",
    "df['date'] = df['date'].apply(lambda x: x.replace(year=2020))\n",
    "\n",
    "# Group the data by quarter, month, and country_code, and calculate median and std\n",
    "# 25th Percentile\n",
    "def q25(x):    \n",
    "    return x.quantile(0.25)\n",
    "# 75th Percentile\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "# Get mean, std, 25th percentile, 75th percentile, min, and max\n",
    "grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({\n",
    "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
    "})\n",
    "\n",
    "grouped_months = df.groupby(['date','country_code']).agg({\n",
    "    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'solar': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],\n",
    "    'price': ['median', 'std', 'min', 'max', q25, q75]\n",
    "})\n",
    "\n",
    "# Reset index and rename columns\n",
    "grouped_quarters = grouped_quarters.reset_index()\n",
    "grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]\n",
    "# Rename country_code_ to country_code\n",
    "grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})\n",
    "\n",
    "grouped_months = grouped_months.reset_index()\n",
    "grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]\n",
    "# Rename country_code_ to country_code\n",
    "grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})\n",
    "grouped_months['quarter'] = 5\n",
    "\n",
    "# Set up the plot\n",
    "g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)\n",
    "\n",
    "# Plot wind, solar, renewable, fossil, and price\n",
    "g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')\n",
    "g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')\n",
    "g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')\n",
    "\n",
    "g.map(plt.plot, 'x_label', 'solar_median', color='orange')\n",
    "g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')\n",
    "g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')\n",
    "\n",
    "g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')\n",
    "g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')\n",
    "g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')\n",
    " \n",
    "#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')\n",
    "#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')\n",
    "#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')\n",
    "\n",
    "#g.map(plt.plot, 'hour', 'price_median', color='black')\n",
    "#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')\n",
    "#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')\n",
    "\n",
    "# Double axis for price\n",
    "# Set plot title, axis labels, and legend\n",
    "g.fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9bf2ab6eb6547abc2a82869035e859262b750b4cc8657698d32a579b0a247f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
