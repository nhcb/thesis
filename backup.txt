 1/1:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points)
 1/2:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(25))
 1/3:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(25))
print(masked_points.columns)
 1/4:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(25)[['point_key','point_label','point_type']])
print(masked_points.columns)
 1/5:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(20)[['point_key','point_label','point_type']])
print(masked_points.columns)
 1/6:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
print(masked_points.columns)
 1/7:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 1/8:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 1/9:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
1/10:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
1/11:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
1/12:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
1/13:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
1/14:
from entsog import EntsogPandasClient
import pandas as pd

print(entsog.__version__)
client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
1/15:
from entsog import EntsogPandasClient
import pandas as pd

print(EntsogPandasClient.__version__)
client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
1/16:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
1/17:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 2/1:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 2/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 2/3:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 3/1:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20171201', tz='Europe/Brussels')
end = pd.Timestamp('20180101', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 3/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 3/3:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


data['value'] = -1* data['value'][data['direction_key'] == 'exit']

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})


(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_
)
 3/4:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


data['value'] = -1* data['value'][data['direction_key'] == 'exit']

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})


(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
 3/5:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line

print(data.head(3))

data['value'] = -1* data['value'][data['direction_key'] == 'exit']

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})


(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
 3/6:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line

print(data.head(3))

data['value'] = -1* data[data['direction_key'] == 'exit']['value']

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})


(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
 3/7:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line
print(data.index)
print(data.head(3))

data['value'] = -1* data[data['direction_key'] == 'exit']['value']

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})


(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
 3/8:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

data['value'] = -1* data[data['direction_key'] == 'exit']['value']

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
 3/9:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

data['net_value'] = data['value']
data['net_value'] = -1* data[data['direction_key'] == 'exit']['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/10:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

data['net_value'] = data['value']
data['net_value'] = -1* data[data['direction_key'] == 'exit']['net_value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/11:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

data['net_value'] = data['value']
data['net_value'] = -1* data[(data['direction_key'] == 'exit')]['net_value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/12:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

data['net_value'] = data['value']
print((data['direction_key'] == 'exit'))


data['net_value'] = -1* data[(data['direction_key'] == 'exit')]['net_value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/13:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

data['net_value'] = data['value']
mask = (data['direction_key'] == 'exit')
print(data[mask])


data['net_value'] = -1* data[(data['direction_key'] == 'exit')]['net_value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/14:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

mask = (data['direction_key'] == 'exit')
print(data[mask])


data['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/15:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

mask = (data['direction_key'] == 'exit')
print(data[mask])


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/16:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line


print(data.head(3))

mask = (data['direction_key'] == 'exit')
print(data[mask])


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_grid('~point_label')
)
3/17:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')
print(data[mask])


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/18:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')
print(data[mask])


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'})

print(data_grouped)

(
    ggplot(data_grouped, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/19:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)

(
    ggplot(data_grouped, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/20:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)

(
    ggplot(data_grouped, aes(x='period_from', y='value'))
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    #+ facet_wrap('~point_label')
)
3/21:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    #+ facet_wrap('~point_label')
)
3/22:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    #+ facet_wrap('~point_label')
)
3/23:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/24:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/25:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_wrap


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

print(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_wrap('~point_label')
)
3/26:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    #+ scale_x_timedelta(breaks=pd.date_range(start, end, freq='M'))
    + facet_grid(facets = '. ~point_label')
)
3/27:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta(breaks=pd.date_range(min(data_grouped['period_from']), max(data_grouped['period_from']), freq='M'))
    + facet_grid(facets = '. ~point_label')
)
3/28:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + aes(x='period_from', y='value')
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line()
    + scale_x_timedelta()
    + facet_grid(facets = '. ~point_label')
)
3/29:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)

    + geom_line()
)
3/30:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)

    + geom_line(x = 'period_from', y = 'value', color = 'blue')
)
3/31:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)

    + geom_line(aes(x = 'period_from', y = 'value', color = 'blue'))
)
3/32:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)

    + geom_line(aes(x = 'period_from', y = 'value'))
)
3/33:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + labs(x='Date', y='Flow', title='Gazprom Flow')
    + geom_line(aes(x='period_from', y='value'))
    + scale_x_timedelta()
    + facet_grid(facets = '. ~point_label')
)
3/34:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value'))
    + scale_x_timedelta()
    + facet_grid(facets = '. ~point_label')
)
3/35:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')


data[mask]['value'] =  - data[mask]['value']


# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value'))
    + facet_grid(facets = '. ~point_label')
)
3/36:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = data['period_from'].dt.strftime('%Y-%m-%d')

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value'))
    + facet_grid(facets = '. ~point_label')
)
3/37:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value'))
    + facet_grid(facets = '. ~point_label')
)
3/38:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)
3/39:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label'))
)
3/40:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid(data_grouped['point_type'], scales='free')
)
3/41:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('point_type', scales='free')
)
3/42:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('point_type', scales='free')
)
3/43:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/44:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], join = 'inner')

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/45:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], join = 'inner').reset_index()

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/46:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.concat([data, masked_points], keys = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], join = 'inner', axis = 1)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/47:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/48:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)
    + geom_line(aes(x='period_from', y='value', color = 'point_label')) 
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/49:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, geom_point, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + facet_grid('.~point_type', scales='free')
)
3/50:
from plotnine import ggplot, aes, scale_x_timedelta, labs, geom_line, geom_point, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_timedelta(freq = 'M')
    + facet_grid('.~point_type', scales='free')
)
3/51:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
)
3/52:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, axis.test.x


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/53:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/54:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
3/55:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
3/56:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text


print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/57:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))


data = data.replace({'': None})
data = data.replace({'-': None})
data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb



# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/58:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})
data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/59:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})
data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  - data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/60:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})
data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/61:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})
data.replace({np.nan: None},inplace = True) # https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value'].astype(float)

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/62:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/63:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/64:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], , format='%Y-%m-%dT%H:%M:%S:00').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/65:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S:00').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/66:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%Z').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/67:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/68:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/69:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

print(data['period_from'].drop_duplicates())
data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/70:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

print(data['period_from'].drop_duplicates())


data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/71:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

print(data['period_from'].drop_duplicates())


data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S %z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))





# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/72:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S %z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)




# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/73:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)




# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
3/74:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
3/75:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
3/76:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 4/1:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 4/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 5/1:
from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 5/2:
from .entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 5/3:
from ..entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 5/4:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 5/5:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 5/6:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], format='%Y-%m-%dT%H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
 5/7:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
 5/8:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
 5/9:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from']).dt.strftime('%Y-%m-%d')

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
5/10:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'])

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
5/11:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'])

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
5/12:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + geom_point()
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
5/13:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(axis_text_x = element_text(angle=45, hjust =1))
)
5/14:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/15:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/16:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/17:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/18:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/19:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/20:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
    + entsog_scale_color_brewer()
)
5/21:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
)
5/22:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
)
5/23:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme, entsog_scale_color_brewer

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
)
5/24:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme()
)
5/25:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
5/26:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
5/27:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
 6/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 6/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 6/3:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
 7/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 7/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 7/3:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
 7/4:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
)
 7/5:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', captions = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
)
 7/6:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data)

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

print(data_grouped)
print(data_grouped.columns)

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
)
 7/7:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
)
 7/8:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
 7/9:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
7/10:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
7/11:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + entsog_theme
)
7/12:
from plotnine import *
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(
    axis_title = element_text(face='bold'),
    axis_text_y = element_text(angle = 90, vjust = 2),
    axis_text_x = element_text(angle = 45, hjust = 1),

    axis_text = element_text(),
    axis_line = element_line(colour = 'black'),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8),
    axis_text_y=element_text(colour="black", size=10),

) 
)
7/13:
import * from plotnine
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(
    axis_title = element_text(face='bold'),
    axis_text_y = element_text(angle = 90, vjust = 2),
    axis_text_x = element_text(angle = 45, hjust = 1),

    axis_text = element_text(),
    axis_line = element_line(colour = 'black'),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8),
    axis_text_y=element_text(colour="black", size=10),

) 
)
7/14:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(
    axis_title = element_text(face='bold'),
    axis_text_y = element_text(angle = 90, vjust = 2),
    axis_text_x = element_text(angle = 45, hjust = 1),

    axis_text = element_text(),
    axis_line = element_line(colour = 'black'),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8),
    axis_text_y=element_text(colour="black", size=10),

) 
)
7/15:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(
    axis_title = element_text(face='bold'),
    axis_text_y = element_text(angle = 90, vjust = 2),
    axis_text_x = element_text(angle = 45, hjust = 1),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8),
    axis_text_y=element_text(colour="black", size=10)

) 
)
7/16:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 90, vjust = 2),
    axis_text_y=element_text(colour="black", size=10, angle = 45, hjust = 1)

) 
)
7/17:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('.~point_type', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
)
7/18:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='Period', y='Flow', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
)
7/19:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
)
7/20:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) + scale_fill_brewer(type="qual", palette="Accent")
)
7/21:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_fill_brewer(type="qual", palette="Accent")
)
7/22:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_fill_brewer(type="qual", palette="Pastel1")
)
7/23:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent")
)
7/24:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Pastel1")
)
7/25:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(palette="Accent")
)
7/26:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(palette="Pastel1")
)
7/27:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('point_type~.', scales='free_y')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Type')
)
7/28:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'point_type']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Type')
)
7/29:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np

from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Type')
)
7/30:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer
import numpy as np


from entsog.mappings import lookup_country
from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.value

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y')
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
7/31:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country
from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.value

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
7/32:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country
from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.value

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
7/33:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country
from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    print(country.value)
    return country.value

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
7/34:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country
from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 8/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 8/2:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 8/3:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
 8/4:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country
from entsog.utils import entsog_theme

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 8/5:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','t_so_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 8/6:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','tso_country','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 'tso_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 8/7:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np


from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 8/8:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
 8/9:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
8/10:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
8/11:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Twh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/12:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Gwh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=10, angle = 90, vjust = 2)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/13:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Physical Flow (Gwh)', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_title = element_text(face='bold'),

    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/14:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/15:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller
from plotnine import scale_y_continous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels = 'comma')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/16:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels = 'comma')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/17:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(suffix = 'comma')
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/18:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)/ 1_000_000

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/19:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/20:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_wrap, theme, element_text, element_line, element_blank, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_wrap('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8)

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/21:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(face="bold")

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/22:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='Gwh', title = 'Gazprom flows', subtitle = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black')

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/23:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),
    caption=element_text(
        size=8,
        margin={'r': -120, 't': -30}
    )

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/24:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous
from plotnine import caption, theme, element_text, element_rect, element_line, element_blank, element_text

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),
    caption=element_text(
        size=8,
        margin={'r': -120, 't': -30}
    )

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/25:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),
    caption=element_text(
        size=8,
        margin={'r': -120, 't': -30}
    )

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/26:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/27:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py') +
    labs('{:>50}'.format("source: JHU..."))
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
8/28:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + labs('{:>50}'.format("source: JHU..."))
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
 9/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
 9/3:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + labs('{:>50}'.format("source: JHU..."))
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/4:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~.', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/5:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'point_type']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~point_type', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/6:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'connected_operators']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~connected_operators', scales='free_y', labeller = labeller(rows = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/7:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/8:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit' & data['flow_status'] == 'Confirmed')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
 9/9:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit' and data['flow_status'] == 'Confirmed')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
9/10:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit' and data['flow_status'] == 'Confirmed')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
9/11:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') and (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
9/12:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit' and data['flow_status'] == 'Confirmed')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
9/13:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = (data['direction_key'] == 'exit' & data['flow_status'] == 'Confirmed')
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
9/14:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
9/15:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
11/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
11/3:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/4:
from plotnine import ggplot, aes, scale_x_date, labs, geom_line, geom_point, facet_grid, theme, element_text, element_line, element_blank, element_rect, scale_fill_brewer, scale_color_brewer, labeller, scale_y_continuous

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    ggplot(data_grouped)+
    aes(x='period_from', y='value', color = 'point_label')
    + geom_line() 
    + labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + scale_x_date(date_break = '1 month')
    + scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + theme(
    axis_text = element_text(),
    axis_ticks = element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=element_blank(),
    axis_line=element_line(size=1, colour="black"),
    panel_grid_major=element_line(colour="#d3d3d3"),
    panel_grid_minor=element_blank(),
    panel_border=element_blank(),
    panel_background=element_blank(),
    plot_title=element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=element_text(family="Tahoma", size=11),
    axis_text_x=element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=element_text(colour="black", size=8),

    strip_background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = element_text(size = 8, colour = 'black'),
    caption=element_text(
        size=8,
        margin={'r': -120, 't': -30}
    )

) 
+ scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/5:
import plotnine as p9

import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    caption=p9.element_text(
        size=8,
        margin={'r': -120, 't': -30}
    )

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/6:
import plotnine as p9
from  plotnine import caption
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    caption=p9.element_text(
        size=8,
        margin={'r': -120, 't': -30}
    )

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/7:
import plotnine as p9
from  plotnine import caption
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/8:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

print(data.head(3))

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()


def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/9:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/10:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    caption = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/11:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    caption_text = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/12:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    caption_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/13:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/14:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -30}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/15:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption_position = "bottom",
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -30}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/16:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -30}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/17:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': -50}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/18:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 0}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/19:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': 0, 't': 0}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/20:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': 50, 't': 0}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/21:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 50}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/22:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog. Generated on {pd.Timestamp.now()}. Provided by entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 50}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/23:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog.\nCreated using entsog-py.\nData from {start} to {end}')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/24:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nCreated using entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}, face = 'italian', family = 'Tahoma')'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/25:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nCreated using entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/26:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 30}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/27:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 20}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/28:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -100, 't': 20}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/29:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -60, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/30:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -120, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/31:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/32:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data[data['adjacent_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/33:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/34:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_key=p9.element_blank(family = 'Tahoma'),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/35:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_text = p9.element_text(family = 'Tahoma'),
    legend_title_text = p9.element_text(family = 'Tahoma', face = 'bold'),
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/36:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_text = p9.element_text(family = 'Tahoma'),
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/37:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(masked_points.column)
print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_text = p9.element_text(family = 'Tahoma'),
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
11/38:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(masked_points.columns)
print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_text = p9.element_text(family = 'Tahoma'),
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
12/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
12/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
12/3:
import plotnine as p9
import numpy as np

data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

print(masked_points.columns)
print(data[data['t_so_country'] == 'LT'])

# Group by point and period_from
data_grouped = data.groupby(['period_from','point_label', 't_so_country', 'adjacent_country']).agg({'value':'sum'}).reset_index()

def col_func(s):
    country = lookup_country(s)
    return country.label

(
    p9.ggplot(data_grouped)
    + p9.aes(x='period_from', y='value', color = 'point_label')
    + p9.geom_line() 
    + p9.labs(x='', y='', title = 'Gazprom flows', caption = f'Source: Entsog\nLibrary: entsog-py')
    + p9.scale_x_date(date_break = '1 month')
    + p9.scale_y_continuous(labels= lambda l: [f"{(v / 1_000_000)} GWh" for v in l])
    + p9.facet_grid('t_so_country~adjacent_country', scales='free_y', labeller = labeller(rows = col_func, cols = col_func))
    + p9.theme(
    axis_text = p9.element_text(),
    axis_ticks = p9.element_line(),

    legend_position="bottom",
    legend_direction="horizontal",
    legend_title_align="center",
    legend_box_spacing=0.4,
    legend_text = p9.element_text(family = 'Tahoma'),
    legend_key=p9.element_blank(),
    axis_line=p9.element_line(size=1, colour="black"),
    panel_grid_major=p9.element_line(colour="#d3d3d3"),
    panel_grid_minor=p9.element_blank(),
    panel_border=p9.element_blank(),
    panel_background=p9.element_blank(),
    plot_title=p9.element_text(size=15, family="Tahoma", 
                            face="bold"),
    text=p9.element_text(family="Tahoma", size=11),
    axis_text_x=p9.element_text(colour="black", size=8, angle = 45, hjust = 1),
    axis_text_y=p9.element_text(colour="black", size=8),

    strip_background=p9.element_rect(colour="#f0f0f0",fill="#f0f0f0"),
    strip_text = p9.element_text(size = 8, colour = 'black'),
    plot_caption = p9.element_text(size = 8, colour = 'black', margin={'r': -50, 't': 0}, face = 'italian', family = 'Tahoma'),

) 
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
12/4:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type']])
12/5:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
12/6:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
12/7:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
13/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
13/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
13/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
data = backup

from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/7:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
13/8:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/9:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/10:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot = plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/11:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot = plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/12:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot = plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/13:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label',facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label',facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

(
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)
+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
13/14:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

flow_status = 'Confirmed'
aggregation = None
facet_row = 'country'
facet_col = None


flow_data = data.replace({'': None})

flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label',facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label',facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

(
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)
+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
13/15:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

flow_status = 'Confirmed'
aggregation = None
facet_row = 'country'
facet_col = None


flow_data = data.replace({'': None})

flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label',facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label',facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

(
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)

+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
13/16:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/17:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = None
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', 'value', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label','value', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

plot = (
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)
#+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
















data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/18:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', 'value', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label','value', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

plot = (
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)
#+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
















data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/19:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', 'value', facet_row, facet_col]).agg(
        {'aggregated': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label','value', facet_row, facet_col]).agg(
        {'aggregated': 'sum'}
    ).reset_index()

plot = (
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='aggregated', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)
#+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
















data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/20:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_break = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_break = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

plot = (
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_break = date_break)
#+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
















data = data.replace({'': None})
data = data.replace({'-': None})

data['value'] = data['value'].astype(float)

mask = ((data['direction_key'] == 'exit') & (data['flow_status'] == 'Confirmed'))
data[mask]['value'] =  -1* data[mask]['value']

data['period_from'] = pd.to_datetime(data['period_from'], utc = True)

data = pd.merge(data, masked_points, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

plot_lines(
    flow_data = data,
    point_data = masked_points
)
13/21:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_breaks = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_breaks = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

plot = (
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_breaks = date_breaks)
#+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
13/22:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


flow_data['value'] = flow_data['value'].astype(float)

# Get right flow status, provisional, confirmed etc.
flow_data = flow_data[(flow_data['flow_status'] == flow_status)]

mask = (flow_data['direction_key'] == 'exit')
flow_data[mask]['value'] =  -1* flow_data[mask]['value']

# Turn to datetime
flow_data['period_from'] = pd.to_datetime(flow_data['period_from'], utc = True)

# Join together
merged = pd.merge(flow_data, flow_data, on = ['tso_item_identifier','tso_eic_code','point_key','direction_key'], suffixes = ('','_y'))

merged.rename(
    columns = {
        't_so_country' : 'country',
    },
    inplace= True
)
# Group by point and period_from
if aggregation:
    date_breaks = '1 month'
    merged_grouped = merged.groupby([pd.Grouper(key = 'period_from',freq = aggregation, label = 'right'), 'point_label', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()
else:
    date_breaks = '1 month'
    merged_grouped = merged.groupby(['period_from', 'point_label', facet_row, facet_col]).agg(
        {'value': 'sum'}
    ).reset_index()

(
p9.ggplot(merged_grouped)
+ p9.aes(x='period_from', y='value', color = 'point_label')
+ p9.geom_line() 
+ p9.labs(x='', y='', title = '', caption = f'Source: Entsog\nLibrary: entsog-py')
+ p9.scale_x_date(date_breaks = date_breaks)
#+ p9.scale_y_continuous(labels= lambda l: [f"{(v / UNIT_TRANSFORMATION[unit])} {unit}" for v in l])
#+ p9.facet_grid(f'{facet_row}~{facet_col}', scales='free_y', labeller = p9.labeller(rows = label_func, cols = label_func))
+ p9.scale_color_brewer(type="qual", palette="Accent", name = 'Point Label')
)
13/23:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data)
13/24:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data)
13/25:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data)
13/26:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data)
13/27:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data)
13/28:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data)
13/29:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data, point_date = masked_points)
13/30:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_country'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data, point_data = masked_points)
14/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
14/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, verbose = False)

print(data.head(5))
backup = data.copy()
14/3:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)

print(data.head(5))
backup = data.copy()
15/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220401', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
15/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)

print(data.head(5))
backup = data.copy()
15/3:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)

print(data.head(5))
backup = data.copy()
15/4:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
15/5:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
15/6:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
16/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
16/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
16/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data, point_data = masked_points)
16/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data, point_data = masked_points)
16/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


plot_lines(flow_data = flow_data, point_data = masked_points)
16/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
16/7:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
17/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
17/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
17/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
17/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
18/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
18/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
18/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
19/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
19/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
19/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
20/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220301', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
20/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
20/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points))
20/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh'))
21/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
21/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
21/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh'))
21/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'm3'))
22/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
22/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
22/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
22/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
23/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
23/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
23/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
24/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
24/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
24/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
25/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
25/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
25/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
26/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
26/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
26/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
26/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh'))
26/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
27/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
27/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
27/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
28/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
28/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
28/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm'))
28/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'connected_operators'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
28/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
29/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
29/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
29/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
29/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})


print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
29/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
29/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_point())
29/7:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size =1))
29/8:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size =0.5))
29/9:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size =0.5, alpha = 0.5))
29/10:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5))
29/11:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5, fill = 'value'))
29/12:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5))
29/13:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(size =0.5, alpha = 0.5, fill = 'point_label'))
30/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
30/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
31/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
31/4:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220601', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
31/5:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
31/7:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

print(keys)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/8:
keys = []
for idx, item in masked_points.iterrows():
    print(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/9:
keys = []
for idx, item in masked_points.iterrows():
    print(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/10:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
31/11:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
print(end)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
31/12:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
print(end)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(max(data['period_from']))
print(data.head(5))
backup = data.copy()
32/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220601', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
32/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
print(end)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
32/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
33/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220601', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
33/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
print(end)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
33/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
34/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220601', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
34/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
print(end)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
34/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
34/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
35/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220601', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
35/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

keys = ['PL-TSO-0001ITP-00104entry']
print(end)
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
35/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country




facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5))
35/4:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
35/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'normal'))
35/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'identity'))
36/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
36/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
36/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'identity'))
36/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
36/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

mask = flow_data['flow_status'] = 'Confirmed'
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data[mask], point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
36/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data[mask], point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
37/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
37/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
37/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
38/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
38/2:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
38/3:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
38/4:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
38/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
39/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
39/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
39/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
40/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
40/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
40/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col) + p9.geom_area(alpha = 0.5, position = 'stack'))
42/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
42/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
42/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
43/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
43/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
43/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
44/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
44/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
44/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
45/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
45/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
45/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
45/4:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
45/5:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
45/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
46/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
46/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
46/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
46/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_fill_brewer(palette = 'Set1'))
46/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_fill_brewer(type = 'qual',palette = 'Set1'))
46/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_lines
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_lines(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_fill_brewer(type = 'qual',palette = 'Pastel1'))
47/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
47/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
47/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
47/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.scale_color_brewer(color = 'black'))
48/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
48/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
48/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
49/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
49/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
49/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
50/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
50/2:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
50/3:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
50/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
50/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col) + p9.geom_line(size = 0.1))
51/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
51/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
51/3:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
51/4:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('202204201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
51/5:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('202204201', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
51/6:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220420', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
51/7:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)

print(data.head(5))
backup = data.copy()
51/8:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220220', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
51/9:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
51/10:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
52/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220220', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
52/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
52/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'GWh', facet_row = facet_row, facet_col = facet_col))
52/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col))
52/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '3D'))
52/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
52/7:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220425', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
52/8:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)

print(data.head(5))
backup = data.copy()
52/9:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
52/10:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'kwh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
52/11:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'kWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
52/12:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
52/13:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'operator'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
52/14:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'operator_key'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
52/15:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'balancing_zone'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
53/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220425', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
53/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
53/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'balancing_zone'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
54/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220425', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
54/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
54/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'balancing_zone'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
54/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'operator_key'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
55/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220425', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
55/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
55/3:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220426', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
55/4:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
55/5:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'operator_label'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
55/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'MWh', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
55/7:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area
from entsog.mappings import lookup_country

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
aggregation = None

flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1H'))
55/8:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
55/9:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20210101', tz='Europe/Brussels')
end = pd.Timestamp('20220501', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
55/10:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
55/11:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'
flow_data = data.replace({'': None})

#mask = (flow_data['flow_status'] == 'Confirmed')
print(max(flow_data['period_from']))
print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
56/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20210420', tz='Europe/Brussels')
end = pd.Timestamp('20220510', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
56/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
56/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'

print(plot_area(flow_data = flow_data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
56/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
56/5:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220420', tz='Europe/Brussels')
end = pd.Timestamp('20220510', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
56/6:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
56/7:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
56/8:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow'], point_directions = keys, period_type = 'hour',verbose = False)

print(data.head(5))
backup = data.copy()
56/9:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
56/10:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220401', tz='Europe/Brussels')
end = pd.Timestamp('20220510', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
56/11:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination','nomination'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
56/12:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination','nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
56/13:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220401', tz='Europe/Brussels')
end = pd.Timestamp('20220510', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
56/14:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
56/15:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/1:
import sys
sys.path.append("..") # Adds higher directory to python modules path.

from entsog import EntsogPandasClient
import pandas as pd

client = EntsogPandasClient()

start = pd.Timestamp('20220401', tz='Europe/Brussels')
end = pd.Timestamp('20220510', tz='Europe/Brussels')

points = client.query_operator_point_directions()
mask = points['connected_operators'].str.contains('Gazprom')
masked_points = points[mask]
print(masked_points.head(5)[['point_key','point_label','point_type','region']])
57/2:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'renomination'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/3:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'point_label'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/4:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/5:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['physical_flow', 'nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/6:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/7:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/8:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/9:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['nomination'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/10:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = ['nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/11:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'renomination'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/12:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/13:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/14:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'nominations'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/15:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'allocation'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/16:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/17:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'firm_booked'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/18:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/19:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'firm_technical'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/20:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/21:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'country'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/22:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'adjacent_region'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/23:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'gcv','physical_flow','firm_technical'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/24:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
57/25:
keys = []
for idx, item in masked_points.iterrows():
    keys.append(f"{item['operator_key']}{item['point_key']}{item['direction_key']}")

#keys = ['PL-TSO-0001ITP-00104entry']
data = client.query_operational_point_data(start = start, end = end, indicators = [ 'gcv','physical_flow','allocation'], point_directions = keys, period_type = 'day',verbose = False)

print(data.head(5))
backup = data.copy()
57/26:
import plotnine as p9
import numpy as np

from entsog.plot_utils import plot_area

facet_row = 'country'
facet_col = 'indicator'
flow_status = 'Confirmed'

print(plot_area(flow_data = data, point_data = masked_points, unit = 'mcm', facet_row = facet_row, facet_col = facet_col, aggregation = '1D'))
59/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
59/2: print(yc)
59/3:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

yc['3 M'].plot(figsize=(15,5), lw=1, color="blue", label='3-Month Government Bond')
yc['6 M'].plot( lw=0.5, color="lightblue",label='6-Month Government Bond')
yc['9 M'].plot( lw=0.5, color="darkgreen",label='9-Month Government Bond')
yc['1 Y'].plot( lw=0.5, color="deepskyblue",label='1-Year Government Bond')
yc['2 Y'].plot( lw=0.5, color="dodgerblue",label='2-Year Government Bond')
yc['3 Y'].plot( lw=0.5, color="steelblue",label='3-Year Government Bond')
yc['4 Y'].plot( lw=0.5, color="blue",label='4-Year Government Bond')
yc['5 Y'].plot( lw=0.5, color="mediumblue",label='5-Year Government Bond')
yc['6 Y'].plot( lw=0.5, color="slategrey", label='6-Year Government Bond')
yc['7 Y'].plot( lw=0.5, color="gray",label='7-Year Government Bond')
yc['8 Y'].plot( lw=0.5, color="red", grid=True,label='8-Year Government Bond')
yc['9 Y'].plot( lw=0.5, color="orange", grid=True,label='9-Year Government Bond')
yc['10 Y'].plot( lw=0.5, color="green", grid=True,label='10-Year Government Bond')
yc['15 Y'].plot( lw=0.5, color="cyan", grid=True,label='15-Year Government Bond')
yc['20 Y'].plot( lw=0.5, color="yellow", grid=True,label='20-Year Government Bond')
yc['25 Y'].plot( lw=0.5, color="pink", grid=True,label='25-Year Government Bond')
yc['30 Y'].plot( lw=0.5, color="black", grid=True,label='30-Year Government Bond')
plt.legend(bbox_to_anchor=(1, 1.02))
plt.xlabel("Date")
plt.ylabel("Canadian Government Bond Rate")
plt.title("The Canadian Term Structure of Interest Rate", fontweight='bold')
59/4:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
59/5:
# Maturity
header = []
for name in ycn.dtype.names[1:]:
    maturity = float(name.split(" ")[0])
    if name.split(" ")[1] == 'M':
        maturity = maturity / 12
    header.append(maturity)
59/6:
# We create three empty lists 
x_data = []; y_data = []; z_data = []
for dt in ycn.Date:
    dt_num = dates.date2num(dt)
    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])
# print ('x_data: ', x_data[1:5])
59/7:
for row in ycn:
    y_data.append(header)
    z_data.append(list(row.tolist()[1:]))
# print ('y_data: ', y_data[1:5])
# print ('z_data: ', z_data[1:5])
59/8:
x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')
# x ==> Dates
# y ==> Maturities
# z ==> Yields
# print ('x:', x) 
# print ('y: ', y)
# print ('z: ', z)
59/9:
%matplotlib inline
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')
z_percent = z*100
ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))
ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')
ax.set_ylabel('Maturity (\u03C4)')
ax.set_zlabel('Yield (Percent)')
plt.savefig('my_pgf_plot.jpeg')

def format_date(x, pos=None):
     return dates.num2date(x).strftime('%Y')
ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))
for tl in ax.w_xaxis.get_ticklabels():
    tl.set_ha('right')
    tl.set_rotation(40)
plt.show()
59/10:
# Stylized fact 1: The average yield curve over time is increasing and concave
average_yc = yc.mean(axis=0) * 100
average_yc.plot()
59/11:
# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, 
# humped, inverted humped, S-shapes)
curve_1 = yc.iloc[1]*100
curve_2 = yc.iloc[4174]*100 #2000
curve_3 = yc.iloc[5600]*100
curve_4 = yc.iloc[7850]*100
curve_1.plot()
curve_2.plot()
curve_3.plot()
curve_4.plot()
59/12:
# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)
# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities
first_autocorrelation = []
tenth_autocorrelation = []
twentieth_autocorrelation = []
for maturity in yc:
    first_autocorrelation.append(yc[maturity].autocorr())
    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))
    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))

# autocorrelations = 
# s = yc['3 M']
# s.autocorr()
data = []
for maturity in yc:
    data.append(maturity)

autocorrelations = pd.DataFrame(data, columns=['Maturity'])
autocorrelations['First autocorrelation'] = first_autocorrelation 
autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation
autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation
autocorrelations.round(5)
59/13:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
betas = np.zeros((rows, 3))
levels = []
slopes = []
curvatures = []
for maturity in header:
    levels.append(1)
    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))
    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))
59/14:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import math
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
59/15:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
betas = np.zeros((rows, 3))
levels = []
slopes = []
curvatures = []
for maturity in header:
    levels.append(1)
    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))
    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))
59/16:
lambda_max = lmbd_range[np.argmax(lmbd_values)]
print(lambda_max)
59/17:
def find_lambda(lmbd):
    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)
    return  lmbd_val
    
lmbd_range = np.arange(-0.1,2,0.0001)
lmbd_values = []
for i in lmbd_range:
    lmbd_values.append(find_lambda(i))
plt.plot(lmbd_range,lmbd_values)
59/18:
lambda_max = lmbd_range[np.argmax(lmbd_values)]
print(lambda_max)
59/19:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
betas = np.zeros((rows, 3))
levels = []
slopes = []
curvatures = []
for maturity in header:
    levels.append(1)
    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))
    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))
59/20: one_row = levels + slopes + curvatures
59/21:
(rows, cols)= np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    B = np.reshape(one_row, (np.size(levels), 3))
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))
    
print(beta)
59/22:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
betas = np.zeros((rows, 3))
levels = []
slopes = []
curvatures = []
for maturity in header:
    levels.append(1)
    slopes.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity))
    curvatures.append((1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity))

print(curvatures)
59/23:
# Create the forecast functions

# Nelson-siegel function
def nelson_siegel(t, beta1, beta2, beta3):
    return beta1 + beta2*(1 - math.exp(-lambda_max*t))/(lambda_max*t) + beta3*((1 - math.exp(-lambda_max*t))/(lambda_max*t) - math.exp(-lambda_max*t))

# Nelson-siegel-svensson function
def nelson_siegel_svensson(t, beta1, beta2, beta3, beta4, beta5):
    return beta1 + beta2*(1 - math.exp(-lambda_max*t))/(lambda_max*t) + beta3*((1 - math.exp(-lambda_max*t))/(lambda_max*t) - math.exp(-lambda_max*t)) + beta4*(1 - math.exp(-lambda_max*t))/(lambda_max*t) + beta5*((1 - math.exp(-lambda_max*t))/(lambda_max*t) - math.exp(-lambda_max*t))

# Build a neural network with algorithm
59/24:
# Create the forecast functions

# Convert the following r function to python
# Nelson siegel forecasts
# ns_ar_forecast <- function(beta,step_size){
#   shift_beta <- lag(beta,step_size)
#   temp_beta <- beta[-c(1:step_size),]
#   shift_beta <- shift_beta[-c(1:step_size),]
  
#   regr <- vector(mode = 'list', length = 3)
  
#   forecasted_beta <- beta # To be adjusted in following loop
#   for(f in 1:3){
#     subset <- cbind(temp_beta[,f], shift_beta[,f] %>% as.data.frame())
#     colnames(subset) <- c("y","x")
    
#     regr[[f]] <- lm(formula = 'y ~ x', data = subset)
#     names(regr)[f] <- colnames(temp_beta)[f]
    
#     const <- coef(regr[[f]])[1]
#     gamma <- coef(regr[[f]])[2]
    
#     forecasted_beta[,f] <- const + gamma * beta[,f]
    
#   }
  
  
  
#   forecasted_y <- forecasted_beta[,1] + forecasted_beta[,2] %*% slope_vector +  forecasted_beta[,3] %*% curvature_vector
  
#   return(tail(forecasted_y,1))
# }
59/25:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
print(B)
60/1:
(rows, cols) = np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))

print(beta)
59/26:
(rows, cols) = np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))

print(beta)
59/27: np.shape(beta)
59/28:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = beta # To be adjusted in following loop
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[:,f] = const + gamma * beta[:,f]
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]

    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/29:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = beta # To be adjusted in following loop
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[:,f] = const + gamma * beta[:,f]
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/30:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
59/31:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = beta # To be adjusted in following loop
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[:,f] = const + gamma * beta[:,f]
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/32:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = beta # To be adjusted in following loop
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[:,f] = const + gamma * beta[:,f]
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/33:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
59/34:
(rows, cols) = np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))

print(beta)
59/35:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = beta # To be adjusted in following loop
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[:,f] = const + gamma * beta[:,f]
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/36:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = beta # To be adjusted in following loop
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[:,f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/37:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/38:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        print(const + gamma * beta[:,f])
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/39:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr.append(sm.OLS(subset[:,0], subset[:,1]).fit())
        const = regr[f].params[0]
        gamma = regr[f].params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/40:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr = sm.OLS(subset[:,0], subset[:,1]).fit()
        const = regr.params[0]
        gamma = regr.params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/41:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        regr = sm.OLS(subset[:,0], subset[:,1]).fit()
        print(regr.summary())
        const = regr.params[0]
        gamma = regr.params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/42:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        # fit regr with constant
        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()
        print(regr.summary())
        const = regr.params[0]
        gamma = regr.params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(forecasted_beta[:,1])
    print(slopes)
    forecasted_y = forecasted_beta[:,0] + forecasted_beta[:,1] * slopes +  forecasted_beta[:,2] * curvatures
    return forecasted_y[-1]

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/43:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        # fit regr with constant
        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()
        print(regr.summary())
        const = regr.params[0]
        gamma = regr.params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures
    print(forecasted_y)
    return forecasted_y

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/44:
# Create the forecast functions
import statsmodels.api as sm

def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        # fit regr with constant
        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()
        print(regr.summary())
        const = regr.params[0]
        gamma = regr.params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(np.shape(slopes))
    print(np.shape(curvatures))
    forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures
    print(forecasted_y)
    return forecasted_y

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/45:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast
def ns_ar_forecast(beta, maturity, t):




def ns_ar_forecast(beta, step_size):
    shift_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    shift_beta = shift_beta[step_size:,:]
    regr = []
    forecasted_beta = [None, None, None]
    for f in range(3):
        print(f)
        subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
        # fit regr with constant
        regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()
        print(regr.summary())
        const = regr.params[0]
        gamma = regr.params[1]
        forecasted_beta[f] = const + gamma * beta[:,f]
        
    print(np.shape(slopes))
    print(np.shape(curvatures))
    forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures
    print(forecasted_y)
    return forecasted_y

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/46:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]

    # Create the X matrix (iota, lagged_beta)
    x = np.concatenate((np.ones((np.shape(temp_beta)[0],1)), lagged_beta), axis=1)
    # Create the Y matrix (temp_beta)
    y = temp_beta
    
    regression = sm.OLS(y, x).fit()
    print(regression.summary())


# def ns_ar_forecast(beta, step_size):
#     shift_beta = np.roll(beta, step_size, axis=0)
#     temp_beta = beta[step_size:,:]
#     shift_beta = shift_beta[step_size:,:]
#     regr = []
#     forecasted_beta = [None, None, None]
#     for f in range(3):
#         print(f)
#         subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
#         # fit regr with constant
#         regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()
#         print(regr.summary())
#         const = regr.params[0]
#         gamma = regr.params[1]
#         forecasted_beta[f] = const + gamma * beta[:,f]
        
#     print(np.shape(slopes))
#     print(np.shape(curvatures))
#     forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures
#     print(forecasted_y)
#     return forecasted_y

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/47:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]

    # Create the X matrix (iota, lagged_beta)
    x = np.concatenate((np.ones((np.shape(temp_beta)[0],1)), lagged_beta), axis=1)
    # Create the Y matrix (temp_beta)
    y = temp_beta
    
    regression = sm.OLS(y, x).fit()
    print(regression.summary())


# def ns_ar_forecast(beta, step_size):
#     shift_beta = np.roll(beta, step_size, axis=0)
#     temp_beta = beta[step_size:,:]
#     shift_beta = shift_beta[step_size:,:]
#     regr = []
#     forecasted_beta = [None, None, None]
#     for f in range(3):
#         print(f)
#         subset = np.concatenate((temp_beta[:,f].reshape(-1,1), shift_beta[:,f].reshape(-1,1)), axis=1)
#         # fit regr with constant
#         regr = sm.OLS(subset[:,0], sm.add_constant(subset[:,1])).fit()
#         print(regr.summary())
#         const = regr.params[0]
#         gamma = regr.params[1]
#         forecasted_beta[f] = const + gamma * beta[:,f]
        
#     print(np.shape(slopes))
#     print(np.shape(curvatures))
#     forecasted_y = forecasted_beta[0] + forecasted_beta[1] * slopes +  forecasted_beta[2] * curvatures
#     print(forecasted_y)
#     return forecasted_y

# Reset
yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/48:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size, slope, curvature):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = np.concatenate((np.ones((np.shape(temp_beta)[0],1)), lagged_beta[:,f]), axis=1)
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, x).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slope + fc_beta[2] * curvature
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    slope = slopes[i]
    curvature = curvatures[i]
    
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE, slope , curvature)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/49:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size, slope, curvature):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slope + fc_beta[2] * curvature
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    slope = slopes[i]
    curvature = curvatures[i]
    
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE, slope , curvature)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/50:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
print(np.shape(B))
59/51:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size, slope, curvature):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    slope = slopes[i]
    curvature = curvatures[i]
    
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/52:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    slope = slopes[i]
    curvature = curvatures[i]
    
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/53:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(subset_beta)
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/54:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/55:
# Do analysis on the forecasts

print(f"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}")
59/56:
# Do analysis on the forecasts

print(f"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}")

print(forecast_yields['random_walk'])
59/57:
# Do analysis on the forecasts

#print(f"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}")

print(forecast_yields['nelson_siegel'])
59/58:
# Do analysis on the forecasts

#print(f"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}")

#print(forecast_yields['nelson_siegel'])

# Convert to dataframe

forecast_yields_df = pd.DataFrame(forecast_yields)
print(forecast_yields_df)
59/59:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    #'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    #forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/60:
# Do analysis on the forecasts

#print(f"Random walk RMSE: {np.sqrt(np.mean((np.array(forecast_yields['random_walk']) - np.array(forecast_yields['realized']))**2))}")

#print(forecast_yields['nelson_siegel'])

# Convert to dataframe

forecast_yields_df = pd.DataFrame(forecast_yields)
print(forecast_yields_df)
59/61:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    #'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = subset.iloc[-1]
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    print(nelson_siegel)
    break
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    #forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/62:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    #'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    print(nelson_siegel)
    print(random_walk)
    break
    # Realized
    realized = yields.iloc[i+WINDOW_SIZE+STEP_SIZE]

    # Append to dictionary
    #forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/63:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    #'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    print(nelson_siegel)
    print(random_walk)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE+STEP_SIZE])
    print(realized)
    break
    # Append to dictionary
    #forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/64:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    #'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    print(nelson_siegel)
    print(random_walk)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE+STEP_SIZE])
    print(realized)
    print(yields.index)
    break
    # Append to dictionary
    #forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
59/65:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE+STEP_SIZE])
    index = yields.index[i+WINDOW_SIZE+STEP_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/66:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    tmp = {
        'index' : forecast_yields['index'],
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/67:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    tmp = {
        'index' : forecast_yields['index'],
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/68:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/69:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.dim(np.array(forecast_yields['index'])))
    print(np.dim(np.array(forecast_yields['random_walk'][idx])))
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/70:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(np.array(forecast_yields['random_walk'][idx])))
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/71:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(np.array(forecast_yields['random_walk'][idx,:])))
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/72:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/73:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk'][:,idx]))
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/74:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/75:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    print(forecast_yields['random_walk'])
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/76:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    print(forecast_yields['random_walk'])
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    print(random_walk)
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/77:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    print(forecast_yields['random_walk'])
    # Select the idx element from every forecast
    print('SELECTINg')
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    print(random_walk)
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/78:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    print(forecast_yields['random_walk'])
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    print(random_walk)
    break
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : forecast_yields['random_walk'][idx],
        'nelson_siegel' : forecast_yields['nelson_siegel'][idx],
        'realized' : forecast_yields['realized'][idx]
    }
    print(tmp)
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/79:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    print(np.shape(np.array(forecast_yields['index'])))
    print(np.shape(forecast_yields['random_walk']))
    print(forecast_yields['random_walk'])
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/80:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
59/81:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

print(maturity_forecasts['1 M'].head())
59/82:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

print(maturity_forecasts)

print(maturity_forecasts['1 M'].head())
59/83:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts)
59/84:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])
59/85:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 1
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/86:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])
59/87:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
59/88:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
plot_forecasts(maturity_forecasts, '30 Y')
59/89:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    maturity_forecasts[maturity] = pd.DataFrame(tmp)

# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
plot_forecasts(maturity_forecasts, '1 Y')
59/90:
# Do analysis on the forecasts

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')
59/91:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)
59/92:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="value", hue="variable")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)
    plt.show()

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)
59/93:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)
59/94:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/95:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, STEP_SIZE):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/96:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)
59/97:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
59/98:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/99:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
59/100:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/101:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)
59/102:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yields, step_size):
    lagged = np.roll(yields, step_size, axis=0)
    temp = yields[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yields[-1,idx]
        
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/103:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/104:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(subset)
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/105:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to matrix (drop the date column)
yields = yields.drop('Date', axis=1)
yields = yields.as_matrix()
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/106:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to matrix (drop the date column)
print(yields)
yields = yields.drop('Date', axis=1)
yields = yields.as_matrix()
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/107:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to matrix (drop the date column)
print(yields)
yields = yields.as_matrix()
STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/108:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields.iloc[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/109:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset.iloc[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields.iloc[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/110:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[maturity]
        x = lagged[maturity]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/111:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/112:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx,] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/113:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty array to store forecasted yield levels
    fc_y = np.zeros((1, len(header)))
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        print(fc_y)
        fc_y[idx,] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/114:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yields = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yields[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yields[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/115:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*2

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
59/116:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)
61/1:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/2:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
61/4:
def find_lambda(lmbd):
    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)
    return  lmbd_val
    
lmbd_range = np.arange(-0.1,2,0.0001)
lmbd_values = []
for i in lmbd_range:
    lmbd_values.append(find_lambda(i))
plt.plot(lmbd_range,lmbd_values)
61/5:
import math
def find_lambda(lmbd):
    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)
    return  lmbd_val
    
lmbd_range = np.arange(-0.1,2,0.0001)
lmbd_values = []
for i in lmbd_range:
    lmbd_values.append(find_lambda(i))
plt.plot(lmbd_range,lmbd_values)
61/6:
lambda_max = lmbd_range[np.argmax(lmbd_values)]
print(lambda_max)
61/7:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
print(np.shape(B))
61/8: header
61/9:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
61/10: print(yc)
61/11:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

yc['3 M'].plot(figsize=(15,5), lw=1, color="blue", label='3-Month Government Bond')
yc['6 M'].plot( lw=0.5, color="lightblue",label='6-Month Government Bond')
yc['9 M'].plot( lw=0.5, color="darkgreen",label='9-Month Government Bond')
yc['1 Y'].plot( lw=0.5, color="deepskyblue",label='1-Year Government Bond')
yc['2 Y'].plot( lw=0.5, color="dodgerblue",label='2-Year Government Bond')
yc['3 Y'].plot( lw=0.5, color="steelblue",label='3-Year Government Bond')
yc['4 Y'].plot( lw=0.5, color="blue",label='4-Year Government Bond')
yc['5 Y'].plot( lw=0.5, color="mediumblue",label='5-Year Government Bond')
yc['6 Y'].plot( lw=0.5, color="slategrey", label='6-Year Government Bond')
yc['7 Y'].plot( lw=0.5, color="gray",label='7-Year Government Bond')
yc['8 Y'].plot( lw=0.5, color="red", grid=True,label='8-Year Government Bond')
yc['9 Y'].plot( lw=0.5, color="orange", grid=True,label='9-Year Government Bond')
yc['10 Y'].plot( lw=0.5, color="green", grid=True,label='10-Year Government Bond')
yc['15 Y'].plot( lw=0.5, color="cyan", grid=True,label='15-Year Government Bond')
yc['20 Y'].plot( lw=0.5, color="yellow", grid=True,label='20-Year Government Bond')
yc['25 Y'].plot( lw=0.5, color="pink", grid=True,label='25-Year Government Bond')
yc['30 Y'].plot( lw=0.5, color="black", grid=True,label='30-Year Government Bond')
plt.legend(bbox_to_anchor=(1, 1.02))
plt.xlabel("Date")
plt.ylabel("Canadian Government Bond Rate")
plt.title("The Canadian Term Structure of Interest Rate", fontweight='bold')
61/12:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import math
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
61/13:
# Maturity
header = []
for name in ycn.dtype.names[1:]:
    maturity = float(name.split(" ")[0])
    if name.split(" ")[1] == 'M':
        maturity = maturity / 12
    header.append(maturity)
61/14:
# We create three empty lists 
x_data = []; y_data = []; z_data = []
for dt in ycn.Date:
    dt_num = dates.date2num(dt)
    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])
# print ('x_data: ', x_data[1:5])
61/15:
for row in ycn:
    y_data.append(header)
    z_data.append(list(row.tolist()[1:]))
# print ('y_data: ', y_data[1:5])
# print ('z_data: ', z_data[1:5])
61/16:
x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')
# x ==> Dates
# y ==> Maturities
# z ==> Yields
# print ('x:', x) 
# print ('y: ', y)
# print ('z: ', z)
61/17:
%matplotlib inline
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')
z_percent = z*100
ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))
ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')
ax.set_ylabel('Maturity (\u03C4)')
ax.set_zlabel('Yield (Percent)')
plt.savefig('my_pgf_plot.jpeg')

def format_date(x, pos=None):
     return dates.num2date(x).strftime('%Y')
ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))
for tl in ax.w_xaxis.get_ticklabels():
    tl.set_ha('right')
    tl.set_rotation(40)
plt.show()
61/18:
# Stylized fact 1: The average yield curve over time is increasing and concave
average_yc = yc.mean(axis=0) * 100
average_yc.plot()
61/19:
# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, 
# humped, inverted humped, S-shapes)
curve_1 = yc.iloc[1]*100
curve_2 = yc.iloc[4174]*100 #2000
curve_3 = yc.iloc[5600]*100
curve_4 = yc.iloc[7850]*100
curve_1.plot()
curve_2.plot()
curve_3.plot()
curve_4.plot()
61/20: curve_1.plot()
61/21: curve_2.plot()
61/22: curve_3.plot()
61/23: curve_4.plot()
61/24:
# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)
# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities
first_autocorrelation = []
tenth_autocorrelation = []
twentieth_autocorrelation = []
for maturity in yc:
    first_autocorrelation.append(yc[maturity].autocorr())
    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))
    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))

# autocorrelations = 
# s = yc['3 M']
# s.autocorr()
data = []
for maturity in yc:
    data.append(maturity)

autocorrelations = pd.DataFrame(data, columns=['Maturity'])
autocorrelations['First autocorrelation'] = first_autocorrelation 
autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation
autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation
autocorrelations.round(5)
61/25:
# Stylized fact 5: The short end of the yield curve is more volatile than the long end of the curve
stdevs = yc.std()
stdevs.round(5)
61/26:
# Stylized fact 6: Yields for different maturities have high cross-correlations
yc.corr().round(3)
61/27: yc
61/28: header
61/29:
import math
def find_lambda(lmbd):
    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)
    return  lmbd_val
    
lmbd_range = np.arange(-0.1,2,0.0001)
lmbd_values = []
for i in lmbd_range:
    lmbd_values.append(find_lambda(i))
plt.plot(lmbd_range,lmbd_values)
61/30:
lambda_max = lmbd_range[np.argmax(lmbd_values)]
print(lambda_max)
61/31:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
print(np.shape(B))
61/32:
(rows, cols) = np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))

print(beta)
61/33: np.shape(beta)
61/34:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/35:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/36:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/37:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/38:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/39:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')


# Group by month and take the average
yc = yc.groupby(yc.index.month).mean()
print(yc)
61/40:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')


# Group by month (timestamp) and take the average
yc = yc.groupby(pd.Grouper(freq='M')).mean()
print(yc)
61/41:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
# Group by month (timestamp) and take the average
yc = yc.groupby(pd.Grouper(freq='M')).mean()
61/42: print(yc)
61/43:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

yc['3 M'].plot(figsize=(15,5), lw=1, color="blue", label='3-Month Government Bond')
yc['6 M'].plot( lw=0.5, color="lightblue",label='6-Month Government Bond')
yc['9 M'].plot( lw=0.5, color="darkgreen",label='9-Month Government Bond')
yc['1 Y'].plot( lw=0.5, color="deepskyblue",label='1-Year Government Bond')
yc['2 Y'].plot( lw=0.5, color="dodgerblue",label='2-Year Government Bond')
yc['3 Y'].plot( lw=0.5, color="steelblue",label='3-Year Government Bond')
yc['4 Y'].plot( lw=0.5, color="blue",label='4-Year Government Bond')
yc['5 Y'].plot( lw=0.5, color="mediumblue",label='5-Year Government Bond')
yc['6 Y'].plot( lw=0.5, color="slategrey", label='6-Year Government Bond')
yc['7 Y'].plot( lw=0.5, color="gray",label='7-Year Government Bond')
yc['8 Y'].plot( lw=0.5, color="red", grid=True,label='8-Year Government Bond')
yc['9 Y'].plot( lw=0.5, color="orange", grid=True,label='9-Year Government Bond')
yc['10 Y'].plot( lw=0.5, color="green", grid=True,label='10-Year Government Bond')
yc['15 Y'].plot( lw=0.5, color="cyan", grid=True,label='15-Year Government Bond')
yc['20 Y'].plot( lw=0.5, color="yellow", grid=True,label='20-Year Government Bond')
yc['25 Y'].plot( lw=0.5, color="pink", grid=True,label='25-Year Government Bond')
yc['30 Y'].plot( lw=0.5, color="black", grid=True,label='30-Year Government Bond')
plt.legend(bbox_to_anchor=(1, 1.02))
plt.xlabel("Date")
plt.ylabel("Canadian Government Bond Rate")
plt.title("The Canadian Term Structure of Interest Rate", fontweight='bold')
61/44:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import math
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
61/45:
# Maturity
header = []
for name in ycn.dtype.names[1:]:
    maturity = float(name.split(" ")[0])
    if name.split(" ")[1] == 'M':
        maturity = maturity / 12
    header.append(maturity)
61/46:
# We create three empty lists 
x_data = []; y_data = []; z_data = []
for dt in ycn.Date:
    dt_num = dates.date2num(dt)
    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])
# print ('x_data: ', x_data[1:5])
61/47:
for row in ycn:
    y_data.append(header)
    z_data.append(list(row.tolist()[1:]))
# print ('y_data: ', y_data[1:5])
# print ('z_data: ', z_data[1:5])
61/48:
x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')
# x ==> Dates
# y ==> Maturities
# z ==> Yields
# print ('x:', x) 
# print ('y: ', y)
# print ('z: ', z)
61/49:
# %matplotlib inline
# fig = plt.figure(figsize=(15, 10))
# ax = fig.add_subplot(111, projection='3d')
# z_percent = z*100
# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))
# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')
# ax.set_ylabel('Maturity (\u03C4)')
# ax.set_zlabel('Yield (Percent)')
# plt.savefig('my_pgf_plot.jpeg')

# def format_date(x, pos=None):
#      return dates.num2date(x).strftime('%Y')
# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))
# for tl in ax.w_xaxis.get_ticklabels():
#     tl.set_ha('right')
#     tl.set_rotation(40)
# plt.show()
61/50:
# Stylized fact 1: The average yield curve over time is increasing and concave
average_yc = yc.mean(axis=0) * 100
average_yc.plot()
61/51:
# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, 
# humped, inverted humped, S-shapes)
curve_1 = yc.iloc[1]*100
curve_2 = yc.iloc[4174]*100 #2000
curve_3 = yc.iloc[5600]*100
curve_4 = yc.iloc[7850]*100
curve_1.plot()
curve_2.plot()
curve_3.plot()
curve_4.plot()
61/52:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
# Group by month (timestamp) and take the average
yc = yc.groupby(pd.Grouper(freq='M')).mean()
61/53: print(yc)
61/54:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

yc['3 M'].plot(figsize=(15,5), lw=1, color="blue", label='3-Month Government Bond')
yc['6 M'].plot( lw=0.5, color="lightblue",label='6-Month Government Bond')
yc['9 M'].plot( lw=0.5, color="darkgreen",label='9-Month Government Bond')
yc['1 Y'].plot( lw=0.5, color="deepskyblue",label='1-Year Government Bond')
yc['2 Y'].plot( lw=0.5, color="dodgerblue",label='2-Year Government Bond')
yc['3 Y'].plot( lw=0.5, color="steelblue",label='3-Year Government Bond')
yc['4 Y'].plot( lw=0.5, color="blue",label='4-Year Government Bond')
yc['5 Y'].plot( lw=0.5, color="mediumblue",label='5-Year Government Bond')
yc['6 Y'].plot( lw=0.5, color="slategrey", label='6-Year Government Bond')
yc['7 Y'].plot( lw=0.5, color="gray",label='7-Year Government Bond')
yc['8 Y'].plot( lw=0.5, color="red", grid=True,label='8-Year Government Bond')
yc['9 Y'].plot( lw=0.5, color="orange", grid=True,label='9-Year Government Bond')
yc['10 Y'].plot( lw=0.5, color="green", grid=True,label='10-Year Government Bond')
yc['15 Y'].plot( lw=0.5, color="cyan", grid=True,label='15-Year Government Bond')
yc['20 Y'].plot( lw=0.5, color="yellow", grid=True,label='20-Year Government Bond')
yc['25 Y'].plot( lw=0.5, color="pink", grid=True,label='25-Year Government Bond')
yc['30 Y'].plot( lw=0.5, color="black", grid=True,label='30-Year Government Bond')
plt.legend(bbox_to_anchor=(1, 1.02))
plt.xlabel("Date")
plt.ylabel("Canadian Government Bond Rate")
plt.title("The Canadian Term Structure of Interest Rate", fontweight='bold')
61/55:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import math
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
61/56:
# Maturity
header = []
for name in ycn.dtype.names[1:]:
    maturity = float(name.split(" ")[0])
    if name.split(" ")[1] == 'M':
        maturity = maturity / 12
    header.append(maturity)
61/57:
# We create three empty lists 
x_data = []; y_data = []; z_data = []
for dt in ycn.Date:
    dt_num = dates.date2num(dt)
    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])
# print ('x_data: ', x_data[1:5])
61/58:
for row in ycn:
    y_data.append(header)
    z_data.append(list(row.tolist()[1:]))
# print ('y_data: ', y_data[1:5])
# print ('z_data: ', z_data[1:5])
61/59:
x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')
# x ==> Dates
# y ==> Maturities
# z ==> Yields
# print ('x:', x) 
# print ('y: ', y)
# print ('z: ', z)
61/60:
# %matplotlib inline
# fig = plt.figure(figsize=(15, 10))
# ax = fig.add_subplot(111, projection='3d')
# z_percent = z*100
# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))
# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')
# ax.set_ylabel('Maturity (\u03C4)')
# ax.set_zlabel('Yield (Percent)')
# plt.savefig('my_pgf_plot.jpeg')

# def format_date(x, pos=None):
#      return dates.num2date(x).strftime('%Y')
# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))
# for tl in ax.w_xaxis.get_ticklabels():
#     tl.set_ha('right')
#     tl.set_rotation(40)
# plt.show()
61/61:
# Stylized fact 1: The average yield curve over time is increasing and concave
average_yc = yc.mean(axis=0) * 100
average_yc.plot()
61/62:
# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, 
# humped, inverted humped, S-shapes)
curve_1 = yc.iloc[1]*100
curve_2 = yc.iloc[4174]*100 #2000
curve_3 = yc.iloc[5600]*100
curve_4 = yc.iloc[7850]*100
curve_1.plot()
curve_2.plot()
curve_3.plot()
curve_4.plot()
61/63:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
# Group by month (timestamp) and take the average
yc_monthly = yc.groupby(pd.Grouper(freq='M')).mean()
61/64: print(yc)
61/65:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

yc['3 M'].plot(figsize=(15,5), lw=1, color="blue", label='3-Month Government Bond')
yc['6 M'].plot( lw=0.5, color="lightblue",label='6-Month Government Bond')
yc['9 M'].plot( lw=0.5, color="darkgreen",label='9-Month Government Bond')
yc['1 Y'].plot( lw=0.5, color="deepskyblue",label='1-Year Government Bond')
yc['2 Y'].plot( lw=0.5, color="dodgerblue",label='2-Year Government Bond')
yc['3 Y'].plot( lw=0.5, color="steelblue",label='3-Year Government Bond')
yc['4 Y'].plot( lw=0.5, color="blue",label='4-Year Government Bond')
yc['5 Y'].plot( lw=0.5, color="mediumblue",label='5-Year Government Bond')
yc['6 Y'].plot( lw=0.5, color="slategrey", label='6-Year Government Bond')
yc['7 Y'].plot( lw=0.5, color="gray",label='7-Year Government Bond')
yc['8 Y'].plot( lw=0.5, color="red", grid=True,label='8-Year Government Bond')
yc['9 Y'].plot( lw=0.5, color="orange", grid=True,label='9-Year Government Bond')
yc['10 Y'].plot( lw=0.5, color="green", grid=True,label='10-Year Government Bond')
yc['15 Y'].plot( lw=0.5, color="cyan", grid=True,label='15-Year Government Bond')
yc['20 Y'].plot( lw=0.5, color="yellow", grid=True,label='20-Year Government Bond')
yc['25 Y'].plot( lw=0.5, color="pink", grid=True,label='25-Year Government Bond')
yc['30 Y'].plot( lw=0.5, color="black", grid=True,label='30-Year Government Bond')
plt.legend(bbox_to_anchor=(1, 1.02))
plt.xlabel("Date")
plt.ylabel("Canadian Government Bond Rate")
plt.title("The Canadian Term Structure of Interest Rate", fontweight='bold')
61/66:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import math
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
61/67:
# Maturity
header = []
for name in ycn.dtype.names[1:]:
    maturity = float(name.split(" ")[0])
    if name.split(" ")[1] == 'M':
        maturity = maturity / 12
    header.append(maturity)
61/68:
# We create three empty lists 
x_data = []; y_data = []; z_data = []
for dt in ycn.Date:
    dt_num = dates.date2num(dt)
    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])
# print ('x_data: ', x_data[1:5])
61/69:
for row in ycn:
    y_data.append(header)
    z_data.append(list(row.tolist()[1:]))
# print ('y_data: ', y_data[1:5])
# print ('z_data: ', z_data[1:5])
61/70:
x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')
# x ==> Dates
# y ==> Maturities
# z ==> Yields
# print ('x:', x) 
# print ('y: ', y)
# print ('z: ', z)
61/71:
# %matplotlib inline
# fig = plt.figure(figsize=(15, 10))
# ax = fig.add_subplot(111, projection='3d')
# z_percent = z*100
# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))
# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')
# ax.set_ylabel('Maturity (\u03C4)')
# ax.set_zlabel('Yield (Percent)')
# plt.savefig('my_pgf_plot.jpeg')

# def format_date(x, pos=None):
#      return dates.num2date(x).strftime('%Y')
# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))
# for tl in ax.w_xaxis.get_ticklabels():
#     tl.set_ha('right')
#     tl.set_rotation(40)
# plt.show()
61/72:
# Stylized fact 1: The average yield curve over time is increasing and concave
average_yc = yc.mean(axis=0) * 100
average_yc.plot()
61/73:
# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, 
# humped, inverted humped, S-shapes)
curve_1 = yc.iloc[1]*100
curve_2 = yc.iloc[4174]*100 #2000
curve_3 = yc.iloc[5600]*100
curve_4 = yc.iloc[7850]*100
curve_1.plot()
curve_2.plot()
curve_3.plot()
curve_4.plot()
61/74: curve_1.plot()
61/75: curve_2.plot()
61/76: curve_3.plot()
61/77: curve_4.plot()
61/78:
# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)
# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities
first_autocorrelation = []
tenth_autocorrelation = []
twentieth_autocorrelation = []
for maturity in yc:
    first_autocorrelation.append(yc[maturity].autocorr())
    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))
    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))

# autocorrelations = 
# s = yc['3 M']
# s.autocorr()
data = []
for maturity in yc:
    data.append(maturity)

autocorrelations = pd.DataFrame(data, columns=['Maturity'])
autocorrelations['First autocorrelation'] = first_autocorrelation 
autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation
autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation
autocorrelations.round(5)
61/79:
# Stylized fact 5: The short end of the yield curve is more volatile than the long end of the curve
stdevs = yc.std()
stdevs.round(5)
61/80:
# Stylized fact 6: Yields for different maturities have high cross-correlations
yc.corr().round(3)
61/81:
yc = yc_monthly
yc
61/82: header
61/83:
import math
def find_lambda(lmbd):
    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)
    return  lmbd_val
    
lmbd_range = np.arange(-0.1,2,0.0001)
lmbd_values = []
for i in lmbd_range:
    lmbd_values.append(find_lambda(i))
plt.plot(lmbd_range,lmbd_values)
61/84:
lambda_max = lmbd_range[np.argmax(lmbd_values)]
print(lambda_max)
61/85:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
print(np.shape(B))
61/86:
(rows, cols) = np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))

print(beta)
61/87: np.shape(beta)
61/88:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 30
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/89:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/90:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/91:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 6
WINDOW_SIZE = 365*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/92:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 6
WINDOW_SIZE = 12*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE])
    
    index = yields.index[i+WINDOW_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/93:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

plot_forecasts_facet(residual_forecasts)
61/94:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 6
WINDOW_SIZE = 12*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])
    
    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/95:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)
61/96:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 1
WINDOW_SIZE = 12*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    #ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])
    
    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    #forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/97:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)
61/98:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)
61/99:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['Realized'], df['Random walk']),
                'mse' : metrics.mean_squared_error(df['Realized'], df['Random walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['Realized'], df['Random walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['Realized'], df['Random walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['Realized'], df['Random walk']),
                'mse' : metrics.mean_squared_error(df['Realized'], df['Random walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['Realized'], df['Random walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['Realized'], df['Random walk'])
            }
        }

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
print(errors)
61/100:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(df):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk']),
                'mape' : metrics.mean_absolute_percentage_error(ddf['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
print(errors)
61/101:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(df):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(ddf['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
print(errors)
61/102:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(df):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
print(errors)
61/103:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
print(errors)
61/104:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
errors
61/105:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
errors
61/106:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
errors['30 Y']

errors['10 Y']
61/107:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        #'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        #'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(pd.DataFrame(errors[key]).T)
    print("")
61/108:
np.shape(beta)
print(beta[1,:])
61/109:
np.shape(beta)
print(beta[:,1])
61/110:
np.shape(beta)
print(beta[:,2])
61/111:
np.shape(beta)
print(beta[:,3])
61/112:
np.shape(beta)
print(beta[:,0])
61/113:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 1
WINDOW_SIZE = 12*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])
    
    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
61/114:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    #ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(pd.DataFrame(errors[key]).T)
    print("")
61/115:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(pd.DataFrame(errors[key]).T)
    print("")
61/116:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(pd.DataFrame(errors[key]).T)
    print("")
62/1:
import pandas as pd
import wetterdienst as wd

# Get all stations from NOAA GHCN
stations = wd.provider.noaa.ghcn.stations()
print(stations)
63/1:
import pandas as pd
import wetterdienst as wd

# Get all stations from NOAA GHCN
stations = wd.provider.noaa.ghcn.stations()
print(stations)
63/2:
import pandas as pd
import wetterdienst as wd

# What is available for us
wd.discover()


# Get all stations from NOAA GHCN
#stations = wd.provider.noaa.ghcn.stations()
#print(stations)
63/3:
import pandas as pd
from wetterdienst import Wetterdienst
# What is available for us
Wetterdienst.discover()


# Get all stations from NOAA GHCN
#stations = wd.provider.noaa.ghcn.stations()
#print(stations)
63/4:
import pandas as pd
from wetterdienst import Wetterdienst
# What is available for us
Wetterdienst.discover()
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
API.discover()
63/5:
import pandas as pd
from wetterdienst import Wetterdienst
# What is available for us
Wetterdienst.discover()
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#API.discover()
63/6:
import pandas as pd
from wetterdienst import Wetterdienst
# What is available for us
Wetterdienst.discover()
63/7:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#API.discover()
# Fetch all stations
API.endpoints()
63/8:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#API.discover()
# Fetch all stations
API.endpoints
63/9:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#API.discover()
# Fetch all stations
API.stations()
63/10:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#API.discover()
# Fetch all stations
API._all()
63/11:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#API.discover()
# Fetch all stations
API.all()
63/12:
# We are interested in data from NOAA GHCN
stations = wd.provider.noaa.ghcn.stations()
63/13:
# We are interested in data from NOAA GHCN
stations = wd.provider.noaa.ghcn._all()
63/14:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
# Show all functions for this API
dir(API)
63/15:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#dir(API)
API.datasets
63/16:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#dir(API)
API.discover()
63/17:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#dir(API)
API._dataset_base
63/18:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#dir(API)
stations = API._all()
63/19:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
#dir(API)
stations = API._all(API)
63/20: print(stations)
63/21:
# Plot all the stations on an interactive map with labels (using leaflet)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=row['NAME']).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations)
63/22:
# Plot all the stations on an interactive map with labels (using leaflet)
print(stations)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=row['NAME']).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations)
63/23:
# Plot all the stations on an interactive map with labels (using leaflet)
print(stations)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude '], row['longitude  ']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations)
63/24:
# Plot all the stations on an interactive map with labels (using leaflet)
print(stations)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude '], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations)
63/25:
# Plot all the stations on an interactive map with labels (using leaflet)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations)
63/26:
# Plot all the stations on an interactive map with labels using clustering
print(stations)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

#plot_stations(stations)
63/27:
# Plot all the stations on an interactive map with labels using clustering
print(stations.columns)
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

#plot_stations(stations)
63/28:
# Plot all the stations on an interactive map with labels using clustering
print(stations.columns)
# Unique state
print(stations['state'].unique())
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

#plot_stations(stations)
63/29:
# Plot all the stations on an interactive map with labels using clustering
print(stations.head())
print(stations.columns)
# Unique state
print(stations['state'].unique())
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

#plot_stations(stations)
63/30:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['id'].str[0:2]
stations.head()
63/31:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id '].str[0:2]
stations.head()
63/32:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]
stations.head()
63/33:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Join the two tables
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
63/34:
# What did not join
print(stations[stations['alpha-3'].isna()])
63/35:
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/36:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_ghcnd = pd.read_csv("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-countries.txt", sep = '\t', header = ['country_code', 'country_name'])
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Join the two tables
stations = stations.merge(lookup_table_ghcnd, left_on = 'country_code', right_on = 'country_code', how = 'left')
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
63/37:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_ghcnd = pd.read_csv("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-countries.txt", sep = '\s', header = ['country_code', 'country_name'])
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Join the two tables
stations = stations.merge(lookup_table_ghcnd, left_on = 'country_code', right_on = 'country_code', how = 'left')
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
63/38:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Join the two tables
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/39:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region_code'].isin(['EU'])]
print(stations_eu.shape)
63/40:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['EU'])]
print(stations_eu.shape)
63/41:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['EU'])]
print(stations_eu)
63/42:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu)
63/43:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/44:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())
def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/45:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

#plot_stations(stations_eu)
63/46:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]
print(stations.shape())


lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
print(stations.shape())
63/47:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]
# Dimensions of the data
print(stations.shape)


lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table, left_on='country_code', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
print(stations.shape)
63/48:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by a new line.
        label  = row.to_string(header=False, index=False)
        
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/49:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/50:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['longitude'], row['latitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m
63/51:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['longitude'], row['latitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/52:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/53:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv', header = ['fips','iso','fips_name'])
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/54:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')
lookup_table_fips.columns = ['fips', 'iso', 'fips_name']
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/55:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

# Is Germany in the list?
print (stations[stations['country_name'] == 'Germany'])

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')

    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/56:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np

    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    # Create fixed size for the map to display
    m._build_map()
    m.fit_bounds(m.get_bounds())
    
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    return m

plot_stations(stations_eu)
63/57:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = row.to_string(header=False, index=False)
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/58:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Label is all the information in the row, seperated by \n (new line)
        label = '\n'.join(row.astype(str))
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/59:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/60:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:6]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/61:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/62:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron')
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/63:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/64:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
# Generate some summary statistics

# (1) How many stations are there per country?
stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# (2) Average years of data per country (to_date - from_date) in years
stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / 365
stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)
63/65:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

print(plot_stations(stations_eu))
# Generate some summary statistics

# (1) How many stations are there per country?
print(stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False))
# (2) Average years of data per country (to_date - from_date) in years
stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)
print(stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False))
63/66:
# (1) How many stations are there per country?
result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# Show as bar chart
result.plot(kind='bar', figsize=(20, 5), title='Number of stations per country')
63/67:
# (1) How many stations are there per country?
result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
plt.title('Number of stations per country')
plt.xticks(rotation=90)
plt.show()
63/68:
# (1) How many stations are there per country?
result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
plt.title('Number of stations per country')
plt.xticks(rotation=90)
plt.show()
63/69:
# (1) How many stations are there per country?
result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
# Put numbers on top of bars
for index, data in enumerate(result.values):
    plt.text(x=index, y =data+1 , s=f"{data}" , fontdict=dict(fontsize=10))
plt.title('Number of stations per country')
plt.xticks(rotation=90)
plt.show()
63/70:
# (1) How many stations are there per country?
result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
# Put numbers on top of bars
for index, data in enumerate(result.values):
    plt.text(x=index, y =data , s=f"{data}" , fontdict=dict(fontsize=10))
plt.title('Number of stations per country')
plt.xticks(rotation=90)
plt.show()
63/71:
# (1) How many stations are there per country?
result = stations_eu.groupby('country_name')['station_id'].count().sort_values(ascending=False)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
# Put numbers on top of bars
for index, data in enumerate(result.values):
    plt.text(x=index, y =data+10 , s=f"{data}" , fontdict=dict(fontsize=10))
plt.title('Number of stations per country')
plt.xticks(rotation=90)
plt.show()
63/72:
# (2) Average years of data per country (to_date - from_date) in years
stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)
result = stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
# Put numbers on top of bars 
for index, data in enumerate(result.values):
    plt.text(x=index, y =data+10 , s=f"{data}" , fontdict=dict(fontsize=10))
plt.title('Average years of data per country')
plt.xticks(rotation=90)
plt.show()
63/73:
# (2) Average years of data per country (to_date - from_date) in years
stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)
result = stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)
# Round to 2 decimals
result = result.round(2)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
# Put numbers on top of bars 
for index, data in enumerate(result.values):
    plt.text(x=index, y =data+10 , s=f"{data}" , fontdict=dict(fontsize=10))
plt.title('Average years of data per country')
plt.xticks(rotation=90)
plt.show()
63/74:
# (2) Average years of data per country (to_date - from_date) in years
stations_eu['years_of_data'] = (stations_eu['to_date'] - stations_eu['from_date']) / pd.Timedelta(days=365)
result = stations_eu.groupby('country_name')['years_of_data'].mean().sort_values(ascending=False)
# Round to 2 decimals
result = result.round(2)
# Show as bar chart using matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.bar(result.index, result.values)
plt.title('Average years of data per country')
plt.xticks(rotation=90)
plt.show()
63/75:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/76:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)

API.discover()
63/77:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

request = NoaaGhcnRequest(
    parameter = parameter,
    resolution = 'daily',
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')
print(request)
63/78:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

request = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')
print(request)
63/79:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

request = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')

df = request.values.all().df.dropna()
63/80:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

request = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')

df = request.values.query()
63/81: print(df.head())
63/82:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

request = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')

data = request.values.query()
63/83:
for result in data:
    print(result.df.dropna().head())
63/84:
for result in data:
    print(result.df.head())
63/85:
for result in data:
    print(result.df.head())
63/86:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

request = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')

print(request)
#data = request.values.query()
63/87:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

stations_object = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')

def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
63/88:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = '2010-01-01'
end_date = '2020-01-01'

stations_object = NoaaGhcnRequest(
    parameter = parameter,
    start_date = start_date,
    end_date = end_date,
).filter_by_station_id('NLE00101920')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
63/89:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter


#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = pd.datetime(2010, 1, 1)
end_date =  pd.datetime(2020, 1, 1)

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,
    start_date=pd.datetime(1992, 1, 1),
    end_date=pd.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=39.559952,
     longitude=2.678001,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
63/90:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,
    start_date=dt.datetime(1992, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=39.559952,
     longitude=2.678001,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/91:
# Select a station in Amsterdam and load the data for the last 10 years (NLE00101920)
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.WIND_SPEED, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(1992, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_station_id('NLE00101920')

def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/92:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

# Amsterdam location
LATITUDE = 52.370216
LONGITUDE = 4.895168

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.WIND_SPEED, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(1992, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=LATITUDE,
     longitude=LONGITUDE,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/93:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

# Amsterdam location
LATITUDE = 52.370216
LONGITUDE = 4.895168

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(1992, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=LATITUDE,
     longitude=LONGITUDE,
     distance=10
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/94:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

# Amsterdam location
LATITUDE = 52.370216
LONGITUDE = 4.895168

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(1992, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_station_id('NLE00101920')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/95:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

# Amsterdam location
LATITUDE = 52.370216
LONGITUDE = 4.895168

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,
    start_date=dt.datetime(1992, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_station_id('NLE00101920')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/96:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

#API.discover()
parameter = ['wind_speed', 'temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200']
start_date = dt.datetime(2010, 1, 1)
end_date =  dt.datetime(2020, 1, 1)

# Amsterdam location
LATITUDE = 52.370216
LONGITUDE = 4.895168

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_station_id('NLE00101920')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
64/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
directory = "C:/Users/Nicky/Documents/School/Assignments/QMFI"
yc = pd.read_excel(f"{directory}/YC.xls", index_col='Date')
# Group by month (timestamp) and take the average
yc_monthly = yc.groupby(pd.Grouper(freq='M')).mean()
64/2: print(yc)
64/3:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

yc['3 M'].plot(figsize=(15,5), lw=1, color="blue", label='3-Month Government Bond')
yc['6 M'].plot( lw=0.5, color="lightblue",label='6-Month Government Bond')
yc['9 M'].plot( lw=0.5, color="darkgreen",label='9-Month Government Bond')
yc['1 Y'].plot( lw=0.5, color="deepskyblue",label='1-Year Government Bond')
yc['2 Y'].plot( lw=0.5, color="dodgerblue",label='2-Year Government Bond')
yc['3 Y'].plot( lw=0.5, color="steelblue",label='3-Year Government Bond')
yc['4 Y'].plot( lw=0.5, color="blue",label='4-Year Government Bond')
yc['5 Y'].plot( lw=0.5, color="mediumblue",label='5-Year Government Bond')
yc['6 Y'].plot( lw=0.5, color="slategrey", label='6-Year Government Bond')
yc['7 Y'].plot( lw=0.5, color="gray",label='7-Year Government Bond')
yc['8 Y'].plot( lw=0.5, color="red", grid=True,label='8-Year Government Bond')
yc['9 Y'].plot( lw=0.5, color="orange", grid=True,label='9-Year Government Bond')
yc['10 Y'].plot( lw=0.5, color="green", grid=True,label='10-Year Government Bond')
yc['15 Y'].plot( lw=0.5, color="cyan", grid=True,label='15-Year Government Bond')
yc['20 Y'].plot( lw=0.5, color="yellow", grid=True,label='20-Year Government Bond')
yc['25 Y'].plot( lw=0.5, color="pink", grid=True,label='25-Year Government Bond')
yc['30 Y'].plot( lw=0.5, color="black", grid=True,label='30-Year Government Bond')
plt.legend(bbox_to_anchor=(1, 1.02))
plt.xlabel("Date")
plt.ylabel("Canadian Government Bond Rate")
plt.title("The Canadian Term Structure of Interest Rate", fontweight='bold')
64/4:
import numpy as np
from mpl_toolkits.mplot3d import axes3d
import matplotlib.dates as dates
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
import math
# Numpy.recarray
ycn = yc.to_records()
# type(ycn)
# ycn
64/5:
# Maturity
header = []
for name in ycn.dtype.names[1:]:
    maturity = float(name.split(" ")[0])
    if name.split(" ")[1] == 'M':
        maturity = maturity / 12
    header.append(maturity)
64/6:
# We create three empty lists 
x_data = []; y_data = []; z_data = []
for dt in ycn.Date:
    dt_num = dates.date2num(dt)
    x_data.append([dt_num for i in range(len(ycn.dtype.names)-1)])
# print ('x_data: ', x_data[1:5])
64/7:
for row in ycn:
    y_data.append(header)
    z_data.append(list(row.tolist()[1:]))
# print ('y_data: ', y_data[1:5])
# print ('z_data: ', z_data[1:5])
64/8:
x = np.array(x_data, dtype='f'); y = np.array(y_data, dtype='f'); z = np.array(z_data, dtype='f')
# x ==> Dates
# y ==> Maturities
# z ==> Yields
# print ('x:', x) 
# print ('y: ', y)
# print ('z: ', z)
64/9:
# %matplotlib inline
# fig = plt.figure(figsize=(15, 10))
# ax = fig.add_subplot(111, projection='3d')
# z_percent = z*100
# ax.plot_surface(x, y, z_percent, rstride=2, cstride=1, cmap='viridis', vmin=np.nanmin(z_percent), vmax=np.nanmax(z_percent))
# ax.set_title('The Canadian Term Structure of Interest Rates (Zero-Coupon Bonds)')
# ax.set_ylabel('Maturity (\u03C4)')
# ax.set_zlabel('Yield (Percent)')
# plt.savefig('my_pgf_plot.jpeg')

# def format_date(x, pos=None):
#      return dates.num2date(x).strftime('%Y')
# ax.w_xaxis.set_major_formatter(ticker.FuncFormatter(format_date))
# for tl in ax.w_xaxis.get_ticklabels():
#     tl.set_ha('right')
#     tl.set_rotation(40)
# plt.show()
64/10:
# Stylized fact 1: The average yield curve over time is increasing and concave
average_yc = yc.mean(axis=0) * 100
average_yc.plot()
64/11:
# Stylized fact 2: The yield curve can take on a variety of shapes (upward sloping, downward sloping, 
# humped, inverted humped, S-shapes)
curve_1 = yc.iloc[1]*100
curve_2 = yc.iloc[4174]*100 #2000
curve_3 = yc.iloc[5600]*100
curve_4 = yc.iloc[7850]*100
curve_1.plot()
curve_2.plot()
curve_3.plot()
curve_4.plot()
64/12: curve_1.plot()
64/13: curve_2.plot()
64/14: curve_3.plot()
64/15: curve_4.plot()
64/16:
# Stylized fact 3: Yield dynamics are (very) persistent (high auto-correlations)
# Stylized fact 4: Yields for long maturities are more persistent than yields for shorter maturities
first_autocorrelation = []
tenth_autocorrelation = []
twentieth_autocorrelation = []
for maturity in yc:
    first_autocorrelation.append(yc[maturity].autocorr())
    tenth_autocorrelation.append(yc[maturity].autocorr(lag=10))
    twentieth_autocorrelation.append(yc[maturity].autocorr(lag=20))

# autocorrelations = 
# s = yc['3 M']
# s.autocorr()
data = []
for maturity in yc:
    data.append(maturity)

autocorrelations = pd.DataFrame(data, columns=['Maturity'])
autocorrelations['First autocorrelation'] = first_autocorrelation 
autocorrelations['Tenth autocorrelation'] = tenth_autocorrelation
autocorrelations['Twentieth autocorrelation'] = twentieth_autocorrelation
autocorrelations.round(5)
64/17:
# Stylized fact 5: The short end of the yield curve is more volatile than the long end of the curve
stdevs = yc.std()
stdevs.round(5)
64/18:
# Stylized fact 6: Yields for different maturities have high cross-correlations
yc.corr().round(3)
64/19:
yc = yc_monthly
yc
64/20: header
64/21:
import math
def find_lambda(lmbd):
    lmbd_val = (1- math.exp(-3*lmbd))/(3*lmbd) - math.exp(-3*lmbd)
    return  lmbd_val
    
lmbd_range = np.arange(-0.1,2,0.0001)
lmbd_values = []
for i in lmbd_range:
    lmbd_values.append(find_lambda(i))
plt.plot(lmbd_range,lmbd_values)
64/22:
lambda_max = lmbd_range[np.argmax(lmbd_values)]
print(lambda_max)
64/23:
# Calcuate the beta1, beta2 and beta3 by first determining level, slope and curvature using lambda_max
(rows, cols)= np.shape(yc)
B = []
for maturity in header:
    B.append([1, (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity), (1 - math.exp(-lambda_max*maturity))/(lambda_max*maturity) - math.exp(-lambda_max*maturity)])
    
B = np.array(B)
# Slopes is second column of B
slopes = B[:,1]
curvatures = B[:,2]
print(B)
print(np.shape(B))
64/24:
(rows, cols) = np.shape(yc)
beta = np.zeros((rows, 3))

for i in range(1, rows): 
    beta[:i] = np.transpose(np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(B), B, None)),np.transpose(B), None), np.transpose(yc[:i]), None))

print(beta)
64/25:
np.shape(beta)
print(beta[:,0])
64/26:
# Create the forecast functions
import statsmodels.api as sm

# Nelson-Siegel AR forecast with comments
def ns_ar_forecast(beta, step_size):
    lagged_beta = np.roll(beta, step_size, axis=0)
    temp_beta = beta[step_size:,:]
    lagged_beta = lagged_beta[step_size:,:]
    fc_beta = [None, None, None]
    for f in range(3):
        # Create the X matrix (iota, lagged_beta)
        x = lagged_beta[:,f]
        # Create the Y matrix (temp_beta)
        y = temp_beta[:,f]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_beta[f] = const + gamma*beta[-1,f]
        #print(regression.summary())

    fc_y = fc_beta[0] + fc_beta[1] * slopes + fc_beta[2] * curvatures
    return fc_y

def ar_yield_level_forecast(yield_data, step_size):
    lagged = np.roll(yield_data, step_size, axis=0)
    temp = yield_data[step_size:,:]
    lagged = lagged[step_size:,:]
    
    # Create empty list of length len(header)
    fc_y = [None] * len(header)
    for idx, maturity in enumerate(header):
        y = temp[idx]
        x = lagged[idx]
        regression = sm.OLS(y, sm.add_constant(x)).fit()
        const = regression.params[0]
        gamma = regression.params[1]
        fc_y[idx] = const + gamma*yield_data[-1,idx]
        
    return fc_y

yields = yc
# Transform to np array
yield_data = np.array(yields)
print(yields)

STEP_SIZE = 1
WINDOW_SIZE = 12*5

forecast_yields = {
    'index' : [],
    'random_walk' : [],
    'nelson_siegel' : [],
    'ar_yield_levels' : [],
    'realized' : []
}

for i in range(0 , len(yields) - WINDOW_SIZE - STEP_SIZE, 1):
    subset = yield_data[i:i+WINDOW_SIZE]
    subset_beta = beta[i:i+WINDOW_SIZE]
    print(f"Iteration {i} of {len(yields) - WINDOW_SIZE - STEP_SIZE}")
    # Random walk
    random_walk = np.array(subset[-1])
    # Nelson-Siegel
    nelson_siegel = ns_ar_forecast(subset_beta, STEP_SIZE)
    # Ar yield levels
    ar_yield_levels = ar_yield_level_forecast(subset, STEP_SIZE)
    # Realized
    realized = np.array(yield_data[i+WINDOW_SIZE + STEP_SIZE])
    
    index = yields.index[i+WINDOW_SIZE + STEP_SIZE]
    # Append to dictionary
    forecast_yields['random_walk'].append(random_walk)
    forecast_yields['nelson_siegel'].append(nelson_siegel)
    forecast_yields['ar_yield_levels'].append(ar_yield_levels)

    forecast_yields['realized'].append(realized)
    forecast_yields['index'].append(index)
64/27:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        error_measures[maturity] = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(pd.DataFrame(errors[key]).T)
    print("")
64/28:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")
64/29:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures
def plot_error_measures(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Error'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="Error", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error", "Value")
    g.add_legend()
    g.fig.suptitle("Yield curve forecast error measures")
    g.fig.subplots_adjust(top=0.9)

plot_error_measures(errors)
64/30:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures
def plot_error_measures(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Error'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="Error", y="error", hue="forecast")
    g.set_xticklabels(rotation=90)
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error", "Value")
    g.add_legend()
    g.fig.suptitle("Yield curve forecast error measures")
    g.fig.subplots_adjust(top=0.9)

plot_error_measures(errors)
64/31:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures
def plot_error_measures(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'forecast'], var_name='maturity', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Error'})
    
    g = sns.FacetGrid(df, col="forecast", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="Error", y="error", hue="maturity")
    g.set_xticklabels(rotation=90)
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error", "Value")
    g.add_legend()
    g.fig.suptitle("Yield curve forecast error measures")
    g.fig.subplots_adjust(top=0.9)

plot_error_measures(errors)
64/32:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures
def plot_error_measures(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'forecast'], var_name='maturity', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Error'})
    
    g = sns.FacetGrid(df, col="forecast", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="Error", y="error", hue="maturity")
    g.set_xticklabels(rotation=90)
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error", "Value")
    g.add_legend()
    g.fig.suptitle("Yield curve forecast error measures")
    g.fig.subplots_adjust(top=0.9)

plot_error_measures(errors)
64/33:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures
def plot_error_measures(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Error'})
    
    g = sns.FacetGrid(df, col="forecast", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="Error", y="error", hue="maturity")
    g.set_xticklabels(rotation=90)
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Maturity", "Value")
    g.add_legend()
    g.fig.suptitle("Yield curve forecast error measures")
    g.fig.subplots_adjust(top=0.9)

plot_error_measures(errors)
64/34:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="error_measure", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures(errors)
64/35:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="error_measure", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/36:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/37:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Free y axis
    g.set(ylim=(None, None))
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/38:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, row="error_measure", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/39:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=1, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/40:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=1, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=(None, None))
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/41:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=1, height=3, aspect=1.5)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/42:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=1, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/43:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=1, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", dodge=False)
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    # Increase bar width
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/44:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    # Increase bar width
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/45:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    # Slightly increasse bar widt
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=3, edgecolor=".2")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/46:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    # Slightly increasse bar widt
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=1, edgecolor=".5")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/47:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    # Slightly increasse bar widt
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=1, edgecolor=".2", width=.5)
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/48:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    # Slightly increasse bar widt
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8,  edgecolor=".2", width=.5)
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    
    
    
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/49:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=1, edgecolor=".2")
    g.set_titles(col_template="{col_name}")
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=0.9)


plot_error_measures_facet(errors)
64/50:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=1, edgecolor=".2")
    g.set_titles(col_template="{col_name}")
    # Increase margin width between x-axis labels
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90)
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=1, wspace=0.3)


plot_error_measures_facet(errors)
64/51:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=1, edgecolor=".2")
    g.set_titles(col_template="{col_name}")
    # Increase margin width between x-axis labels
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90, fontsize=8)
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=.9)


plot_error_measures_facet(errors)
64/52:
# Do analysis on the forecasts
import seaborn as sns

# Convert to dataframe (for every maturity)
maturity_forecasts = {}
residual_forecasts = {}

for idx, maturity in enumerate(yields.columns):
    # Select the idx element from every forecast
    random_walk = np.array(forecast_yields['random_walk'])[:,idx]
    nelson_siegel = np.array(forecast_yields['nelson_siegel'])[:,idx]
    ar_yield_levels = np.array(forecast_yields['ar_yield_levels'])[:,idx]
    realized = np.array(forecast_yields['realized'])[:,idx]
    tmp = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : random_walk,
        'nelson_siegel' : nelson_siegel,
        'ar_yield_levels' : ar_yield_levels,
        'realized' : realized
    }
    tmp_residual = {
        'index' : np.array(forecast_yields['index']),
        'random_walk' : realized - random_walk,
        'nelson_siegel' : realized - nelson_siegel,
        'ar_yield_levels' : ar_yield_levels - realized

    }
    
    maturity_forecasts[maturity] = pd.DataFrame(tmp)
    residual_forecasts[maturity] = pd.DataFrame(tmp_residual)


# Print all the maturities
print(maturity_forecasts.keys())
print(maturity_forecasts['30 Y'])

# Seems about correct. Now we can plot the forecasts in a facet grid (with the maturities)
def plot_forecasts(maturity_forecasts, maturity):
    df = maturity_forecasts[maturity]
    df = df.set_index('index')
    df = df.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
    df.plot(figsize=(15,5), title=f"Yield curve forecasts for {maturity} maturity")
    
# Plot within a facet grid
def plot_forecasts_facet(maturity_forecasts):
    df = pd.DataFrame()
    for maturity in maturity_forecasts.keys():
        tmp = maturity_forecasts[maturity]
        tmp = tmp.set_index('index')
        tmp = tmp.rename(columns={'realized': 'Realized', 'random_walk': 'Random walk', 'nelson_siegel': 'Nelson-Siegel'})
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='yield')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'Date'})
    
    g = sns.FacetGrid(df, col="maturity", col_wrap=4, height=3, aspect=1.5)
    g.map_dataframe(sns.lineplot, x="Date", y="yield", hue="forecast")
    g.set_titles(col_template="{col_name}")
    g.set_axis_labels("Date", "Yield")
    g.add_legend()
    g.fig.suptitle("Yield curve forecasts")
    g.fig.subplots_adjust(top=0.9)

plot_forecasts(maturity_forecasts, '1 Y')

plot_forecasts(maturity_forecasts, '25 Y')

plot_forecasts(residual_forecasts, '1 Y')

#plot_forecasts_facet(maturity_forecasts)

#plot_forecasts_facet(residual_forecasts)

# Compute all error measures
import sklearn.metrics as metrics

def compute_error_measures(maturity_forecasts):
    error_measures = {}
    for maturity in maturity_forecasts.keys():
        df = maturity_forecasts[maturity]
        # Compute error measures
        errors_dict = {
            'random_walk' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['random_walk']),
                'mse' : metrics.mean_squared_error(df['realized'], df['random_walk']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['random_walk'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['random_walk'])
            },
            'nelson_siegel' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['nelson_siegel']),
                'mse' : metrics.mean_squared_error(df['realized'], df['nelson_siegel']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['nelson_siegel'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['nelson_siegel'])
            },
            'ar' : {
                'mae' : metrics.mean_absolute_error(df['realized'], df['ar_yield_levels']),
                'mse' : metrics.mean_squared_error(df['realized'], df['ar_yield_levels']),
                'rmse' : np.sqrt(metrics.mean_squared_error(df['realized'], df['ar_yield_levels'])),
                'mape' : metrics.mean_absolute_percentage_error(df['realized'], df['ar_yield_levels'])
            }
        }
        # Convert to dataframe
        error_df = pd.DataFrame(errors_dict)
        error_measures[maturity]  = error_df
    return error_measures

# Show in tabular format
errors = compute_error_measures(maturity_forecasts)
for key in errors.keys():
    print(f"Error measures for {key} maturity")
    print(errors[key].T)
    print("")

# Facet grid on error measures (x is maturity, y is error value and hue is the forecast)
def plot_error_measures_facet(error_measures):
    df = pd.DataFrame()
    for maturity in error_measures.keys():
        tmp = error_measures[maturity]
        tmp['maturity'] = maturity
        # pivot longer
        tmp = tmp.reset_index().melt(id_vars=['index', 'maturity'], var_name='forecast', value_name='error')
        df = df.append(tmp)
    df = df.reset_index()
    df = df.rename(columns={'index': 'error_measure'})
    g = sns.FacetGrid(df, col="error_measure", col_wrap=2, height=3, aspect=1.5, sharey = False)
    g.map_dataframe(sns.barplot, x="maturity", y="error", hue="forecast", palette="Set2", ci=None, alpha=0.8, linewidth=1, edgecolor=".2")
    g.set_titles(col_template="{col_name}")
    # Increase margin width between x-axis labels
    # Disable shared y axis across facets
    g.set(ylim=None)
    # Increase size between x axis labels
    g.set_xticklabels(rotation=90, fontsize=8)
    g.set_axis_labels("Error measure", "Error")
    g.add_legend()
    g.fig.suptitle("Error measures")
    g.fig.subplots_adjust(top=.9)
    # Increase plot size
    g.fig.set_size_inches(15, 5)


plot_error_measures_facet(errors)
63/97:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('AMSTERDAM')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/98:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('BERLIN')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/99:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=39.559952,
     longitude=2.678001,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/100:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=39.559952,
     longitude=2.678001,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/101:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('AMSTERDAM')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/102:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('BERLIN')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/103:
# Select a station in Amsterdam and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_station_id('GME00127990')

# CHeck which parameters are available for stations in Berlin


print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/104:
# Select a station in London and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_sql("name = '%London%'")

# CHeck which parameters are available for stations in Berlin


print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/105:
# Select a station in London and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('London')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/106:
# Select a station in London and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=39.559952,
     longitude=2.678001,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/107:
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['value'])
plt.title('Temperature in London')
plt.show()
63/108:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_distance(
     latitude=39.559952,
     longitude=2.678001,
     distance=30
)

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/109:
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['value'])
plt.title('Temperature in London')
plt.show()
63/110:
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['value'])
plt.xticks(rotation=90)
plt.show()
63/111:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.PRECIPITATION_HEIGHT,
    start_date=dt.datetime(2010, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('AMSTERDAM')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/112:
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['value'])
plt.xticks(rotation=90)
plt.show()
63/113:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('AMSTERDAM')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/114:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_200,
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('AMSTERDAM')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/115:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.WIND_SPEED,
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('AMSTERDAM')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/116:
# Get historical temperature data for Dresden.
from wetterdienst import Settings
from wetterdienst.provider.dwd.observation import DwdObservationRequest

request = DwdObservationRequest(
   parameter=["tavg"],
   resolution="daily",
   start_date="1990-01-01",  # if not given timezone defaulted to UTC
   end_date="2020-01-01",  # if not given timezone defaulted to UTC
).filter_by_station_id(station_id=(1048))

df = request.values.all().df
63/117:
# Get historical temperature data for Dresden.
from wetterdienst import Settings
from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter


request = DwdObservationRequest(
   parameter=["climate_summary"],
   resolution="daily",
   start_date="1990-01-01",  # if not given timezone defaulted to UTC
   end_date="2020-01-01",  # if not given timezone defaulted to UTC
).filter_by_station_id(station_id=(1048))

df = request.values.all().df
63/118: print(df.head())
63/119:
print(df.head())
print(df['parameter'].unique())
63/120:
# Get historical temperature data for Dresden.
from wetterdienst import Settings
from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter


request = DwdObservationRequest(
   parameter=["climate_summary"],
   resolution="daily",
   start_date="2000-01-01",  # if not given timezone defaulted to UTC
   end_date="2020-01-01",  # if not given timezone defaulted to UTC
).filter_by_station_id(station_id=(4411))

df = request.values.query().df
63/121:
# Get historical temperature data for Dresden.
from wetterdienst import Settings
from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter


request = DwdObservationRequest(
   parameter=["climate_summary"],
   resolution="daily",
   start_date="2000-01-01",  # if not given timezone defaulted to UTC
   end_date="2020-01-01",  # if not given timezone defaulted to UTC
).filter_by_station_id(station_id=(4411))

df = get_data_from_stations_request(request)
63/122:
print(df.head())
print(df['parameter'].unique())
63/123:
print(df)
print(df['parameter'].unique())
63/124:
print(df.head())
print(df['parameter'].unique())
63/125:
print(df.head())

# Plot the temperature data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df.index, df['TT_TU'])
plt.title('Temperature in Dresden')
plt.show()
63/126:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df.index, df['TT_TU'])
plt.title('Temperature in Dresden')
plt.show()
63/127:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df.index, df['temperature_air_mean_200'])
plt.title('Temperature in Dresden')
plt.show()
63/128:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df.index, df['temperature_air_mean_200'])
plt.title('Temperature in Dresden')
plt.show()
63/129:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'])
plt.title('Temperature in Dresden')
plt.show()
63/130:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'].values, df['value'].values)
plt.title('Temperature in Dresden')
plt.show()
63/131:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temperature = df[df['parameter'] == 'temperature_air_mean_200']
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(temperature['date'].values, temperature['value'].values)
plt.title('Temperature in Dresden')
plt.show()
63/132:
print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_200', 'temperature_air_200_max', 'temperature_air_200_min'])]
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()
63/133:
#print(df.head())
#print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_200', 'temperature_air_200_max', 'temperature_air_200_min'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()
63/134:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_200', 'temperature_air_200_max', 'temperature_air_200_min'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()
63/135:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()
63/136:
import pandas as pd
import numpy as np
from wetterdienst import Wetterdienst
import wetterdienst as wd
# What is available for us
Wetterdienst.discover()
63/137:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season.index.month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
63/138:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season['date'].month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
63/139:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season['date'].dt.month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
63/140:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():
    plt.hist(temps_season[temps_season['season'] == season]['value'], alpha=0.5, label=season)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/141:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():
    plt.hist(temps_season[temps_season['season'] == season, temps_season['parameter'] == 'temperature_air_mean_200']['value'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/142:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():
    plt.hist(temps_season[temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200']['value'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/143:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():
    plt.hist(temps_season[temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200']['value'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/144:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():
    mask = temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200']
    plt.hist(temps_season[mask]['value'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/145:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():
    mask = temps_season['season'] == season & temps_season['parameter'] == 'temperature_air_mean_200'
    plt.hist(temps_season[mask]['value'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/146:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value')
63/147:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season['date'].dt.month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
63/148:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value')
temps
63/149:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()
temps
63/150:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()
63/151:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season['date'].dt.month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
63/152:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season[mask]['value'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/153:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season['temperature_air_mean_200'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/154:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season['temperature_air_mean_200'], alpha=0.5, label=season, bins=100)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/155:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season['temperature_air_mean_200'], alpha=0.5, label=season, bins=20)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/156:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/157:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5, bins = 100, density = True)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/158:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()
# Convert Kelvin to Celsius
temps['temperature_air_mean_200'] = temps['temperature_air_mean_200'] - 273.15
temps['temperature_air_max_200'] = temps['temperature_air_max_200'] - 273.15
temps['temperature_air_min_200'] = temps['temperature_air_min_200'] - 273.15
63/159:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season['date'].dt.month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
63/160:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5, bins = 100, density = True)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
63/161:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps = temps.set_index('date')
temps
63/162:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps = temps.set_index('date')
temps
63/163:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()
# Convert Kelvin to Celsius
temps['temperature_air_mean_200'] = temps['temperature_air_mean_200'] - 273.15
temps['temperature_air_max_200'] = temps['temperature_air_max_200'] - 273.15
temps['temperature_air_min_200'] = temps['temperature_air_min_200'] - 273.15
63/164:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition
63/165:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
temps_decomposition['T'].rolling(window = 365*10).mean().plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")
temps_decomposition['T'].rolling(window = 365*10).var().plot(figsize=(8,4), color="tab:red", title="Rolling variance over annual periods");
63/166:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
temps_decomposition['T'].rolling(window = 365*10).mean().plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")
temps_decomposition['T'].rolling(window = 365*10).var().plot(figsize=(8,4), color="tab:red", title="Rolling variance over annual periods");
63/167:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean().plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")
temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var().plot(figsize=(8,4), color="tab:red", title="Rolling variance over annual periods");
63/168:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean().plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")
#temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var().plot(figsize=(8,4), color="tab:red", title="Rolling variance over annual periods");
63/169:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean().plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")
temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var().plot(figsize=(8,4), color="tab:red", title="Rolling variance over annual periods")
63/170:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance
plt.figure(figsize=(12,6))
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var(), label='Rolling variance')
plt.title('Rolling mean and variance')
plt.legend()
plt.show()
63/171:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).mean(), label='Rolling mean')
plt.subplot(temps_decomposition['temperature_air_mean_200'].rolling(window = 365*10).var(), label='Rolling variance')
plt.title('Rolling mean and variance')
plt.legend()
plt.show()
63/172:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).std(), label = 'Rolling std')
plt.legend(loc='best')
plt.title('Temperature with rolling mean and standard deviation')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).std(), label = 'Rolling std')
plt.legend(loc='best')
plt.title('Temperature with rolling mean and standard deviation')
plt.show()
63/173:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).std(), label = 'Rolling std')
plt.legend(loc='best')
plt.title('Temperature with rolling mean and standard deviation')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).std(), label = 'Rolling std')
plt.legend(loc='best')
plt.title('Differenced Temperature with rolling mean and standard deviation')
plt.show()
63/174:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling std')
plt.legend(loc='best')
plt.title('Temperature with rolling mean and standard deviation')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).var(), label = 'Rolling std')
plt.legend(loc='best')
plt.title('Differenced Temperature with rolling mean and standard deviation')
plt.show()
63/175:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance')
plt.legend(loc='best')
plt.title('Temperature with rolling mean and variance')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna(), label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).mean(), label='Rolling mean')
plt.plot(temps_decomposition['temperature_air_mean_200'].diff().dropna().rolling(window=365).var(), label = 'Rolling variance')
plt.legend(loc='best')
plt.title('Differenced Temperature with rolling mean and variance')
plt.show()
63/176:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean')
plt.legend(loc='best')
plt.title('Temperature with rolling mean')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance')
plt.legend(loc='best')
plt.title('Rolling Variance')
plt.show()
63/177:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean (1Y)')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling mean (1M)')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=7).mean(), label='Rolling mean (1W)')

plt.legend(loc='best')
plt.title('Temperature with rolling mean')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance')
plt.legend(loc='best')
plt.title('Rolling Variance')
plt.show()
63/178:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean (1Y)')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling mean (1M)')

plt.legend(loc='best')
plt.title('Temperature with rolling mean')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling variance (1M)')

plt.legend(loc='best')
plt.title('Rolling Variance')
plt.show()
63/179:
decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')
trend = decompose_result.trend
seasonal = decompose_result.seasonal
residual = decompose_result.resid
### Visualise All Data
decompose_result.plot()
plt.show()
### Visualise 10 years
years_examine = 365*10
fig, axs = plt.subplots(3, figsize=(8,6))
fig.suptitle('Removed Trend and Seasonality')
axs[0].plot(trend[-years_examine:])
axs[1].plot(seasonal[-years_examine:])
axs[1].set_ylim([-10,10])
axs[2].plot(residual[-years_examine:])
axs[2].set_ylim([-15,15])
63/180:
decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')
trend = decompose_result.trend
seasonal = decompose_result.seasonal
residual = decompose_result.resid
### Visualise All Data
decompose_result.plot()
plt.show()
### Visualise 10 years
years_examine = 365*10
fig, axs = plt.subplots(3, figsize=(8,6))
fig.suptitle('Removed Trend and Seasonality')
axs[0].plot(trend[-years_examine:])
axs[1].plot(seasonal[-years_examine:])
axs[1].set_ylim([-10,10])
axs[2].plot(residual[-years_examine:])
63/181:
decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')
trend = decompose_result.trend
seasonal = decompose_result.seasonal
residual = decompose_result.resid
### Visualise All Data
decompose_result.plot()
plt.show()
63/182:
decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')
trend = decompose_result.trend
seasonal = decompose_result.seasonal
residual = decompose_result.resid
### Visualise All Data
decompose_result.plot()
### Visualise 10 years
years_examine = 365*10
fig, axs = plt.subplots(3, figsize=(8,6))
fig.suptitle('Removed Trend and Seasonality')
axs[0].plot(trend[-years_examine:])
axs[1].plot(seasonal[-years_examine:])
axs[1].set_ylim([-10,10])
axs[2].plot(residual[-years_examine:])
63/183:
dftest = adfuller(residual, autolag = 'AIC')
print("1. ADF : ",dftest[0])
print("2. P-Value : ", dftest[1])
print("3. Num Of Lags : ", dftest[2])
print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
print("5. Critical Values :")
for key, val in dftest[4].items():
    print("\t",key, ": ", val)
63/184:
plot_acf(residual, lags=100)
plt.show()
63/185:
plot_acf(residual, lags=30)
plt.show()
63/186:
plot_pacf(residual, lags=40)
plt.show()
63/187:
residuals = residual.copy(deep=True)
residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')
mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)
aic = []
for key, val in mod.aic.items():
    if key != 0:
        aic.append((key[-1], val))
aic.sort()    
x,y = [x for x,y in aic],[y for x,y in aic]
plt.scatter(x, y)
plt.plot([0,40],[y[15],y[15]], 'tab:orange')
plt.text(3,y[15]+0.002, '{0}'.format(round(y[15],3)),color='tab:orange')
plt.plot([0,40],[y[20],y[20]], 'k--')
plt.text(3,y[20]-0.004, '{0}'.format(round(y[20],3)))
plt.title("AIC Criterion")
plt.xlabel("Lags in AR Model")
plt.ylabel("AIC")
plt.show()
63/188:
residuals = residual.copy(deep=True)
residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')
mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)
aic = []
for key, val in mod.aic.items():
    if key != 0:
        aic.append((key[-1], val))
aic.sort()    
x,y = [x for x,y in aic],[y for x,y in aic]
plt.scatter(x, y)
plt.plot([0,40],[y[15],y[15]], 'tab:orange')
plt.text(3,y[15]+0.002, '{0}'.format(round(y[15],3)),color='tab:orange')
plt.plot([0,40],[y[20],y[20]], 'k--')
plt.text(3,y[20]-0.004, '{0}'.format(round(y[20],3)))
plt.title("AIC Criterion")
plt.xlabel("Lags in AR Model")
plt.ylabel("AIC")
plt.show()

# Look at the AIC criterion and choose the lag with the lowest AIC
print(aic)
63/189:
residuals = residual.copy(deep=True)
residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')
mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)
aic = []
for key, val in mod.aic.items():
    if key != 0:
        aic.append((key[-1], val))
aic.sort()    
x,y = [x for x,y in aic],[y for x,y in aic]
plt.scatter(x, y)
plt.title("AIC Criterion")
plt.xlabel("Lags in AR Model")
plt.ylabel("AIC")
plt.show()

# Look at the AIC criterion and choose the lag with the lowest AIC
print(aic)
# Choose the lag with the lowest AIC, get the minimum AIC and the corresponding lag
63/190:
residuals = residual.copy(deep=True)
residuals.index = pd.DatetimeIndex(residuals.index).to_period('D')
mod = ar_select_order(residuals, maxlag=40, ic='aic', old_names=True)
aic = []
for key, val in mod.aic.items():
    if key != 0:
        aic.append((key[-1], val))
aic.sort()    
x,y = [x for x,y in aic],[y for x,y in aic]
plt.scatter(x, y)
plt.title("AIC Criterion")
plt.xlabel("Lags in AR Model")
plt.ylabel("AIC")
plt.show()

# Look at the AIC criterion and choose the lag with the lowest AIC
print(aic)
# Choose the lag with the lowest AIC, get the minimum AIC and the corresponding lag
min_aic = min(aic, key=lambda x: x[1])
print(min_aic)
63/191:
model = AutoReg(residuals, lags=4, old_names=True,trend='n')
model_fit  = model.fit()
coef = model_fit.params
res = model_fit.resid
res.index = res.index.to_timestamp()
print(model_fit.summary())
fig, axs = plt.subplots(2,2, figsize=(8,6))
fig.suptitle('Residuals after AR(4) model')
axs[0,0].plot(res)
axs[1,0].plot(res[-200:])
plot_acf(res, lags=20, ax=axs[0,1])
plot_pacf(res, lags=20, ax=axs[1,1])
plt.show()
from scipy.stats import norm
qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')
plt.show()
63/192:
model = AutoReg(residuals, lags=4, old_names=True,trend='n')
model_fit  = model.fit()
coef = model_fit.params
res = model_fit.resid
res.index = res.index.to_timestamp()
print(model_fit.summary())
fig, axs = plt.subplots(2,2, figsize=(8,6))
fig.suptitle('Residuals after AR(4) model')
axs[0,0].plot(res)
axs[1,0].plot(res[-200:])
plot_acf(res, lags=20, ax=axs[0,1])
plot_pacf(res, lags=20, ax=axs[1,1])
plt.show()
from scipy.stats import norm
qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')
plt.show()
63/193:
model = AutoReg(residuals, lags=4, old_names=True,trend='n')
model_fit  = model.fit()
coef = model_fit.params
res = model_fit.resid
res.index = res.index.to_timestamp()
print(model_fit.summary())
fig, axs = plt.subplots(2,2, figsize=(8,6))
fig.suptitle('Residuals after AR(4) model')
# Show residuals
axs[0,0].plot(res)
plot_acf(res, lags=20, ax=axs[0,1])
plot_pacf(res, lags=20, ax=axs[1,1])
plt.show()
from scipy.stats import norm
qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')
plt.show()
63/194:
model = AutoReg(residuals, lags=4, old_names=True,trend='n')
model_fit  = model.fit()
coef = model_fit.params
res = model_fit.resid
res.index = res.index.to_timestamp()
print(model_fit.summary())
fig, axs = plt.subplots(2,2, figsize=(8,6))
fig.suptitle('Residuals after AR(4) model')
# Show residuals
axs[0,0].plot(res) 
axs[0,0].set_title('Residuals')
# Show residuals histogram
axs[0,1].hist(res, bins=100)
plot_acf(res, lags=20, ax=axs[0,1])
plot_pacf(res, lags=20, ax=axs[1,1])
plt.show()
from scipy.stats import norm
qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')
plt.show()
63/195:
model = AutoReg(residuals, lags=4, old_names=True,trend='n')
model_fit  = model.fit()
coef = model_fit.params
res = model_fit.resid
res.index = res.index.to_timestamp()
print(model_fit.summary())
fig, axs = plt.subplots(2,2, figsize=(8,6))
fig.suptitle('Residuals after AR(4) model')
# Show residuals
axs[0,0].plot(res) 
axs[0,0].set_title('Residuals')
# Show residuals histogram
axs[1,0].hist(res, bins=100)
plot_acf(res, lags=20, ax=axs[0,1])
plot_pacf(res, lags=20, ax=axs[1,1])
plt.show()
from scipy.stats import norm
qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')
plt.show()
63/196:
model = AutoReg(residuals, lags=4, old_names=True,trend='n')
model_fit  = model.fit()
coef = model_fit.params
res = model_fit.resid
res.index = res.index.to_timestamp()
print(model_fit.summary())
fig, axs = plt.subplots(2,2, figsize=(8,6))
fig.suptitle('Residuals after AR(4) model')
# Show residuals (date vs residuals)
axs[0,0].plot(res , label='Residuals')
axs[0,0].set_title('Residuals')
# Show residuals histogram
axs[1,0].hist(res, bins=100)
plot_acf(res, lags=20, ax=axs[0,1])
plot_pacf(res, lags=20, ax=axs[1,1])
plt.show()
from scipy.stats import norm
qqplot(res, marker='x', dist=norm, loc=0, scale=np.std(res), line='45')
plt.show()
63/197:
from scipy import signal

def apply_convolution(x, window):
    conv = np.repeat([0., 1., 0.], window)
    filtered = signal.convolve(x, conv, mode='same') / window
    return filtered
denoised = temps.apply(lambda x: apply_convolution(x, 90))
denoised['T'][-lookback:-100].plot(figsize=(12,6))
plt.ylabel('Temperature (deg C)')
plt.show()
63/198:
from scipy import signal
lookback = 365*5
def apply_convolution(x, window):
    conv = np.repeat([0., 1., 0.], window)
    filtered = signal.convolve(x, conv, mode='same') / window
    return filtered
denoised = temps.apply(lambda x: apply_convolution(x, 90))
denoised['temperature_air_mean_200'][-lookback:-100].plot(figsize=(12,6))
plt.ylabel('Temperature (deg C)')
plt.show()
63/199:
from scipy import signal
lookback = 365*5
def apply_convolution(x, window):
    conv = np.repeat([0., 1., 0.], window)
    filtered = signal.convolve(x, conv, mode='same') / window
    return filtered
denoised = temps_decomposition.apply(lambda x: apply_convolution(x, 90))
denoised['temperature_air_mean_200'][-lookback:-100].plot(figsize=(12,6))
plt.ylabel('Temperature (deg C)')
plt.show()
63/200:
denoised['MA'] = denoised['temperature_air_mean_200'].rolling(window = lookback).mean()
plt.ylabel('Temperature (deg C)')
denoised['MA'].plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")
63/201:
denoised['MA'] = denoised['temperature_air_mean_200'].rolling(window = lookback).mean()
plt.ylabel('Temperature (deg C)')
denoised['MA'].plot(figsize=(8,4), color="tab:red", title="Rolling mean over annual periods")

denoised['S'] = denoised['temperature_air_mean_200'] - denoised['MA']
denoised['S'][-lookback:-100].plot(figsize=(12,6))
plt.ylabel('Temperature (deg C)')
plt.show()
63/202:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps['T'].copy(deep=True)
temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/203:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/204:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.rename(columns  = {'temperature_air_mean_200':'T'})
temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/205:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.rename({'temperature_air_mean_200':'T'})
temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/206:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
# Rename columns
temp_t = temp_t.rename({'temperature_air_mean_200':'T'})
temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/207:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
# Rename columns
temp_t = temp_t.rename(columns={'temperature_air_mean_200': 'T'})
temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/208:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
# Rename columns
print(temp_t.head())




temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/209:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition[['temperature_air_mean_200']].copy(deep=True)
# Rename columns
print(temp_t.head())




temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/210:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition[['temperature_air_mean_200']].copy(deep=True)
# Rename columns
temp_t.columns = ['T']




temp_t = temp_t.to_frame()
def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/211:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/212:
temp_t = temps['T'].copy(deep=True)
temp_t = temp_t.to_frame()
def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365.25 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/213:
def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365.25 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/214:
def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365.25 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/215:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']
def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365.25 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/216:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365.25 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/217:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/218:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365.25 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/219:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/220:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/221:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred

if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/222:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model(x, params):
    a,b,a1,b1 = params
    omega = 2*np.pi/365 #365.25
    theta = np.arctan(a1/b1)
    alpha = np.sqrt( a1**2 + b1**2)   
    print('Parameters:\n     a {0:0.3}\n     b {1:0.3}\n alpha {2:0.3}\n theta {3:0.3}'.format(a,b,alpha,theta))
    y_pred = a + b*x + alpha*np.sin(omega*x + theta)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred

if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t.plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/223:
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)

if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params_all, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
    
temp_t['model'] = model(temp_t.index-first_ord, params_all)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
    
temp_t[-2000:].plot(figsize=(18,6), style=['s','^-','k-'] , markersize=4, linewidth=3 )
63/224:
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t['res'] = temp_t['T']-temp_t['model']
temp_t['res'][-5000:].plot(figsize=(12,6))
plt.show()
fig, axs = plt.subplots(2,2, figsize=(12,8))
fig.suptitle('Residuals after de-trending and removing seasonality from the DAT')
axs[0,0].plot(temp_t['res'])
axs[1,0].plot(temp_t['res'][-2000:])
plot_acf(temp_t['res'], lags=40, ax=axs[0,1])
plot_pacf(temp_t['res'], lags=40, ax=axs[1,1])
plt.show()
63/225:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
63/226:
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t['res'] = temp_t['T']-temp_t['model']
temp_t['res'][-5000:].plot(figsize=(12,6))
plt.show()
fig, axs = plt.subplots(2,2, figsize=(12,8))
fig.suptitle('Residuals after de-trending and removing seasonality from the DAT')
axs[0,0].plot(temp_t['res'])
axs[1,0].plot(temp_t['res'][-2000:])
plot_acf(temp_t['res'], lags=40, ax=axs[0,1])
plot_pacf(temp_t['res'], lags=40, ax=axs[1,1])
plt.show()
63/227:
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t['res'] = temp_t['T']-temp_t['model_fit']
temp_t['res'][-5000:].plot(figsize=(12,6))
plt.show()
fig, axs = plt.subplots(2,2, figsize=(12,8))
fig.suptitle('Residuals after de-trending and removing seasonality from the DAT')
axs[0,0].plot(temp_t['res'])
axs[1,0].plot(temp_t['res'][-2000:])
plot_acf(temp_t['res'], lags=40, ax=axs[0,1])
plot_pacf(temp_t['res'], lags=40, ax=axs[1,1])
plt.show()
63/228:
import scipy.stats as stats
stats.probplot(temp_t['res'], dist="norm", plot=plt)
plt.title("Probability Plot model")
plt.show()
mu, std = norm.fit(temp_t['res'])
z = (temp_t['res'] - mu)/std
plt.hist(temp_t['res'], density=True, alpha=0.6, bins=100, label='Temp Error')
xmin, xmax = plt.xlim()
ymin, ymax = plt.ylim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mu, std)
data = np.random.randn(100000)
plt.plot(x, p, 'k', linewidth=2, label='Normal Dist')
plt.plot([std*2,std*2],[0,ymax])
print('P(Z > 2): {:0.3}% vs Normal Distibution: {:0.3}% '.format(len(z[z >= 2])/len(z)*100, (1-norm.cdf(2))*100))
print('SKEW    : {:0.3}'.format(stats.skew(z)))
print('KURTOSIS: {:0.3}'.format(stats.kurtosis(z)+3))
plt.ylabel('Density')
plt.xlabel('Temperature Error')
plt.legend()
plt.show()
63/229:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')
lookup_table_fips.columns = ['fips', 'iso', 'fips_name']
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()

# Get variables from inventory table
inventory = pd.read_csv("https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt", sep='\s+', header=None)
print(inventory.shape)


# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/230:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')
lookup_table_fips.columns = ['fips', 'iso', 'fips_name']
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/231:
# Get variables from inventory table
inventory = pd.read_csv("https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt", sep='\s+', header=None)
print(inventory.shape)
63/232:
# Merge with stations
print(inventory.columns)
63/233:
# Merge with stations
inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']
print(inventory)
#stations = stations.merge(inventory, on='station_id', how='left')
63/234:
# Merge with stations
inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']
# Put variables in JSON format, grouped by station_id
inventory_2 = inventory.groupby('station_id')['variable'].apply(list).reset_index()
print(inventory_2)
#stations = stations.merge(inventory, on='station_id', how='left')
63/235:
# Merge with stations
inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']
# Put variables in JSON format, grouped by station_id
inventory = inventory.groupby('station_id')['variable'].apply(list).reset_index()
stations = stations.merge(inventory, on='station_id', how='left')
63/236:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')
lookup_table_fips.columns = ['fips', 'iso', 'fips_name']
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
63/237:
# Merge with stations
inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']
# Put variables in JSON format, grouped by station_id
inventory = inventory.groupby('station_id')['variable'].apply(list).reset_index()
stations = stations.merge(inventory, on='station_id', how='left')
63/238:
# Get variables from inventory table
inventory = pd.read_csv("https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt", sep='\s+', header=None)
print(inventory.shape)
63/239:
# Merge with stations
inventory.columns = ['station_id', 'latitude', 'longitude', 'variable', 'first_year', 'last_year']
# Put variables in JSON format, grouped by station_id
inventory = inventory.groupby('station_id')['variable'].apply(list).reset_index()
stations = stations.merge(inventory, on='station_id', how='left')
63/240:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/241:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[0:10, -1]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/242:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        # Display columns 1:10 and last column
        display_cols = [1:10] + [-1]
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/243:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        # Display columns 1:10 and last column
        display_cols = [1:10,-1]
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/244:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        # Display columns 1:10 and last column
        display_cols = [1:10,len(stations.columns)-1]
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/245:
# Only keep the stations that are in Europe.
stations_eu = stations[stations['region'].isin(['Europe'])]
# Remove Russia
stations_eu = stations_eu[stations_eu['alpha-2'] != 'RU']
#print(stations_eu.head())

def plot_stations(stations):
    import folium
    from folium.plugins import MarkerCluster
    import pandas as pd
    import numpy as np
    # Create a figure to place the map in 
    fig = folium.Figure(width=800, height=600)
    # Create a map
    m = folium.Map(location=[50, 10], zoom_start=4, tiles='cartodbpositron', scrollWheelZoom=False)
    
    # Create a marker cluster
    marker_cluster = MarkerCluster().add_to(m)

    # Add markers to the map
    for index, row in stations.iterrows():
        # Join (bold) columns with values to display label
        # Display columns 1:10 and last column
        display_cols = [len(stations.columns)-1] + list(range(1,10))
        label = '<br>'.join(['<b>{}</b>: {}'.format(col, row[col]) for col in stations.columns[display_cols]])
        folium.Marker(location=[row['latitude'], row['longitude']], popup = label).add_to(marker_cluster)

    # Display the map
    m.add_to(fig)
    return fig

plot_stations(stations_eu)
63/246:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=NoaaGhcnParameter.DAILY.WIND_SPEED,
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('SCHIPHOL')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/247:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('SCHIPHOL')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
63/248:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('SCHIPHOL')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)

# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at Schiphol airport')
plt.legend()
plt.show()
63/249:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('SCHIPHOL')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
print(df)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value')
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at Schiphol airport')
plt.legend()
plt.show()
63/250:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('SCHIPHOL')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)

# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at Schiphol airport')
plt.legend()
plt.show()
63/251:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200,NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at Schiphol airport')
plt.legend()
plt.show()
63/252:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()
65/1:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/School/Assignments/portm/RET.csv")
print(df.head())
65/2:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
print(df.head())
65/3:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
df
65/4:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
df
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
65/5:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
df
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
df
65/6:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
# Set the index to the date column
df = df.set_index(pd.to_datetime(df['Date'], format='%Y%m%d'))
print(df)
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
df
65/7:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
# Set the index to the date column
df = df.set_index(pd.to_datetime(df['date'], format='%Y%m%d'))
print(df)
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
df
65/8:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
# Set the index to the date column
print(df)
df = df.set_index(pd.to_datetime(df['date'], format='%Y%m%d'))
print(df)
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
df
65/9:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
# Set the index to the date column
df = df.set_index(pd.to_datetime(df['DATE'], format='%Y%m%d'))
print(df)
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
df
65/10:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
# Set the index to the date column
df = df.set_index(pd.to_datetime(df['DATE'], format='%Y-%m-%d'))
print(df)
# Sample 50 assets (columns) randomly, also include the date column
df = df.sample(n=50, axis=1)
df
65/11:
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Nicky/Documents/School/Assignments/portm/RET.csv")
# Set the index to the date column
df = df.set_index(pd.to_datetime(df['DATE'], format='%Y-%m-%d'))
# Sample 50 assets (columns) randomly
df = df.sample(n=50, axis=1)
df
63/253:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()
63/254:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()
66/1:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()
66/2:
import pandas as pd
import numpy as np
from wetterdienst import Wetterdienst
import wetterdienst as wd
# What is available for us
Wetterdienst.discover()
66/3:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_max_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()
66/4:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Max temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()
66/5:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset)')
plt.legend()
plt.show()
66/6:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 2)')
plt.legend()
plt.show()
66/7:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('MALTA')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 2)')
plt.legend()
plt.show()
66/8:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('Berlin')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 2)')
plt.legend()
plt.show()
66/9:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('HEATHROW')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 2)')
plt.legend()
plt.show()
66/10:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('HEATHROW')[0]

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []

    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 2)')
plt.legend()
plt.show()
66/11:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('MALIN HEAD')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []
    
    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De Bilt (subset 2)')
plt.legend()
plt.show()
66/12:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('MALIN HEAD')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []
    
    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De MALIN HEAD')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De MALIN HEAD (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De MALIN HEAD (subset 2)')
plt.legend()
plt.show()
67/1:
import pandas as pd
import numpy as np
from wetterdienst import Wetterdienst
import wetterdienst as wd
# What is available for us
Wetterdienst.discover()
67/2:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('MALIN HEAD')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []
    
    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De MALIN HEAD')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De MALIN HEAD (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De MALIN HEAD (subset 2)')
plt.legend()
plt.show()
67/3:
# Select a station and load the data for the last 10 years
from wetterdienst.provider.noaa.ghcn.api import NoaaGhcnRequest, NoaaGhcnParameter
import datetime as dt

stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200],
    start_date=dt.datetime(2015, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')

print(stations_object)
def get_data_from_stations_request(
    stations_object: NoaaGhcnRequest,
) -> pd.DataFrame:
    """
    Takes a stations request object and process queries

    Args:
        stations_object: DwdObservationRequest object that holds all required information
        for downloading opendata dwd data

    Returns:
        DataFrame with content from DwdObservationRequest

    """
    observation_data = []
    
    for result in stations_object.values.query():
        observation_data.append(result.df)

    return pd.concat(observation_data)

df = get_data_from_stations_request(stations_object)
# Pivot the data based on parameter
df = df.pivot(index='date', columns='parameter', values='value').reset_index()
print(df)
# Plot the data
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(df['date'], df['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De BILT')
plt.legend()
plt.show()

# Select subset as subplot 
subset = df[1:365]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De BILT (subset 1)')
plt.legend()
plt.show()


# Select different as subplot 
subset = df[1000:1500]
plt.figure(figsize=(12,6))
plt.plot(subset['date'], subset['temperature_air_mean_200'], label='Avg temp')
#plt.plot(df['date'], df['temperature_air_min_200'], label='Min temp')
plt.title('Temperature at De BILT (subset 2)')
plt.legend()
plt.show()
67/4:
# Get historical temperature data.
from wetterdienst import Settings
from wetterdienst.provider.dwd.observation import DwdObservationRequest, DwdObservationParameter


# request = DwdObservationRequest(
#    parameter=["climate_summary"],
#    resolution="daily",
#    start_date="2000-01-01",  # if not given timezone defaulted to UTC
#    end_date="2022-01-01",  # if not given timezone defaulted to UTC
# ).filter_by_station_id(station_id=(4411))

# df = get_data_from_stations_request(request)

request = stations_object = NoaaGhcnRequest(
    parameter=[NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MEAN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MIN_200, NoaaGhcnParameter.DAILY.TEMPERATURE_AIR_MAX_200],
    start_date=dt.datetime(2000, 1, 1),
    end_date=dt.datetime(2022, 1, 1)
).filter_by_name('DE BILT')


df = get_data_from_stations_request(request)
67/5:
#print(df.head())
print(df['parameter'].unique())
# Plot the temperature data
temps = df[df['parameter'].isin(['temperature_air_mean_200', 'temperature_air_max_200', 'temperature_air_min_200'])]
print(temps.head())
# Plot the temperature data , with different colors for each parameter
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
for parameter in temps['parameter'].unique():
    plt.plot(temps[temps['parameter'] == parameter]['date'], temps[temps['parameter'] == parameter]['value'], label=parameter)
plt.title('Temperature')
plt.ylabel('Temperature (K)')
plt.legend()
plt.show()

# Widen the dataframe to have one column per parameter
temps = temps.pivot(index='date', columns='parameter', values='value').reset_index()
# Convert Kelvin to Celsius
temps['temperature_air_mean_200'] = temps['temperature_air_mean_200'] - 273.15
temps['temperature_air_max_200'] = temps['temperature_air_max_200'] - 273.15
temps['temperature_air_min_200'] = temps['temperature_air_min_200'] - 273.15
67/6:
winter_months = [10, 11, 12, 1, 2, 3]
summer_months = [4, 5, 6, 7, 8, 9]

temps_season = temps.copy(deep=True)
temps_season['month'] = temps_season['date'].dt.month
temps_season['season'] = temps_season['month'].apply(lambda x: 'winter' if x in winter_months else 'summer')
temps_season
67/7:
# Temperature distributions in histogram
plt.figure(figsize=(12,6))
for season in temps_season['season'].unique():    
    plt.hist(temps_season[temps_season['season'] == season]['temperature_air_mean_200'], label=season, alpha=0.5, bins = 100, density = True)
plt.title('Temperature distribution')
plt.ylabel('Frequency')
plt.legend()
plt.show()
67/8:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = temps.set_index('date')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots)
plt.figure(figsize=(12,6))
plt.subplot(211)
plt.plot(temps_decomposition['temperature_air_mean_200'], label='Original')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).mean(), label='Rolling mean (1Y)')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling mean (1M)')

plt.legend(loc='best')
plt.title('Temperature with rolling mean')
plt.subplot(212)
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
plt.plot(temps_decomposition['temperature_air_mean_200'].rolling(window=30).mean(), label='Rolling variance (1M)')

plt.legend(loc='best')
plt.title('Rolling Variance')
plt.show()
67/9:
decompose_result = seasonal_decompose(temps['temperature_air_mean_200'], model='additive', period=int(365), extrapolate_trend='freq')
trend = decompose_result.trend
seasonal = decompose_result.seasonal
residual = decompose_result.resid
### Visualise All Data
decompose_result.plot()
### Visualise 10 years
years_examine = 365*10
fig, axs = plt.subplots(3, figsize=(8,6))
fig.suptitle('Removed Trend and Seasonality')
axs[0].plot(trend[-years_examine:])
axs[1].plot(seasonal[-years_examine:])
axs[1].set_ylim([-10,10])
axs[2].plot(residual[-years_examine:])
67/10:
from scipy.stats import norm
from scipy.optimize import curve_fit
temp_t = temps_decomposition['temperature_air_mean_200'].copy(deep=True)
temp_t = temp_t.to_frame()
# Rename the series to 'T'
temp_t.columns = ['T']

def model_fit_general(x, a, b, a1, b1, theta, phi):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x + theta) + b1*np.sin(omega*x + phi)
    return y_pred
def model_fit(x, a, b, a1, b1):
    omega = 2*np.pi/365 #365.25
    y_pred = a + b*x + a1*np.cos(omega*x) + b1*np.sin(omega*x)
    return y_pred
def RSS(y, y_pred):
    return np.sqrt( (y - y_pred)**2 ).sum()
if isinstance(temp_t.index , pd.DatetimeIndex):
    first_ord = temp_t.index.map(dt.datetime.toordinal)[0]
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
    
params, cov = curve_fit(model_fit, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov))
for name, p, sd in zip( param_list, params, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
    
temp_t['model_fit'] = model_fit(temp_t.index-first_ord, *params)
if isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.toordinal)
params1, cov1 = curve_fit(model_fit_general, xdata = temp_t.index-first_ord, ydata = temp_t['T'], method='lm')
param_list = ['a', 'b', 'a1', 'b1', 'theta', 'phi']
print('\n Model 1 \n') 
std_dev = np.sqrt(np.diag(cov1))
for name, p, sd in zip( param_list, params1, std_dev):
    print('{0} :  {1:0.3}  CI ~normally [{2:0.2e},{3:0.2e}]'.format(name, p, p-1.96*sd,p+1.96*sd))
temp_t['model_fit_general'] = model_fit_general(temp_t.index-first_ord, *params1)
if not isinstance(temp_t.index , pd.DatetimeIndex):
    temp_t.index=temp_t.index.map(dt.datetime.fromordinal)
temp_t[:2000].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
temp_t[-2000:].plot(figsize=(12,4), style=['s','^-','k--'] , markersize=4, linewidth=2 )
RSS(temp_t['T'], temp_t['model_fit_general'])
print('Residual Sum of Squares (RSS)')
print('  RSS model generalised:', round(RSS(temp_t['T'], temp_t['model_fit_general']),2))
print('  RSS model sine curve :', round(RSS(temp_t['T'], temp_t['model_fit']),2))
68/1:
import pandas as pd
import numpy as np
from wetterdienst import Wetterdienst
import wetterdienst as wd
# What is available for us
Wetterdienst.discover()
68/2:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')
lookup_table_fips.columns = ['fips', 'iso', 'fips_name']
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
69/1:
import pandas as pd
import numpy as np
from wetterdienst import Wetterdienst
import wetterdienst as wd
# What is available for us
Wetterdienst.discover()
69/2:
# We are interested in data from NOAA GHCN
API = Wetterdienst(provider='NOAA', network = 'GHCN')
stations = API._all(API) # Available on http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt
stations['country_code'] = stations['station_id'].str[0:2]

lookup_table_fips = pd.read_csv('https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv')
lookup_table_fips.columns = ['fips', 'iso', 'fips_name']
lookup_table = pd.read_csv("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")
# Rename name to country_name
lookup_table.rename(columns={'name': 'country_name'}, inplace=True)
# Join the two tables
stations = stations.merge(lookup_table_fips, left_on='country_code', right_on='fips', how='left')
stations = stations.merge(lookup_table, left_on='iso', right_on='alpha-2', how='left')
stations.head()
# What country codes did not join?
print(stations[stations['alpha-2'].isna()]['country_code'].unique())
69/3:
# Get variables from inventory table
inventory = pd.read_csv("https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt", sep='\s+', header=None)
print(inventory.shape)
70/1:

import pandas as pd
import seaborn as sns

iris = sns.load_dataset('iris')
df = iris.sample(n=10, random_state=1)
df
70/2:
def highlight_cells(val):
    color = 'yellow' if val == 5.1 else '#C6E2E9' # Pastel blue
    return 'background-color: {}'.format(color)

df.style.applymap(highlight_cells)
70/3:
df.style.format(formatter={"sepal_length": "{:.1f}", "sepal_width": "{:.1f}",
                           "petal_length": "{:.1f}", "petal_width": "{:.1f}"})

def highlight_cells(val):
    color = 'yellow' if val == 5.1 else '#C6E2E9' # Pastel blue
    return 'background-color: {}'.format(color)

df.style.applymap(highlight_cells)
70/4:
df.style.format(formatter={"sepal_length": "{:.1f}", "sepal_width": "{:.1f}",
                           "petal_length": "{:.1f}", "petal_width": "{:.1f}"})

def highlight_rows(row):
    value = row.loc['species']
    if value == 'versicolor':
        color = '#FFB3BA' # Red
    elif value == 'setosa':
        color = '#BAFFC9' # Green
    else:
        color = '#BAE1FF' # Blue
    return ['background-color: {}'.format(color) for r in row]

df.style.apply(highlight_rows, axis=1)
70/5:
df.style.format(formatter={"sepal_length": "{:.1f}", "sepal_width": "{:.1f}",
                           "petal_length": "{:.1f}", "petal_width": "{:.1f}"})

def highlight_rows(row):
    value = row.loc['species']
    if value == 'versicolor':
        color = '#FFB3BA' # Red
    elif value == 'setosa':
        color = '#BAFFC9' # Green
    else:
        color = '#BAE1FF' # Blue
    return ['background-color: {}'.format(color) for r in row]

df.style.apply(highlight_rows, axis=1)
def format_sepal(val):
    condition = (val >= 3.5) & (val <= 5.5)
    font_color = 'red' if condition else 'black'
    font_weight = 'bold' if condition else 'normal'
    return 'color: {}; font-weight: {}'.format(font_color, font_weight)

df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])
70/6:
# Df to html
df.to_html('df.html')
70/7:
# Render the dataframe as an HTML text
df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\
        .render()
70/8:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\
        .render()
        
# Directly link the <style> ... </style> tag to the HTML text
70/9:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\
        .render()
        
# Link the stylesheet to the HTML text
html_text = '<link rel="stylesheet" type="text/css" href="style.css">' + html_text
70/10:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\
        .render()
        
# Show the html text
html_text
70/11:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\
        .render()
        
# Show the html text
html_text.to_html_table_string()
70/12:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])\
        .render()
        
# Show the html text
html_text.to_html()
70/13:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])
        
# Show the html text
html_text.to_html()
70/14:
# Render the dataframe as an HTML text
html_text = df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])
        
# Show the html text
html_text.to_html_table_string()
70/15:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
70/16:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
html_text
70/17:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
html_text
# Show in html text
print(html_text.render())
70/18:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
html_text
# Show in html text
print(html_text.to_html())
70/19:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
html_text
# Show in html text
html_text.to_html()
70/20:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
html_text
# Print the style as a dictionary
html_text._compute().data
70/21:
# Render the dataframe as an HTML text
html_text = df.style.background_gradient()
html_text
# Print the style object as a dictionary
html_text._repr_html_()
70/22:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
style
70/23:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, based on the CSS selectors
style_dict = {}
for selector in style[style.find('{')+1:style.find('}')].split(';'):
    selector = selector.split(':')
    if len(selector) == 2:
        style_dict[selector[0].strip()] = selector[1].strip()
style_dict
70/24:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'}
style_dict = {}
for style_row in style[style.find('{')+1:style.find('}')].split(';'):
    if style_row.strip():
        style_dict[style_row.split(':')[0].strip()] = style_row.split(':')[1].strip()
        
# Make a new dataframe with the style dictionary
style_df = pd.DataFrame(style_dict.items(), columns=['id', 'style'])
style_df
70/25:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
style_dict = {}
for s in style.split('}'):
    if s:
        keys = s.split('{')[0]
        # different ids are seperated by ,
        keys = keys.split(',')
        value = s.split('{')[1]
        for key in keys:
            key = key.strip() 
            style_dict[key] = value
    
style_dict
70/26:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
style_dict = {}
for s in style.split('}'):
    if s:
        keys = s.split('{')[0]
        print(keys)
        # different ids are seperated by ,
        keys = keys.split(',')
        value = s.split('{')[1]
        for key in keys:
            key = key.strip() 
            style_dict[key] = value
    
style_dict
70/27:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
style_dict = {}
print(style)
for s in style.split('}'):
    if s:
        keys = s.split('{')[0]
        print(keys)
        # different ids are seperated by ,
        keys = keys.split(',')
        value = s.split('{')[1]
        for key in keys:
            key = key.strip() 
            style_dict[key] = value
    
style_dict
70/28:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
style_dict = {}
#print(style)
for s in style.split('}'):
    print(s)
    if s:
        keys = s.split('{')[0]
        print(keys)
        # different ids are seperated by ,
        keys = keys.split(',')
        value = s.split('{')[1]
        for key in keys:
            key = key.strip() 
            style_dict[key] = value
    
style_dict
70/29:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
def convert_css_to_dict(style_string):
    style_list = style_string.strip().split("\n")
    style_dict = {}
    for style in style_list:
        if "{" in style:
            selector = style.split("{")[0].strip()
            values = style.split("{")[1].split("}")[0].strip()
            style_dict[selector] = {}
            values = values.split(";")
            for value in values:
                if value:
                    prop = value.split(":")[0].strip()
                    val = value.split(":")[1].strip()
                    style_dict[selector][prop] = val
    return style_dict

style_dict = convert_css_to_dict(style)
style_dict
70/30:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
style
70/31:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
import re

def css_to_dict(style_string):
    styles = re.findall(r'#[\w_]+ {\n([\s\S]+?)\n}', style_string)
    css_dict = {}
    for style in styles:
        style_lines = style.strip().split('\n')
        selector = re.search(r'#[\w_]+', style_lines[0]).group()
        properties = {}
        for line in style_lines[1:]:
            key, value = line.strip().split(': ')
            properties[key] = value
        css_dict[selector] = properties
    return css_dict

style_dict = css_to_dict(style)
style_dict
70/32:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
style
70/33:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'
lines = style.strip().split("\n")
lines
70/34:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style> and </style> tags
style = style[7:-8]
style
70/35:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>')+1:style.rfind('<')]
style
70/36:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]
style
70/37:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
lines
70/38:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    # Split the line into the selector and the style
    selector = l[:l.find('{')]
    style_css = l[l.find('{')+1:l.find('}')]
    print(selector, style_css)
70/39:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    # Split the line into the selector and the style
    selector = l[:l.find('{')]
    style_css = l[l.find('{')+1:l.find('}')]
    print(selector)
70/40:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    print(l)
    # Split the line into the selector and the style
    selector = l[:l.find('{')]
    style_css = l[l.find('{')+1:l.find('}')]
70/41:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    print(l)
    # Split the line into the selector and the style
    selector = l[l.find('{'):]
    print(selector)
    style_css = l[l.find('{')+1:l.find('}')]
70/42:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    # Split the line into the selector and the style
    selector = l[l.find('{'):]
    print(selector)
    style_css = l[l.find('{')+1:l.find('}')]
70/43:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    # Split the line into the selector and the style
    selector = l[:l.find('{')]
    print(selector)
    style_css = l[l.find('{')+1:l.find('}')]
70/44:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for l in lines:
    # Split the line into the selector and the style
    selector = l[:l.find('{')]
    print(l)
    style_css = l[l.find('{')+1:l.find('}')]
70/45:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("\n")
for idx, l in enumerate(lines):
    # Split the line into the selector and the style
    selector = l[:l.find('{')]
    print(idx)
    print(l)
    style_css = l[l.find('{')+1:l.find('}')]
70/46:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("}")
for idx, l in enumerate(lines):
    print(idx)
    print(l)
70/47:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    print(selectors)
    print(idx)
    print(l)
70/48:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    print(selectors)
    print(idx)
    print(l)
70/49:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:]
    
    print(selectors)
    print(style)
    print(idx)
    print(l)
70/50:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:]
    
    print(selectors)
    print(style)
70/51:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:]
    
    print(selectors)
    print(_style)
70/52:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip()[1:] for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:]
    
    print(selectors)
    print(_style)
70/53:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:]
    
    print(selectors)
    print(_style)
70/54:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:]
    
    for s in selectors:
        style_dict[s] = _style
        
print(style_dict)
70/55:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
print(style_dict)
70/56:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

table
70/57:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(key, '<td id="{}" style="{}">'.format(key, value))

table
70/58:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(key, f'id="{key}"', value)

table
70/59:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(f'id="{key}"', value)

table
70/60:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(f'id="{key}"', value)

new_table
70/61:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(f'id="{key}"', value)
print(style_dict)
new_table
70/62:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(f'id="{key}"', value)
print(style_dict)
table
70/63:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    value_formatted = value.replace(':', '=')
    new_table = new_table.replace(f'id="{key}"', f'id="{key}" style="{value_formatted}"')
print(style_dict)
table
70/64:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    value_formatted = value.replace(':', '=')
    new_table = new_table.replace(f'id="{key}"', f'id="{key}" style="{value_formatted}"')
table
70/65:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    value_formatted = value.replace(':', '=')
    new_table = new_table.replace(f'id="{key}"', f'id="{key}" style="{value_formatted}"')
new_table
70/66:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    value_formatted = value.replace(':', '=')
    new_table = new_table.replace(f'id="{key}"', f'id="{key}" style="{value_formatted}"')

new_table.replace('\n', '')
70/67:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()
html_gr
# Show in html text
html_text = html_gr.to_html()
# Get the full <style> ... </style> block
style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
# Get the full <table> ... </table> block
table = html_text[html_text.find('<table'):html_text.find('</table>')+8]

# Convert the style to a dictionary, e.g. {'T_81cd3_row0_col0': 'background-color: #b1c2de; color: #000000;'} from '#T_81cd3_row0_col0 {background-color: #b1c2de; color: #000000;}'

# Drop the <style type="text/css"> and </style> tags
style = style[style.find('>\n')+1:style.rfind('\n<')]

# Remove all \n
style = style.replace('\n', '')
lines = style.strip().split("}")
style_dict = {}
for idx, l in enumerate(lines):
    # Find the selector(s), e.g. #T_81cd3_row0_col0
    selectors = l[:l.find('{')]
    # Seperate the selectors by ,
    selectors = selectors.split(',')
    # Trim whitespace and remove #
    selectors = [s.replace('#','').strip() for s in selectors]
    # Find the style, e.g. background-color: #b1c2de; color: #000000;
    _style = l[l.find('{')+1:].strip()
    
    for s in selectors:
        style_dict[s] = _style
        
#print(style_dict)

# Replace everything in table with id="T_81cd3_row0_col0" to <td id="T_81cd3_row0_col0" style="background-color: #b1c2de; color: #000000;">
new_table = table
for key, value in style_dict.items():
    new_table = new_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')

new_table.replace('\n', '')
71/1:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()

# Wrap in function for html styled df
def switch_style(df):
    html_text = df.to_html()
    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
    # Get the full <table> ... </table> block
    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
    
    # Drop the <style type="text/css"> and </style> tags
    html_style = html_style[html_style.find('>\n')+1:html_style.rfind('\n<')].replace('\n', '')
    html_table = html_table.replace('\n', '')
    
    # Iterate over style
    style_lines = html_style.strip().split("}")
    style_dict = {}
    for idx, l in enumerate(lines):
        # Find the selector(s), e.g. #T_81cd3_row0_col0
        selectors = l[:l.find('{')]
        # Seperate the selectors by ,
        selectors = selectors.split(',')
        # Trim whitespace and remove #
        selectors = [s.replace('#','').strip() for s in selectors]
        # Find the style, e.g. background-color: #b1c2de; color: #000000;
        _style = l[l.find('{')+1:].strip()
        for s in selectors:
            style_dict[s] = _style
            
    for key, value in style_dict.items():
        html_table = html_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')
        
    return html_table
    
    
switch_style(html_gr)
71/2:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()

# Wrap in function for html styled df
def switch_style(data):
    html_text = data.to_html()
    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
    # Get the full <table> ... </table> block
    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
    
    # Drop the <style type="text/css"> and </style> tags
    html_style = html_style[html_style.find('>\n')+1:html_style.rfind('\n<')].replace('\n', '')
    html_table = html_table.replace('\n', '')
    
    # Iterate over style
    style_lines = html_style.strip().split("}")
    style_dict = {}
    for idx, l in enumerate(lines):
        # Find the selector(s), e.g. #T_81cd3_row0_col0
        selectors = l[:l.find('{')]
        # Seperate the selectors by ,
        selectors = selectors.split(',')
        # Trim whitespace and remove #
        selectors = [s.replace('#','').strip() for s in selectors]
        # Find the style, e.g. background-color: #b1c2de; color: #000000;
        _style = l[l.find('{')+1:].strip()
        for s in selectors:
            style_dict[s] = _style
            
    for key, value in style_dict.items():
        html_table = html_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')
        
    return html_table
    
    
switch_style(html_gr)
71/3:

import pandas as pd
import seaborn as sns

iris = sns.load_dataset('iris')
df = iris.sample(n=10, random_state=1)
df
71/4:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()

# Wrap in function for html styled df
def switch_style(data):
    html_text = data.to_html()
    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
    # Get the full <table> ... </table> block
    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
    
    # Drop the <style type="text/css"> and </style> tags
    html_style = html_style[html_style.find('>\n')+1:html_style.rfind('\n<')].replace('\n', '')
    html_table = html_table.replace('\n', '')
    
    # Iterate over style
    style_lines = html_style.strip().split("}")
    style_dict = {}
    for idx, l in enumerate(lines):
        # Find the selector(s), e.g. #T_81cd3_row0_col0
        selectors = l[:l.find('{')]
        # Seperate the selectors by ,
        selectors = selectors.split(',')
        # Trim whitespace and remove #
        selectors = [s.replace('#','').strip() for s in selectors]
        # Find the style, e.g. background-color: #b1c2de; color: #000000;
        _style = l[l.find('{')+1:].strip()
        for s in selectors:
            style_dict[s] = _style
            
    for key, value in style_dict.items():
        html_table = html_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')
        
    return html_table
    
    
switch_style(html_gr)
71/5:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()

# Wrap in function for html styled df
def switch_style(data):
    html_text = data.to_html()
    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
    # Get the full <table> ... </table> block
    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
    
    # Drop the <style type="text/css"> and </style> tags
    html_style = html_style[html_style.find('>\n')+1:html_style.rfind('\n<')].replace('\n', '')
    html_table = html_table.replace('\n', '')
    
    # Iterate over style
    style_lines = html_style.strip().split("}")
    style_dict = {}
    for idx, l in enumerate(style_lines):
        # Find the selector(s), e.g. #T_81cd3_row0_col0
        selectors = l[:l.find('{')]
        # Seperate the selectors by ,
        selectors = selectors.split(',')
        # Trim whitespace and remove #
        selectors = [s.replace('#','').strip() for s in selectors]
        # Find the style, e.g. background-color: #b1c2de; color: #000000;
        _style = l[l.find('{')+1:].strip()
        for s in selectors:
            style_dict[s] = _style
            
    for key, value in style_dict.items():
        html_table = html_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')
        
    return html_table
    
    
switch_style(html_gr)
71/6:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()

# Wrap in function for html styled df
def switch_style(data):
    html_text = data.to_html()
    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
    # Get the full <table> ... </table> block
    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
    
    # Drop the <style type="text/css"> and </style> tags
    html_style = html_style[html_style.find('>\n')+1:html_style.rfind('\n<')].replace('\n', '')
    html_table = html_table.replace('\n', '')
    
    # Iterate over style
    style_lines = html_style.strip().split("}")
    style_dict = {}
    for idx, l in enumerate(style_lines):
        # Find the selector(s), e.g. #T_81cd3_row0_col0
        selectors = l[:l.find('{')]
        # Seperate the selectors by ,
        selectors = selectors.split(',')
        # Trim whitespace and remove #
        selectors = [s.replace('#','').strip() for s in selectors]
        # Find the style, e.g. background-color: #b1c2de; color: #000000;
        _style = l[l.find('{')+1:].strip()
        for s in selectors:
            style_dict[s] = _style
            
    for key, value in style_dict.items():
        html_table = html_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')
        
    return html_table
    
    
switch_style(html_gr)
71/7:

import pandas as pd
import seaborn as sns

iris = sns.load_dataset('iris')
df = iris.sample(n=10, random_state=1)
df
71/8:
df.style.format(formatter={"sepal_length": "{:.1f}", "sepal_width": "{:.1f}",
                           "petal_length": "{:.1f}", "petal_width": "{:.1f}"})

def highlight_rows(row):
    value = row.loc['species']
    if value == 'versicolor':
        color = '#FFB3BA' # Red
    elif value == 'setosa':
        color = '#BAFFC9' # Green
    else:
        color = '#BAE1FF' # Blue
    return ['background-color: {}'.format(color) for r in row]

df.style.apply(highlight_rows, axis=1)
def format_sepal(val):
    condition = (val >= 3.5) & (val <= 5.5)
    font_color = 'red' if condition else 'black'
    font_weight = 'bold' if condition else 'normal'
    return 'color: {}; font-weight: {}'.format(font_color, font_weight)

df.style.apply(highlight_rows, axis=1)\
        .applymap(format_sepal, subset=['sepal_length', 'sepal_width'])
71/9:
# Render the dataframe as an HTML text
html_gr = df.style.background_gradient()

# Wrap in function for html styled df
def switch_style(data):
    html_text = data.to_html()
    html_style = html_text[html_text.find('<style'):html_text.find('</style>')+8]
    # Get the full <table> ... </table> block
    html_table = html_text[html_text.find('<table'):html_text.find('</table>')+8]
    
    # Drop the <style type="text/css"> and </style> tags
    html_style = html_style[html_style.find('>\n')+1:html_style.rfind('\n<')].replace('\n', '')
    html_table = html_table.replace('\n', '')
    
    # Iterate over style
    style_lines = html_style.strip().split("}")
    style_dict = {}
    for idx, l in enumerate(style_lines):
        # Find the selector(s), e.g. #T_81cd3_row0_col0
        selectors = l[:l.find('{')]
        # Seperate the selectors by ,
        selectors = selectors.split(',')
        # Trim whitespace and remove #
        selectors = [s.replace('#','').strip() for s in selectors]
        # Find the style, e.g. background-color: #b1c2de; color: #000000;
        _style = l[l.find('{')+1:].strip()
        for s in selectors:
            style_dict[s] = _style
            
    for key, value in style_dict.items():
        html_table = html_table.replace(f'id="{key}"', f'id="{key}" style="{value}"')
        
    return html_table
    
    
switch_style(html_gr)
71/10:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
df = pd.DataFrame({'performance': np.random.randn(365), 'id': np.random.choice(['C-103', 'C-102', 'C-101'], 365), 'date': pd.date_range('1/1/2018', periods=365)})
df
71/11:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
# Let us create a dataframe with 3 columns: id, date and performance
# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals
df = pd.DataFrame({'id': ['C-103', 'C-102', 'C-101'], 'date': pd.date_range('2023-01-01', '2023-02-01', freq='H')})
# Create a column called performance with random numbers
df['performance'] = np.random.randint(0, 100, df.shape[0])

df
71/12:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
# Let us create a dataframe with 3 columns: id, date and performance
# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals
df = pd.DataFrame({'date': pd.date_range('2023-01-01', '2023-02-01', freq='H')})
# Create a column called performance with random numbers
df['performance'] = np.random.randint(0, 100, df.shape[0])

df
71/13:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
# Let us create a dataframe with 3 columns: id, date and performance
# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals
df = pd.DataFrame({'date': pd.date_range('2023-01-01', '2023-02-01', freq='H')})
# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values
df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])
# Create a column called performance with random numbers
df['performance'] = np.random.randint(65, 100, df.shape[0])

df
71/14:
# Summarize the dataframe by id and 12h performance, 24h performance, 7d performance and 1M perfromance
table_performance = df.groupby('id').resample('12H', on='date').performance.agg(['mean']).reset_index()
table_performance
71/15:
# Summarize the dataframe by id and most recent 12h performance, 24h performance, 7d performance and 1M perfromance
table_performance = df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(12).performance.mean()).reset_index().rename(columns={0: '12h'})
table_performance
71/16:
# Summarize the dataframe by id and most recent 12h performance, most recent 24h performance, most recent 7d performance and most recent 1M perfromance
table_performance = df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(12).performance.mean()).reset_index().rename(columns={0: '12h'})
table_performance = pd.merge(table_performance, df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(24).performance.mean()).reset_index().rename(columns={0: '24h'}), how='left', on='id')
table_performance = pd.merge(table_performance, df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(168).performance.mean()).reset_index().rename(columns={0: '7d'}), how='left', on='id')
table_performance = pd.merge(table_performance, df.groupby('id').apply(lambda x: x.sort_values('date', ascending=False).head(720).performance.mean()).reset_index().rename(columns={0: '1M'}), how='left', on='id')
table_performance
71/17:
# Summarize the dataframe by id and create an average 7-day rolling performance column
df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(7).mean()))
df
71/18:
# Summarize the dataframe by id and create an average 7-day rolling performance column
df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(7*24).mean()))
df
71/19:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
# Let us create a dataframe with 3 columns: id, date and performance
# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals
df = pd.DataFrame({'date': pd.date_range('2022-12-01', '2023-02-01', freq='H')})
# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values
df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])
# Create a column called performance with random numbers
df['performance'] = np.random.randint(65, 100, df.shape[0])

df
71/20:
# Summarize the dataframe by id and create an average 1-day rolling performance column
df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(24).mean()))
df

# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))
table_performance[table_performance.date == datetime.date.today()]
71/21:
# Summarize the dataframe by id and create an average 1-day rolling performance column
df = df.groupby('id').apply(lambda x: x.assign(rolling_performance=x.performance.rolling(24).mean()))
df

# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))
table_performance
71/22:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))
table_performance
71/23:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
# Let us create a dataframe with 3 columns: id, date and performance
# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals
df = pd.DataFrame({'date': pd.date_range('2022-12-01', '2023-02-01', freq='H')})
# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values
df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])
# Create a column called performance with random numbers
df['performance'] = np.random.randint(65, 100, df.shape[0])

df
71/24:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))
table_performance
71/25:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
71/26:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
table_performance
71/27:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(index='date', columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
71/28:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(index='date', columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance
71/29:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance
71/30:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance.style.background_gradient()
71/31:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance.style.background_gradient().reset_index()
71/32:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance.reset_index().style.background_gradient()
71/33:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance.reset_index(drop=True).style.background_gradient()
71/34:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(values='id', columns=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance.reset_index(drop=True).style.background_gradient()
71/35:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
71/36:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(columns='id', values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance
71/37:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance = table_performance.pivot(values=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'])
table_performance
71/38:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id and show the performance columns
table_performance
71/39:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]
# Pivot on id, show the id's as columns and the performance_columns as index.
table_performance = table_performance.pivot(index=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id')

table_performance
71/40:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id', columns='date', values='rolling_performance_12h', aggfunc='first').fillna('-')
71/41:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id', columns='date', values='rolling_performance_12h', aggfunc='first').fillna('-')
table_performance
71/42:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index=['date', 'rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id', values='performance').reset_index()

table_performance
71/43:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id', values='performance').reset_index()

table_performance
71/44:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index=['rolling_performance_12h', 'rolling_performance_24h', 'rolling_performance_7d', 'rolling_performance_30d'], columns='id', values='performance')

table_performance
71/45:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  columns='date',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])
table_performance
71/46:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])
table_performance
71/47:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))

# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])
table_performance.style.background_gradient()
71/48:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])
table_performance.style.background_gradient()
# In percentage
table_performance.style.format("{:.2%}")
71/49:

# Create fake timeseries data with one column called performance and one column called id with C-103, C-102 and C-101 as values. Also create a column called date with dates from 1/1/2018 to 1/1/2019.
import pandas as pd
import numpy as np
import datetime
# Let us create a dataframe with 3 columns: id, date and performance
# Time series go from 2023-01-01 to 2023-02-01 in 1 hour intervals
df = pd.DataFrame({'date': pd.date_range('2022-12-01', '2023-02-01', freq='H')})
# Duplicate the dataframe 3 times and add a column called id with C-103, C-102 and C-101 as values
df = pd.concat([df.assign(id='C-103'), df.assign(id='C-102'), df.assign(id='C-101')])
# Create a column called performance with random numbers
df['performance'] = np.random.randint(65, 100, df.shape[0]) / 100

df
71/50:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage
table_performance.style.format("{:.2%}")
table_performance.style.background_gradient()
71/51:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage
table_performance.style.format("{:.2%}%")
table_performance.style.background_gradient()
71/52:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format

table_performance.style.format("{:.2%}")
table_performance.style.background_gradient()
71/53:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format

table_performance.style.format("{:.3%}")
table_performance.style.background_gradient()
71/54:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format

table_performance.style.format("{:.2%}")
#table_performance.style.background_gradient()
71/55:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format

table_p = table_performance.style.format("{:.2%}")
table_p =  table_p.style.background_gradient()
table_p
71/56:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format

table_performance.style.background_gradient().format("{:.2%}")
71/57:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h
71/58:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
71/59:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
71/60:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h


# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/61:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')

# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/62:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')

# In percentage format
71/63:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')

# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/64:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
table_performance_24h.index = table_performance_24h.index.strftime('%H-%H')
# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
#tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
#tb_p
71/65:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx + 1).astype(str).str.zfill(2)
 
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
#tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
#tb_p
71/66:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (int(indx) + 1).astype(str).str.zfill(2)
 
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
#tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
#tb_p
71/67:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx..astype(int) + 1).astype(str).str.zfill(2)
 
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
#tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
#tb_p
71/68:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
 
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
#tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
#tb_p
71/69:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
 
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/70:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
 
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/71:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=23)]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/72:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24) & (df.date <= df.date.max())]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/73:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[df.date >= df.date.max() - datetime.timedelta(hours=24) & (df.date < df.date.max())]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/74:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[(df.date >= df.date.max() - datetime.timedelta(hours=24)) and (df.date < df.date.max())]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
71/75:
# Summarize the dataframe by id and generate 12h, 24h, 7d and 30d rolling performance columns. Then show it for today.
table_performance = df.groupby('id').apply(lambda x: x.assign(rolling_performance_12h=x.performance.rolling(12).mean(),
                                                             rolling_performance_24h=x.performance.rolling(24).mean(),
                                                             rolling_performance_7d=x.performance.rolling(24*7).mean(),
                                                             rolling_performance_30d=x.performance.rolling(24*30).mean()))



# Fetch the most recent
table_performance = table_performance[table_performance.date == table_performance.date.max()]

# Id needs to be columns, such that C-101, C-102 and C-103 are the column names
# On the index, we find 12h, 24h, 7d and 30d
table_performance = table_performance.pivot_table(index='id',
                                                  values=['rolling_performance_12h',
                                                          'rolling_performance_24h',
                                                          'rolling_performance_7d',
                                                          'rolling_performance_30d'])

# In percentage format
tb_p = table_performance.style.background_gradient().format("{:.2%}")


# Fetch the last 24 hours
table_performance_24h = df[(df.date >= df.date.max() - datetime.timedelta(hours=24)) & (df.date < df.date.max())]
# Show the performance for the last 24 hours, with index 00-01, 01-02, 02-03, etc.
# index name is the date
# columns are the ids
table_performance_24h = table_performance_24h.pivot_table(index='date',columns='id',values='performance')
# strftime 00-01
indx = table_performance_24h.index.strftime('%H')
indx = indx + '-' + (indx.astype(int) + 1).astype(str).str.zfill(2)
table_performance_24h.index = indx

# Name of the index is the date
table_performance_24h.index.name = df.date.max().strftime('%Y-%m-%d')
table_performance_24h
# In percentage format
tb_p = table_performance_24h.style.background_gradient().format("{:.2%}")
tb_p
73/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from .secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/2:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/3:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/4:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/5:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

with server:
    df = load_query('raw', query)

df
73/6:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/7:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

with server:
    df = load_query('raw', query)

df
73/8:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/9:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

with server:
    df = load_query('raw', query)

df
73/10:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

with server:
    df = load_query('raw', query)

df
73/11:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

with server.start():
    df = load_query('raw', query)

df
73/12:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/13:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/14:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/15:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/16:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/17:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/18:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder
print(DB_USER)
# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/19:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import REMOTE_HOST, REMOTE_USER, REMOTE_PASS, REMOTE_PORT, DB_HOST, DB_USER, DB_PASS, DB_PORT
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/20:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/21:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""user={user} dbname={database} password={password} host={host}"""
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/22:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/23:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user = {user}
    dbname = {database}
    password = {password}
    host = {host}
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
73/24:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
df = load_query('raw', query)
df
73/25:
server = SSHTunnelForwarder(
    (REMOTE_HOST, REMOTE_PORT),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
# Check if tunnel is open
print(server.local_bind_port)
    
df = load_query('raw', query)
df
73/26:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 25),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
# Check if tunnel is open
print(server.local_bind_port)
    
df = load_query('raw', query)
df
74/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user = {user}
    dbname = {database}
    password = {password}
    host = {host}
    """
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(DB_USER, DB_PASS, DB_HOST, database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/2:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
# Check if tunnel is open
print(server.local_bind_port)
    
df = load_query('raw', query)
df
74/3:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    """
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/4:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
74/5:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/6:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
74/7:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    port=5432
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/8:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
74/9:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password=bk7Iut6Me9O4gh4f6jSe
    host={host}
    port=5432
    """
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/10:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
74/11:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password="bk7Iut6Me9O4gh4f6jSe"
    host={host}
    port=5432
    """
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/12:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
74/13:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
74/14:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/2:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'grafana_reader'
DB_HOST = 'localhost'
DB_PASS = 'bk7Iut6Me9O4gh4f6jSe'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/3:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/4:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/5:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'nicky'
DB_HOST = 'localhost'
DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user={user}
    dbname={database}
    password={password}
    host={host}
    """
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/6:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/7:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'nicky'
DB_HOST = 'localhost'
DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user = {user}
    dbname = {database}
    password = {password}
    host = {host}
    """
    keepalive_kwargs = {
    "keepalives": 1,
    "keepalives_idle": 250,
    "keepalives_interval": 5,
    "keepalives_count": 5,
    }
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/8:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/9:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'nicky'
DB_HOST = 'localhost'
DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    DSN = f"""
    user = {user}
    dbname = {database}
    password='{password}'
    host = {host}
    """
    keepalive_kwargs = {
    "keepalives": 1,
    "keepalives_idle": 250,
    "keepalives_interval": 5,
    "keepalives_count": 5,
    }
    print(DSN)
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(DSN, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/10:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/11:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

REMOTE_HOST = '116.203.157.240'
REMOTE_USER = 'root'
REMOTE_PASS = 'q03Selwh9NCHWNMnvDhm'
REMOTE_PORT = 22

DB_USER = 'nicky'
DB_HOST = 'localhost'
DB_PASS = 'Ie3tq4GDjs29rHBJWE3y'
DB_PORT = 5432
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': 5432,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5,}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(database: str, query: str) -> pd.DataFrame:
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = database)
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/12:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('raw', query)
df
75/13:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
    
df = load_query('postgres', query)
df
75/14:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/15:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL) AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
df
75/16:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""

server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
df
75/17:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

print(df)
75/18:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],
    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']
}

w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_daily
75/19:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],
    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']
}

w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_daily
75/20:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],
    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/21:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'time_zone' : 'UTC',
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        print(data)
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/22:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],
    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/23:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        print(data)
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/24:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],
    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/25:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        print(data)
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/26:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : daily_weather_variables['temperature'] + daily_weather_variables['radiation'] + daily_weather_variables['wind'],
    'hourly' : hourly_weather_variables['temperature'] + hourly_weather_variables['wind'] + hourly_weather_variables['energy']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/27:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max','windspeed_10m_min', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/28:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/29:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ', '.join(hourly_params),
              'daily' : ', '.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        print(urllib.parse.urlencode(params, safe = ','))
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        print(data)
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/30:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/31:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        print(urllib.parse.urlencode(params, safe = ','))
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        print(data)
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/32:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        print(urllib.parse.urlencode(params, safe = ','))
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/33:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/34:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        print(urllib.parse.urlencode(params, safe = ','))
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        print(data)
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/35:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_80m','windspeed_120m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/36:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/37:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/38:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2020,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/39:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2023,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/40:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/41:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily.append(daily_data)
        if hourly_data is not None:
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/42:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    lat = location_eindhoven[0], 
    lon=location_eindhoven[1], 
    weather_variables=w_variables)

w_hourly
75/43:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20,10))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.plot(w_hourly['time'], w_hourly['shortwave_radiation'], label='Solar Radiation')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s), Solar Radiation (W/m2)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
75/44:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20,10))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
75/45:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10,5))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
75/46:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(8,2))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
75/47:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20,3))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
75/48:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    weather_variables=w_variables)

w_hourly
75/49:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    location_eindhoven,
    weather_variables=w_variables)

w_hourly
75/50:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = (51.441642, 5.469722)
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    [location_eindhoven],
    weather_variables=w_variables)

w_hourly
75/51:
TSO_COUNTRY_CODES = {
    "SK"
"DK"
"LT"
"SI"
"CZ"
"FR"
"LV"
"RO"
"NL"
"EE"
"HU"
"AT"
"FI"
"RS"
"IE"
"PL"
"UA"
"ES"
"PT"
"NO"
"BA"
"GB"
"BG"
"IT"
"HR"
"SE"
"GR"
"CH"
"BE"
"DE"

'Germany' : 'DE',

}

# Locations of large European cities
LOCATIONS = {
    'Amsterdam' : {'location' : (52.370216, 4.895168), 'country' : 'NL'},
    'Barcelona' : {'location' : (41.385064, 2.173403), 'country' : 'ES'},
    'Berlin' : {'location' : (52.520008, 13.404954), 'country' : 'DE'},
    'Frankfurt' : {'location' : (50.110922, 8.682127), 'country' : 'DE'},
    'Arhus' : {'location' : (56.162939, 10.203921), 'country' : 'DK'},
    'Copenhagen' : {'location' : (55.676097, 12.568337), 'country' : 'DK'},
    'Brussels' : {'location' : (50.850340, 4.351710), 'country' : 'BE'},
}
75/52:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

LOCATIONS = {
    "Bratislava": {"location": (48.148598, 17.107748), "country": "SK"},
    "Koice": {"location": (48.720595, 21.257698), "country": "SK"},
    "Copenhagen": {"location": (55.676097, 12.568337), "country": "DK"},
    "Aarhus": {"location": (56.162939, 10.203921), "country": "DK"},
    "Vilnius": {"location": (54.687157, 25.279652), "country": "LT"},
    "Kaunas": {"location": (54.898521, 23.903597), "country": "LT"},
    "Ljubljana": {"location": (46.056946, 14.505751), "country": "SI"},
    "Maribor": {"location": (46.554650, 15.646049), "country": "SI"},
    "Prague": {"location": (50.075539, 14.437800), "country": "CZ"},
    "Brno": {"location": (49.195060, 16.606837), "country": "CZ"},
    "Paris": {"location": (48.856614, 2.352222), "country": "FR"},
    "Marseille": {"location": (43.296482, 5.369780), "country": "FR"},
    "Riga": {"location": (56.949649, 24.105186), "country": "LV"},
    "Daugavpils": {"location": (55.874296, 26.536963), "country": "LV"},
    "Bucharest": {"location": (44.426765, 26.102537), "country": "RO"},
    "Cluj-Napoca": {"location": (46.771210, 23.623635), "country": "RO"},
    "Amsterdam": {"location": (52.370216, 4.895168), "country": "NL"},
    "Rotterdam": {"location": (51.920179, 4.481774), "country": "NL"},
    "Tallinn": {"location": (59.436962, 24.753574), "country": "EE"},
    "Tartu": {"location": (58.380624, 26.725056), "country": "EE"},
    "Budapest": {"location": (47.497913, 19.040236), "country": "HU"},
    "Debrecen": {"location": (47.531604, 21.627312), "country": "HU"},
    "Vienna": {"location": (48.208176, 16.373819), "country": "AT"},
    "Graz": {"location": (47.070714, 15.439504), "country": "AT"},
    "Helsinki": {"location": (60.169856, 24.938379), "country": "FI"},
    "Turku": {"location": (60.451810, 22.266630), "country": "FI"},
    "Warsaw": {"location": (52.229676, 21.012229), "country": "PL"},
    "Lodz": {"location": (51.759250, 19.455983), "country": "PL"},
    "Madrid": {"location": (40.416775, -3.703790), "country": "ES"},
    "Barcelona": {"location": (41.385064, 2.173403), "country": "ES"},
    "Lisbon": {"location": (38.722252, -9.139337), "country": "PT"},
    "Porto": {"location": (41.157944, -8.629105), "country": "PT"},
    "Oslo": {"location": (59.913869, 10.752245), "country": "NO"},
    "Bergen": {"location": (60.392050, 5.322050), "country": "NO"},
    "Sofia": {"location": (42.697708, 23.321868), "country": "BG"},
    "Plovdiv": {"location": (42.135407, 24.745290), "country": "BG"},
    "Rome": {"location": (41.902782, 12.496366), "country": "IT"},
    "Milan": {"location": (45.464204, 9.189982), "country": "IT"},
    "Zagreb": {"location": (45.815011, 15.981919), "country": "HR"},
    "Split": {"location": (43.508132, 16.440193), "country": "HR"},
    "Stockholm": {"location": (59.329323, 18.068581), "country": "SE"},
    "Gothenburg": {"location": (57.708870, 11.974560), "country": "SE"},
    "Athens": {"location": (37.983810, 23.727539), "country": "GR"},
    "Thessaloniki": {"location": (40.640060, 22.944420), "country": "GR"},
    "Zrich": {"location": (47.376887, 8.541694), "country": "CH"},
    "Geneva": {"location": (46.204391, 6.143158), "country": "CH"},
    "Brussels": {"location": (50.850340, 4.351710), "country": "BE"},
    "Antwerp": {"location": (51.219448, 4.402464), "country": "BE"},
    "Berlin": {"location": (52.520007, 13.404954), "country": "DE"},
    "Hamburg": {"location": (53.551086, 9.993682), "country": "DE"}
}
75/53:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

LOCATIONS = {
    "Bratislava": {"location": (48.148598, 17.107748), "country": "SK"},
    "Koice": {"location": (48.720595, 21.257698), "country": "SK"},
    "Copenhagen": {"location": (55.676097, 12.568337), "country": "DK"},
    "Aarhus": {"location": (56.162939, 10.203921), "country": "DK"},
    "Vilnius": {"location": (54.687157, 25.279652), "country": "LT"},
    "Kaunas": {"location": (54.898521, 23.903597), "country": "LT"},
    "Ljubljana": {"location": (46.056946, 14.505751), "country": "SI"},
    "Maribor": {"location": (46.554650, 15.646049), "country": "SI"},
    "Prague": {"location": (50.075539, 14.437800), "country": "CZ"},
    "Brno": {"location": (49.195060, 16.606837), "country": "CZ"},
    "Paris": {"location": (48.856614, 2.352222), "country": "FR"},
    "Marseille": {"location": (43.296482, 5.369780), "country": "FR"},
    "Riga": {"location": (56.949649, 24.105186), "country": "LV"},
    "Daugavpils": {"location": (55.874296, 26.536963), "country": "LV"},
    "Bucharest": {"location": (44.426765, 26.102537), "country": "RO"},
    "Cluj-Napoca": {"location": (46.771210, 23.623635), "country": "RO"},
    "Amsterdam": {"location": (52.370216, 4.895168), "country": "NL"},
    "Rotterdam": {"location": (51.920179, 4.481774), "country": "NL"},
    "Tallinn": {"location": (59.436962, 24.753574), "country": "EE"},
    "Tartu": {"location": (58.380624, 26.725056), "country": "EE"},
    "Budapest": {"location": (47.497913, 19.040236), "country": "HU"},
    "Debrecen": {"location": (47.531604, 21.627312), "country": "HU"},
    "Vienna": {"location": (48.208176, 16.373819), "country": "AT"},
    "Graz": {"location": (47.070714, 15.439504), "country": "AT"},
    "Helsinki": {"location": (60.169856, 24.938379), "country": "FI"},
    "Turku": {"location": (60.451810, 22.266630), "country": "FI"},
    "Warsaw": {"location": (52.229676, 21.012229), "country": "PL"},
    "Lodz": {"location": (51.759250, 19.455983), "country": "PL"},
    "Madrid": {"location": (40.416775, -3.703790), "country": "ES"},
    "Barcelona": {"location": (41.385064, 2.173403), "country": "ES"},
    "Lisbon": {"location": (38.722252, -9.139337), "country": "PT"},
    "Porto": {"location": (41.157944, -8.629105), "country": "PT"},
    "Oslo": {"location": (59.913869, 10.752245), "country": "NO"},
    "Bergen": {"location": (60.392050, 5.322050), "country": "NO"},
    "Sofia": {"location": (42.697708, 23.321868), "country": "BG"},
    "Plovdiv": {"location": (42.135407, 24.745290), "country": "BG"},
    "Rome": {"location": (41.902782, 12.496366), "country": "IT"},
    "Milan": {"location": (45.464204, 9.189982), "country": "IT"},
    "Zagreb": {"location": (45.815011, 15.981919), "country": "HR"},
    "Split": {"location": (43.508132, 16.440193), "country": "HR"},
    "Stockholm": {"location": (59.329323, 18.068581), "country": "SE"},
    "Gothenburg": {"location": (57.708870, 11.974560), "country": "SE"},
    "Athens": {"location": (37.983810, 23.727539), "country": "GR"},
    "Thessaloniki": {"location": (40.640060, 22.944420), "country": "GR"},
    "Zrich": {"location": (47.376887, 8.541694), "country": "CH"},
    "Geneva": {"location": (46.204391, 6.143158), "country": "CH"},
    "Brussels": {"location": (50.850340, 4.351710), "country": "BE"},
    "Antwerp": {"location": (51.219448, 4.402464), "country": "BE"},
    "Berlin": {"location": (52.520007, 13.404954), "country": "DE"},
    "Hamburg": {"location": (53.551086, 9.993682), "country": "DE"}
}

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}
75/54:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

LOCATIONS = {
    "Bratislava": {"location": (48.148598, 17.107748), "country": "SK"},
    "Koice": {"location": (48.720595, 21.257698), "country": "SK"},
    "Copenhagen": {"location": (55.676097, 12.568337), "country": "DK"},
    "Aarhus": {"location": (56.162939, 10.203921), "country": "DK"},
    "Vilnius": {"location": (54.687157, 25.279652), "country": "LT"},
    "Kaunas": {"location": (54.898521, 23.903597), "country": "LT"},
    "Ljubljana": {"location": (46.056946, 14.505751), "country": "SI"},
    "Maribor": {"location": (46.554650, 15.646049), "country": "SI"},
    "Prague": {"location": (50.075539, 14.437800), "country": "CZ"},
    "Brno": {"location": (49.195060, 16.606837), "country": "CZ"},
    "Paris": {"location": (48.856614, 2.352222), "country": "FR"},
    "Marseille": {"location": (43.296482, 5.369780), "country": "FR"},
    "Riga": {"location": (56.949649, 24.105186), "country": "LV"},
    "Daugavpils": {"location": (55.874296, 26.536963), "country": "LV"},
    "Bucharest": {"location": (44.426765, 26.102537), "country": "RO"},
    "Cluj-Napoca": {"location": (46.771210, 23.623635), "country": "RO"},
    "Amsterdam": {"location": (52.370216, 4.895168), "country": "NL"},
    "Rotterdam": {"location": (51.920179, 4.481774), "country": "NL"},
    "Tallinn": {"location": (59.436962, 24.753574), "country": "EE"},
    "Tartu": {"location": (58.380624, 26.725056), "country": "EE"},
    "Budapest": {"location": (47.497913, 19.040236), "country": "HU"},
    "Debrecen": {"location": (47.531604, 21.627312), "country": "HU"},
    "Vienna": {"location": (48.208176, 16.373819), "country": "AT"},
    "Graz": {"location": (47.070714, 15.439504), "country": "AT"},
    "Helsinki": {"location": (60.169856, 24.938379), "country": "FI"},
    "Turku": {"location": (60.451810, 22.266630), "country": "FI"},
    "Warsaw": {"location": (52.229676, 21.012229), "country": "PL"},
    "Lodz": {"location": (51.759250, 19.455983), "country": "PL"},
    "Madrid": {"location": (40.416775, -3.703790), "country": "ES"},
    "Barcelona": {"location": (41.385064, 2.173403), "country": "ES"},
    "Lisbon": {"location": (38.722252, -9.139337), "country": "PT"},
    "Porto": {"location": (41.157944, -8.629105), "country": "PT"},
    "Oslo": {"location": (59.913869, 10.752245), "country": "NO"},
    "Bergen": {"location": (60.392050, 5.322050), "country": "NO"},
    "Sofia": {"location": (42.697708, 23.321868), "country": "BG"},
    "Plovdiv": {"location": (42.135407, 24.745290), "country": "BG"},
    "Rome": {"location": (41.902782, 12.496366), "country": "IT"},
    "Milan": {"location": (45.464204, 9.189982), "country": "IT"},
    "Zagreb": {"location": (45.815011, 15.981919), "country": "HR"},
    "Split": {"location": (43.508132, 16.440193), "country": "HR"},
    "Stockholm": {"location": (59.329323, 18.068581), "country": "SE"},
    "Gothenburg": {"location": (57.708870, 11.974560), "country": "SE"},
    "Athens": {"location": (37.983810, 23.727539), "country": "GR"},
    "Thessaloniki": {"location": (40.640060, 22.944420), "country": "GR"},
    "Zrich": {"location": (47.376887, 8.541694), "country": "CH"},
    "Geneva": {"location": (46.204391, 6.143158), "country": "CH"},
    "Brussels": {"location": (50.850340, 4.351710), "country": "BE"},
    "Antwerp": {"location": (51.219448, 4.402464), "country": "BE"},
    "Berlin": {"location": (52.520007, 13.404954), "country": "DE"},
    "Hamburg": {"location": (53.551086, 9.993682), "country": "DE"}
}

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2022,1,1)
END_DATE = dt.date(2023,1,1)
75/55:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
75/56:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    [city['location'] for city in LOCATIONS],
    weather_variables=W_VARIABLES)

w_daily
75/57:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    [city_value['location'] for city_name, city_value in LOCATIONS.items()],
    weather_variables=W_VARIABLES)

w_daily
75/58:
# Add city name and country to weather dataframe (based on lat/lon)
for _df in [w_daily, w_hourly]:
    # Select city based on lat/lon
    _df['city'] = _df.apply(lambda x: [city_name for city_name, city_value in LOCATIONS.items() if city_value['location'] == (x['lat'], x['lon'])][0], axis=1)
    # Select country based on city
    _df['country'] = _df.apply(lambda x: LOCATIONS[x['city']]['country'], axis=1)

w_daily
75/59:
# Add city name and country to weather dataframe (based on lat/lon)
for _df in [w_daily, w_hourly]:
    # Select city based on lat/lon
    _df['city'] = _df.apply(lambda x: [city_name for city_name, city_value in LOCATIONS.items() if city_value['location'] == (x['latitude'], x['longitude'])][0], axis=1)
    # Select country based on city
    _df['country'] = _df.apply(lambda x: LOCATIONS[x['city']]['country'], axis=1)

w_daily
75/60:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    [location_eindhoven],
    weather_variables=w_variables)

w_hourly
75/61:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection: {}".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres: {}".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
75/62:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    [location_eindhoven],
    weather_variables=w_variables)

w_hourly
75/63:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE"}
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2022,1,1)
END_DATE = dt.date(2023,1,1)
75/64:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE"}
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2022,12,1)
END_DATE = dt.date(2023,1,1)
75/65:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    [city_value['location'] for city_name, city_value in LOCATIONS.items()],
    weather_variables=W_VARIABLES)

w_daily
75/66:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
75/67: w_hourly
75/68:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(15, 5))
for i, city in enumerate(LOCATIONS):
    ax.plot(w_hourly['temperature_2m'].loc[city], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/69:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(15, 5))
for i, city in enumerate(LOCATIONS):
    ax.plot(w_hourly['temperature_2m'][w_hourly['city'] == city], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/70:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(10, 5))
for i, city in enumerate(LOCATIONS):
    ax.plot(w_hourly['temperature_2m'][w_hourly['city'] == city], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/71:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(10, 5))
for i, city in enumerate(LOCATIONS):
    ax.plot(w_hourly[w_hourly['city']['temperature_2m'] == city], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/72:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(10, 5))
for i, city in enumerate(LOCATIONS):
    ax.plot(w_hourly[w_hourly['city'] == city]['temperature_2m'], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/73:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(10, 5))
for i, city in enumerate(LOCATIONS):
    ax.plot(w_hourly[w_hourly['city'] == city['city']]['temperature_2m'], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/74:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(10, 5))
for i, city in enumerate(LOCATIONS):
    _df = w_daily[w_daily['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/75:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(10, 5))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/76:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(20, 3))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])
ax.legend()
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/77:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(20, 3))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])

# Horizontal legend
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/78:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(20, 3))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])

# Horizontal legend
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=5)
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/79:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(20, 3))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])

# Horizontal legend
ax.legend(loc='upper center', fancybox=True, shadow=True, ncol=5)
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/80:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(20, 3))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])

# Horizontal legend outside the plot
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)
#ax.legend(loc='upper center', fancybox=True, shadow=True, ncol=5)
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/81:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2022,12,1)
END_DATE = dt.date(2023,1,1)
75/82:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
75/83: w_hourly
75/84:
# Visualize the different temperatures in the different cities
fig, ax = plt.subplots(figsize=(20, 3))
for i, city in enumerate(LOCATIONS):
    _df = w_hourly[w_hourly['city'] == city['city']]
    ax.plot(_df['time'], _df['temperature_2m'], label=city['city'])

# Horizontal legend outside the plot
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=5)
ax.set_ylabel('Temperature (C)')
ax.set_title('Temperature in different cities')
plt.show()
75/85:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_daily, col="region", col_wrap=3, height=3, aspect=1.5)
graph.map(plt.plot, "date", "temperature_2m_max", color="red")
graph.map(plt.plot, "date", "temperature_2m_min", color="blue")
graph.set_axis_labels("Date", "Temperature (C)")
graph.set_titles("{col_name}")
graph.fig.suptitle("Daily temperature in different cities", y=1.05)
graph.fig.tight_layout()
graph.fig.subplots_adjust(top=0.9)
graph
75/86:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_daily, col="region", col_wrap=3, height=3, aspect=1.5)
graph.map(plt.plot, "date", "temperature_2m_max", color="red")
graph.map(plt.plot, "date", "temperature_2m_min", color="blue")
graph.set_axis_labels("Date", "Temperature (C)")
graph.set_titles("{col_name}")
graph.fig.suptitle("Daily temperature in different cities", y=1.05)
graph.fig.tight_layout()
graph.fig.subplots_adjust(top=0.9)
graph
75/87:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_daily, col="region", col_wrap=3, height=3, aspect=1.5)
graph.map(plt.plot, "date", "temperature_2m_max", color="red")
graph.map(plt.plot, "date", "temperature_2m_min", color="blue")
graph.set_axis_labels("Date", "Temperature (C)")
graph.fig.suptitle("Daily temperature in different cities", y=1.05)
graph.fig.tight_layout()
graph.fig.subplots_adjust(top=0.9)
graph
75/88:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_daily, col="region", col_wrap=3, height=3, aspect=1.5)
graph.map(plt.plot, "time", "temperature_2m_max", color="red")
graph.map(plt.plot, "time", "temperature_2m_min", color="blue")
graph.set_axis_labels("Date", "Temperature (C)")
graph.set_titles("{col_name}")
graph.fig.suptitle("Daily temperature in different cities", y=1.05)
graph.fig.tight_layout()
graph.fig.subplots_adjust(top=0.9)
graph
75/89:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_hourly, col="region", hue = 'city', col_wrap=3, height=3, aspect=1.5)
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "temperature_2m", "tip", edgecolor ="w").add_legend()
plt.show()
75/90:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_hourly, col="region", hue = 'city', col_wrap=3, height=3, aspect=1.5)
# map the above form facetgrid with some attributes
graph.map(plt.scatter, "temperature_2m", edgecolor ="w").add_legend()
plt.show()
75/91:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_hourly, col="region", hue = 'city', col_wrap=3, height=3, aspect=1.5)
# map the above form facetgrid with some attributes
graph.map(plt.scatter, 'time',"temperature_2m", edgecolor ="w").add_legend()
plt.show()
75/92:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_hourly, col="region", hue = 'city', col_wrap=3, height=3, aspect=1.5)
# map the above form facetgrid with some attributes
graph.map(plt.plot, 'time',"temperature_2m", edgecolor ="w").add_legend()
plt.show()
75/93:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

graph = sns.FacetGrid(w_hourly, col="region", hue = 'city', col_wrap=3, height=3, aspect=1.5)
# map the above form facetgrid with some attributes
graph.map(plt.plot, 'time',"temperature_2m").add_legend()
plt.show()
75/94:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("ticks")
g = sns.FacetGrid(w_hourly,x="time",y="temperature_2m", col="region", hue = 'city', col_wrap=3,
                dodge=False, 
                aspect = 1,
                sharey = True,
                legend_out = False,               # REMOVE MASTER LEGEND
               ).despine(left=True)
# MASTER SERIES OF serial
ser_vals = pd.Series(w_hourly['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['serial'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/95:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("ticks")
g = sns.catplot(w_hourly,x="time",y="temperature_2m", col="region", hue = 'city', col_wrap=3,
                dodge=False, 
                aspect = 1,
                sharey = True,
                legend_out = False,               # REMOVE MASTER LEGEND
               ).despine(left=True)
# MASTER SERIES OF serial
ser_vals = pd.Series(w_hourly['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['serial'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/96:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("ticks")
g = sns.catplot(w_hourly,x="time",y="temperature_2m", col="region", hue = 'city', col_wrap=3,
                dodge=False, 
                aspect = 1,
                sharey = True,
                legend_out = False,               # REMOVE MASTER LEGEND
               ).despine(left=True)
# MASTER SERIES OF serial
ser_vals = pd.Series(w_hourly['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['region'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/97:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

g = sns.catplot(w_hourly,x="time",y="temperature_2m", col="region", hue = 'city', col_wrap=3,
                legend_out = False,               # REMOVE MASTER LEGEND
               ).despine(left=True)
# MASTER SERIES OF serial
ser_vals = pd.Series(w_hourly['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), w_hourly.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['city'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/98:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

g = sns.FacetGrid(w_daily, col="region", hue = 'city', col_wrap=3,
                legend_out = False,               # REMOVE MASTER LEGEND
               )
g.map(plt.plot, "time", "temperature_2m_max")

# MASTER SERIES OF serial
ser_vals = pd.Series(w_daily['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), w_daily.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['city'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/99:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df, col="region", hue='city', col_wrap=3,
                legend_out = False,               # REMOVE MASTER LEGEND
               )
g.map(plt.plot, "time", "temperature_2m_max")

# MASTER SERIES OF serial
ser_vals = pd.Series(df['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), df.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['city'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/100:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df, col="region", hue='city', col_wrap=3,
                legend_out = False,               # REMOVE MASTER LEGEND
               )
g.map(plt.plot, "time", "temperature_2m_max")

# MASTER SERIES OF serial
ser_vals = pd.Series(df['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), df.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['city'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
        print(idx)
       axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/102:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df, col="region", hue='city', col_wrap=3,
                legend_out = False,               # REMOVE MASTER LEGEND
               )
g.map(plt.plot, "time", "temperature_2m_max")

# MASTER SERIES OF serial
ser_vals = pd.Series(df['city'].sort_values().unique())

for axes, (i, d) in zip(g.axes.ravel(), df.groupby(['region'])):
    handles, labels = axes.get_legend_handles_labels()

    # SUBSET MASTER SERIES OF serial
    vals = ser_vals[ser_vals.isin(d['city'].unique())]    
    idx = vals.index.tolist()

    if len(idx) > 0:
        print(idx)
        axes.legend(handles = [handles[i] for i in idx], 
                   labels = vals.tolist())

plt.show()
75/103:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df, col="region")
for (row_var, col_var), facet_df in df.groupby(["region", "city"]):
    ax = g.axes[g.row_names.index(row_var), g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

plt.show()
75/104:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df, col="region")
for (row_var, col_var), facet_df in df.groupby(["region"]):
    ax = g.axes[g.row_names.index(row_var), g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

plt.show()
75/105:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df,col="region")
for (col_var), facet_df in df.groupby(["region"]):
    ax = g.axes[g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/106:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df,col="region")
for (col_var), facet_df in df.groupby(["region"]):
    ax = g.axes['time',g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/107:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
g = sns.FacetGrid(df,col="region")
for (col_var), facet_df in df.groupby(["region"]):
    ax = g.axes[None,g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/108:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)

g = sns.FacetGrid(df, row='region',col="region")
for (row_var, col_var), facet_df in df.groupby(["region", "region"]):
    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/109:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
df['area'] = 'EU'
g = sns.FacetGrid(df, row='region',col="area")
for (row_var, col_var), facet_df in df.groupby(["region", "area"]):
    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    #sns.swarmplot(x="time", y="total_bill", hue="size", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/110:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
df['area'] = 'EU'
g = sns.FacetGrid(df, row='region',col="area")
for (row_var, col_var), facet_df in df.groupby(["region", "area"]):
    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/111:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
df['area'] = 'EU'
g = sns.FacetGrid(df, row='area',col="region")
for (row_var, col_var), facet_df in df.groupby(["area", "region"]):
    ax = g.axes[g.col_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/112:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
df['area'] = 'EU'
g = sns.FacetGrid(df, row='area',col="region")
for (row_var, col_var), facet_df in df.groupby(["area", "region"]):
    ax = g.axes[g.row_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/113:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
df['area'] = 'EU'
g = sns.FacetGrid(df, row='region',col="area")
for (row_var, col_var), facet_df in df.groupby(["region", "area"]):
    ax = g.axes[g.row_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/114:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)


# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(3, 1, figsize=(15, 15), sharex=True, sharey=True)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/115:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(5, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/116:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/117:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/118:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 4, title="")
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/119:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title="", nrows= 4)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/120:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title="", nrow = 4)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/121:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot. 4 rows
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title="", ncol=4)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/122:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot.
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title="", ncols = 2)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/123:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
# Visualize the different temperatures in the different cities. 
# Matplotlib
facet_plots = []
fig, axes = plt.subplots(7, 1, figsize=(15, 15), sharex=True, sharey=False)
for i, (region, region_df) in enumerate(df.groupby('region')):
    ax = axes[i]
    sns.lineplot(x="time", y="temperature_2m_max",hue='city', data=region_df, ax=ax)
    # Attach the legend to the right of the plot, outside of the plot.
    ax.legend(bbox_to_anchor=(1.05, 1), loc=0, borderaxespad=0., title="", ncols = 2)
    ax.set(xlabel="", ylabel="")
    ax.set_title(region)
    facet_plots.append(ax)
75/124:
# Visualize the different temperatures in the different cities. Facet the plot by region
import seaborn as sns
import matplotlib.pyplot as plt

df = w_daily.sort_values(['region', 'city']).reset_index(drop=True)
df['area'] = 'EU'
g = sns.FacetGrid(df, row='area',col="region")
for (row_var, col_var), facet_df in df.groupby(["area", "region"]):
    ax = g.axes[g.row_names.index(row_var),g.col_names.index(col_var)]
    sns.lineplot(x="time", y="temperature_2m_max", data=facet_df, ax=ax)
    ax.set(xlabel="", ylabel="")

g.set_axis_labels("time", "Temperature (C)")
g.set_titles()
75/125:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city

facet_plots = []
for region in set([l['region'] for l in LOCATIONS]):
    g = sns.FacetGrid(
            w_daily[w_daily['region'] == region], 
            col='region', 
            hue='city', 
            col_wrap=3, 
            height=3, 
            aspect=1.5
        )
    g.map(sns.lineplot, 'date', 'temperature_2m_max')
    # Add a legend
    g.add_legend()
    facet_plots.append(g)
    
plt.show()
75/126:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city

facet_plots = []
for region in set([l['region'] for l in LOCATIONS]):
    g = sns.FacetGrid(
            w_daily[w_daily['region'] == region], 
            col='region', 
            hue='city', 
            col_wrap=3, 
            height=3, 
            aspect=1.5
        )
    g.map(sns.lineplot, 'time', 'temperature_2m_max')
    # Add a legend
    g.add_legend()
    facet_plots.append(g)
    
plt.show()
75/127:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20))
for i, region in enumerate(REGIONS):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='date', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('Date')
 
plt.show()
75/128:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20))
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='date', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('Date')
 
plt.show()
75/129:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20))
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('Date')
 
plt.show()
75/130:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
 
plt.show()
75/131:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
 
plt.show()
75/132:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
    fig.subplots_adjust(hspace=0.5)
 
plt.show()
75/133:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
 
plt.show()
75/134:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
 
plt.show()
75/135:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/136:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2022,1,1)
END_DATE = dt.date(2023,1,1)
75/137:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
75/138:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='temperature_2m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/139:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(5, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/140:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 5), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/141:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 15), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/142:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/143:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    axes[i].lines[0].set_linewidth(1)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/144:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    axes[i].lines[0].set_linewidth(0.75)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/145:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    axes[i].lines[0].set_linewidth(0.75)
    # Smaller alpha
    axes[i].lines[0].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/146:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    axes[i].lines[0].set_linewidth(0.5)
    # Smaller alpha
    axes[i].lines[0].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/147:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    axes[i].lines[0].set_linewidth(0.5)
    # Make sure the hue is seethrough
    axes[i].lines[0].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/148:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    axes[i].lines[i].set_linewidth(0.5) for i in range(1, len(axes[i].lines))
    # Make sure the hue is seethrough
    axes[i].lines[i].set_alpha(0.5) for i in range(1, len(axes[i].lines))
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/149:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    [axes[i].lines[i].set_linewidth(0.5) for i in range(1, len(axes[i].lines))]
    # Make sure the hue is seethrough
    [axes[i].lines[i].set_alpha(0.5) for i in range(1, len(axes[i].lines))]
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/150:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    for i in range(1, len(axes[i].lines)):
        axes[i].lines[i].set_linewidth(0.5)
        axes[i].lines[i].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/151:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    for i in range(0, len(axes[i].lines)):
        axes[i].lines[i].set_linewidth(0.5)
        axes[i].lines[i].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/152:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[j].lines)):
        axes[i].lines[j].set_linewidth(0.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/153:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(0.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/154:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1)
        axes[i].lines[j].set_alpha(0.75)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/155:
import seaborn as sns

# Plot the daily temperature in matplotlib by region, hue by city
# Multiplot 7 pltos
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='city', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/156:
import seaborn as sns

df = w_daily.copy()
df['label'] = df['city'] + '(' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(w_daily['region'].unique()):
    sns.lineplot(data=w_daily[w_daily['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/157:
import seaborn as sns

df = w_daily.copy()
df['label'] = df['city'] + '(' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/158:
import seaborn as sns

df = w_daily.copy()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/159:
import seaborn as sns

df = w_daily.copy()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m_max', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/160:
import seaborn as sns

df = w_hourly.copy()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/161:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('W', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'


fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windgusts_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/162:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('W', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/163:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('W', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/164:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=1)
 
plt.show()
75/165:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.75)
 
plt.show()
75/166:
# Investigate the distribution of wind speed for each city
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=20)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    axes[i].set_xlabel('Wind Speed (m/s)')
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
75/167:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=20)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    axes[i].set_xlabel('Wind Speed (m/s)')
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
75/168:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=20)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    axes[i].set_xlabel('Wind Speed (m/s)')
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/169:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    axes[i].set_xlabel('Wind Speed (m/s)')
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/170:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    axes[i].set_xlabel('Wind Speed (m/s)')
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/171:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/172:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/173:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    title = old_legend.get_title().get_text()
    ax.legend(handles, labels, loc=new_loc, title=title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 'upper right', bbox_to_anchor=(1.05, 1), ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/174:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    title = old_legend.get_title().get_text()
    ax.legend(handles, labels, loc=new_loc, title=title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 'upper right', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/175:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    title = old_legend.get_title().get_text()
    ax.legend(handles, labels, loc=new_loc, title=title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], loc=2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0.)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/176:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    title = old_legend.get_title().get_text()
    ax.legend(handles, labels, loc=new_loc, title=title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0.)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/177:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    title = old_legend.get_title().get_text()
    ax.legend(handles, labels, loc=new_loc, title=title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., name = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/178:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    title = old_legend.get_title().get_text()
    ax.legend(handles, labels, loc=new_loc, title=title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/179:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/180:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/181:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
75/182:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
75/183:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Wind Speed (m/s)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.75)
plt.show()
75/184:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Wind Speed (m/s)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.75)
plt.show()
75/185:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/186:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/187:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/188:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
df['windspeed_10m'] = np.log(df['windspeed_10m'])
def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/189:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
df['log_windspeed_10m'] = np.log(df['windspeed_10m'])

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/190:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
df['log_windspeed_10m'] = np.log(df['windspeed_10m'] if df['windspeed_10m'] > 0 else 0.0001)

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/191:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
df['log_windspeed_10m'] = np.log(df['windspeed_10m']) if df['windspeed_10m'] > 0 else 0

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/192:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/193:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')s
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/194:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/195:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/196:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5, stack = True)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/197:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5,  multiple = 'stack')
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/198:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
#df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5,  multiple = 'stack')
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/199:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
#df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.75)
plt.show()
75/200:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
#df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.5)
plt.show()
75/201:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Wind Speed (m/s)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
plt.show()
75/202:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
plt.show()
75/203:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ({country_list})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ({country_list})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys())
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys())
75/204:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ({country_list})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ({country_list})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys().to_list())
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, TSO_COUNTRIES.keys().to_list())
75/205:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ({country_list})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ({country_list})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/206:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ({','.join(country_list)})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ({','.join(country_list)})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/207:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ({','.join(country_list)})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ({','.join(country_list)})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/208:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ({"','".join(country_list)})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ({"','".join(country_list)})
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/209:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/210:
import seaborn as sns

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['time', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='time').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'time']).sum().reset_index()

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    sns.lineplot(data=df[df['country_code'] == country], x='time', y='generation', hue='type', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/211:
import seaborn as sns

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='time').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/212:
import seaborn as sns

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/213:
import seaborn as sns

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Area plot with fill
    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], palette='Set1', fill=True, alpha = 0.5, multiple = 'stack')
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/214:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Area plot with fill
    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], palette='Set1', alpha = 0.5, multiple = 'stack')
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/215:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Area plot with fill
    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], palette='Set1', alpha = 0.5, multiple = 'stack')
    # Area fill
    axes[i].fill_between(df[df['country_code'] == country]['timestamp'], df[df['country_code'] == country]['generation'], alpha=0.5)
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/216:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Area plot with fill
    sns.lineplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], alpha = 0.5, multiple = 'stack')
    # Area fill
    axes[i].fill_between(df[df['country_code'] == country]['timestamp'], df[df['country_code'] == country]['generation'], alpha=0.5)
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/217:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Multiple stack
    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i], alpha = 0.5, )
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/218:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Multiple stack
    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type', ax=axes[i] )
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/219:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_generation_production.copy()
# Pivot wide to long
df = df.melt(id_vars=['timestamp', 'country_code', 'price_area'], var_name='type', value_name='generation')
# groupby 
df = df.groupby(['country_code', 'price_area', 'type']).resample('M', on='timestamp').mean().reset_index()
# Sum all price areas
df = df.groupby(['country_code', 'type', 'timestamp']).sum().reset_index()

country_length = len(df['country_code'].unique())
fig, axes = plt.subplots(country_length, 1, figsize=(10, 20), sharex=False)
for i, country in enumerate(df['country_code'].unique()):
    # Multiple stack
    plt.stackplot(data=df[df['country_code'] == country], x='timestamp', y='generation', hue='type' )
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(country)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Generation (MW)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/220:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()

fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='generation', hue='price_area', ax=axes[i] )
# Smaller line width
for j in range(0, len(axes[i].lines)):
    axes[i].lines[j].set_linewidth(1.5)
    axes[i].lines[j].set_alpha(0.5)
axes[i].set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes[i].set_ylabel('Price (EUR/MWh)')
axes[i].set_xlabel('')
# Rotate xticks
axes[i].tick_params(axis='x', rotation=45)
# Legends outside the plot
axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/221:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()

fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='generation', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes[i].lines)):
    axes[i].lines[j].set_linewidth(1.5)
    axes[i].lines[j].set_alpha(0.5)
axes[i].set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes[i].set_ylabel('Price (EUR/MWh)')
axes[i].set_xlabel('')
# Rotate xticks
axes[i].tick_params(axis='x', rotation=45)
# Legends outside the plot
axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/222:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()

fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes[i].lines)):
    axes[i].lines[j].set_linewidth(1.5)
    axes[i].lines[j].set_alpha(0.5)
axes[i].set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes[i].set_ylabel('Price (EUR/MWh)')
axes[i].set_xlabel('')
# Rotate xticks
axes[i].tick_params(axis='x', rotation=45)
# Legends outside the plot
axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/223:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
print(df)
fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes[i].lines)):
    axes[i].lines[j].set_linewidth(1.5)
    axes[i].lines[j].set_alpha(0.5)
axes[i].set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes[i].set_ylabel('Price (EUR/MWh)')
axes[i].set_xlabel('')
# Rotate xticks
axes[i].tick_params(axis='x', rotation=45)
# Legends outside the plot
axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/224:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_day_ahead_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/225:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
print(df)
fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes[i].lines)):
    axes[i].lines[j].set_linewidth(1.5)
    axes[i].lines[j].set_alpha(0.5)
axes[i].set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes[i].set_ylabel('Price (EUR/MWh)')
axes[i].set_xlabel('')
# Rotate xticks
axes[i].tick_params(axis='x', rotation=45)
# Legends outside the plot
axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/226:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/227:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
print(df)
fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes[i].lines)):
    axes[i].lines[j].set_linewidth(1.5)
    axes[i].lines[j].set_alpha(0.5)
axes[i].set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes[i].set_ylabel('Price (EUR/MWh)')
axes[i].set_xlabel('')
# Rotate xticks
axes[i].tick_params(axis='x', rotation=45)
# Legends outside the plot
axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/228:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='time').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/229:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(10, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/230:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(5, 20), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/231:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(10, 10), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/232:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(10, 5), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/233:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(20, 5), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/234:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(12, 5), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
75/235:
# Get the data for France
country_of_interest = 'FR'
# Get the data for the price area of interest
da_prices_country = hourly_entsoe_day_ahead_prices[hourly_entsoe_day_ahead_prices['country_code'] == country_of_interest]
production_country = hourly_entsoe_generation_production[hourly_entsoe_generation_production['country_code'] == country_of_interest]
weather_country = w_hourly[w_hourly['country'] == country_of_interest]

# Visualize the wind speed, wind power production and day ahead prices
fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)
# Plot the wind speed
sns.lineplot(data=weather_country, x='time', y='windspeed_10m', ax=axes[0])
axes[0].set_title('Wind Speed')
axes[0].set_ylabel('Wind Speed (m/s)')
# Plot the wind power production
sns.lineplot(data=production_country, x='time', y='wind_onshore_generation_actual', ax=axes[1])
axes[1].set_title('Wind Power Production')
axes[1].set_ylabel('Wind Power Production (MW)')
# Plot the day ahead prices
sns.lineplot(data=da_prices_country, x='timestamp', y='price', ax=axes[2])
axes[2].set_title('Day Ahead Prices')
75/236:
hourly_entsoe_day_ahead_prices.columns
w_hourly.columns
hourly_entsoe_generation_production.columns
75/237:
print(hourly_entsoe_day_ahead_prices.columns)
print(w_hourly.columns)
print(hourly_entsoe_generation_production.columns)
75/238:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/239:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
df = df[df['country_code'] == 'DE']

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/240:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
df = df[df['country_code'] == 'NL']

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/241:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
df = df[df['country_code'] == ['NL','DE']]

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/242:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
df = df[df['country_code'].isin['NL','DE']]

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/243:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
df = df[df['country_code'] in ['NL','DE']]

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/244:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.PeriodIndex(df['timestamp'], freq='Q')
df['month'] = pd.DatetimeIndex(df['timestamp']).month

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'month', 'country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'month', 'country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'month', 'wind_mean', color='blue')
g.map(plt.fill_between, 'month', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'month', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'month', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'month', 'renewable_mean', color='green')
g.map(plt.fill_between, 'month', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'month', 'fossil_mean', color='red')
g.map(plt.fill_between, 'month', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'month', 'price_mean', color='black')
g.map(plt.fill_between, 'month', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/245:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'hour', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'hour', 'renewable_mean', color='green')
g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
g.map(plt.fill_between, 'hour', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
g.map(plt.fill_between, 'hour', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/246:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g['wind_mean'] = g['wind_mean'].astype(float)
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'hour', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'hour', 'renewable_mean', color='green')
g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
g.map(plt.fill_between, 'hour', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
g.map(plt.fill_between, 'hour', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/247:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price

g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', 'wind_mean' - grouped['wind_std'], 'wind_mean' + grouped['wind_std'], alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'hour', 'solar_mean' - grouped['solar_std'], 'solar_mean' + grouped['solar_std'], alpha=0.1, color='yellow')

g.map(plt.plot, 'hour', 'renewable_mean', color='green')
g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
g.map(plt.fill_between, 'hour', 'fossil_mean' - grouped['fossil_std'], 'fossil_mean' + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
g.map(plt.fill_between, 'hour', 'price_mean' - grouped['price_std'], 'price_mean' + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/248:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price

g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', grouped['wind_mean'] - grouped['wind_std'], grouped['wind_mean'] + grouped['wind_std'], alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/249:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Flatten the column index
grouped.columns = grouped.columns.get_level_values(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price

g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', grouped['wind_mean'] - grouped['wind_std'], grouped['wind_mean'] + grouped['wind_std'], alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/250:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped.columns)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', grouped['wind_mean'] - grouped['wind_std'], grouped['wind_mean'] + grouped['wind_std'], alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/251:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped.columns)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/252:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped.columns)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/253:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped.columns)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/254:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped.columns)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Show lines
g.map(plt.axhline, y=0, ls=":", c=".5")
# Set plot title, axis labels, and legend
g.fig
75/255:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/256:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).quarter

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped)
print(grouped['hour'].unique())

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/257:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

print(grouped)
print(grouped['hour'].unique())

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
#g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/258:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})

# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour','wind_mean' -'wind_std', 'wind_mean' +'wind_std', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/259:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate mean and std
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std'],
    'solar': ['mean', 'std'],
    'other_renewable': ['mean', 'std'],
    'fossil_gas': ['mean', 'std'],
    'price': ['mean', 'std']
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Generate the mean - std and mean + std columns
for col in grouped.columns:
    if 'mean' in col:
        grouped[col.replace('mean', 'min')] = grouped[col] - grouped[col.replace('mean', 'std')]
        grouped[col.replace('mean', 'max')] = grouped[col] + grouped[col.replace('mean', 'std')]

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', 'wind_min', 'wind_max', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/260:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
#  The lines represent the median, the dark shading represents the inner 50% of observations (25th to 75th percentile) and the light shading represents the outer 50% of observations (0th to 100th percentile) of the daily averaged value for that same day in each of the 39 years of record

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['mean', 'std', 'min', 'max', '25%', '75%'],
    'solar': ['mean', 'std', 'min', 'max', '25%', '75%'],
    'other_renewable': ['mean', 'std', 'min', 'max', '25%', '75%'],
    'fossil_gas': ['mean', 'std', 'min', 'max', '25%', '75%'],
    'price': ['mean', 'std', 'min', 'max', '25%', '75%']
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['quarter', 'hour','country_code', 'wind_mean', 'wind_std', 'solar_mean', 'solar_std',
                   'renewable_mean', 'renewable_std', 'fossil_mean', 'fossil_std', 'price_mean', 'price_std']

# Generate the mean - std and mean + std columns
for col in grouped.columns:
    if 'mean' in col:
        grouped[col.replace('mean', 'min')] = grouped[col] - grouped[col.replace('mean', 'std')]
        grouped[col.replace('mean', 'max')] = grouped[col] + grouped[col.replace('mean', 'std')]

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', 'wind_min', 'wind_max', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/261:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
print(grouped.head())

# Generate the mean - std and mean + std columns
for col in grouped.columns:
    if 'mean' in col:
        grouped[col.replace('mean', 'min')] = grouped[col] - grouped[col.replace('mean', 'std')]
        grouped[col.replace('mean', 'max')] = grouped[col] + grouped[col.replace('mean', 'std')]

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_mean', color='blue')
g.map(plt.fill_between, 'hour', 'wind_min', 'wind_max', alpha=0.1, color='blue')
g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/262:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'median', color='blue')
g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/263:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
print(grouped.columns)
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'median', color='blue')
g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/264:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns if col[1] != '']
print(grouped.columns)
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'median', color='blue')
g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/265:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns if col[1] != '']
print(grouped.columns)
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'median', color='blue')
g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/266:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
print(grouped.columns)
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code_', col='quarter_', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'median', color='blue')
g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/267:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'median', color='blue')
g.map(plt.fill_between, 'hour', 'q25', 'q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'min', 'max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/268:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshoremedian', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'wwind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/269:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.5, color='blue')
g.map(plt.fill_between, 'hour', 'wwind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/270:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.5)
# 75th Percentile
def q75(x):
    return x.quantile(0.9)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/271:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_mean', color='yellow')
#g.map(plt.fill_between, 'hour', grouped['solar_mean'] - grouped['solar_std'], grouped['solar_mean'] + grouped['solar_std'], alpha=0.1, color='yellow')

#g.map(plt.plot, 'hour', 'renewable_mean', color='green')
#g.map(plt.fill_between, 'hour', 'renewable_mean' - grouped['renewable_std'], 'renewable_mean' + grouped['renewable_std'], alpha=0.1, color='green')

g.map(plt.plot, 'hour', 'fossil_mean', color='red')
#g.map(plt.fill_between, 'hour', grouped['fossil_mean']  - grouped['fossil_std'], grouped['fossil_mean'] + grouped['fossil_std'], alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_mean', color='black')
#g.map(plt.fill_between, 'hour', grouped['price_mean'] - grouped['price_std'], grouped['price_mean']  + grouped['price_std'], alpha=0.1, color='black')

# Set plot title, axis labels, and legend
g.fig
75/272:
# Merge the two dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_median', color='black')
g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
g.map(plt.twinx)
# Set plot title, axis labels, and legend
g.fig
75/273:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
75/274:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices,hourly_entsoe_day_ahead_forecast, on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_median', color='black')
g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
g.map(plt.twinx)
# Set plot title, axis labels, and legend
g.fig
75/275:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast, on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_median', color='black')
g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
g.map(plt.twinx)
# Set plot title, axis labels, and legend
g.fig
75/276:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

g.map(plt.plot, 'hour', 'price_median', color='black')
g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
g.map(plt.twinx)
# Set plot title, axis labels, and legend
g.fig
75/277:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/278:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})


# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/279:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
df['month'] = pd.DatetimeIndex(df['timestamp']).month


# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['month','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'month'})
grouped_months['quarter'] = None

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/280:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
df['month'] = pd.DatetimeIndex(df['timestamp']).month


# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['month','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'hour'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'month'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'hour', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'hour', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'hour', 'solar_median', color='orange')
g.map(plt.fill_between, 'hour', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'hour', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'hour', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'hour', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/281:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
df['month'] = pd.DatetimeIndex(df['timestamp']).month


# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['month','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/282:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
df['month'] = pd.DatetimeIndex(df['timestamp']).month


# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['month','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/283:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['month','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'month_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/284:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/285:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months])

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/286:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/287:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/288:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})



# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
print(grouped)
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/289:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Replace NaN values with 0
df = df.fillna(0)

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour

# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):
    # Return the 25th percentile as a float value
    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
75/290:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
77/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
77/2:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20,3))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
77/3:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
77/4:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
77/5:
# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
77/6:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
79/1:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
79/2:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
79/3:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
79/4:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
79/5:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
79/6:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
79/7:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
80/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
80/2:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
80/3:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
80/4:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)

# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
80/5:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
print(grouped.columns)
# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
81/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
81/2:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
81/3:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
81/4:
# Print numpy version
print(np.__version__)
import seaborn as sns
print(sns.__version__)
81/5:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
print(grouped.columns)
# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
81/6:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Convert median,std,min,max,q25,q75 columns to float64
for col in grouped.columns[3:]:
    grouped[col] = grouped[col].astype('float64')
# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
81/7:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Convert median,std,min,max,q25,q75 columns to float64
for col in grouped.columns[3:]:
    grouped[col] = grouped[col].astype('float')
# Replace NaN values with 0
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
81/8:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
81/9:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
81/10:
# Print numpy version
print(np.__version__)
import seaborn as sns
print(sns.__version__)
82/1:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
82/2:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
82/3:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
82/4:
# Print numpy version
print(np.__version__)
import seaborn as sns
print(sns.__version__)
82/5:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Convert median,std,min,max,q25,q75 columns to float64
for col in grouped.columns[3:]:
    grouped[col] = grouped[col].astype('float')
# Replace NaN values with 0 np.nan
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/6:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Convert median,std,min,max,q25,q75 columns to float64
for col in grouped.columns[3:]:
    grouped[col] = grouped[col].astype('float')
# Replace NaN values with 0 np.nan
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/7:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 0

# Concatenate the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Convert median,std,min,max,q25,q75 columns to float64
for col in grouped.columns[3:]:
    grouped[col] = grouped[col].astype('float')
# Replace NaN values with 0 np.nan
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped_months, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/8:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Bind the two dataframes
grouped = pd.concat([grouped_quarters, grouped_months], axis=0)
# Convert median,std,min,max,q25,q75 columns to float64
for col in grouped.columns[3:]:
    grouped[col] = grouped[col].astype('float')
# Replace NaN values with 0 np.nan
grouped = grouped.fillna(0)

# Set up the plot
g = sns.FacetGrid(grouped, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/9:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE'])]

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/10:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', height=3, aspect=2, margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/11:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
#df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/12:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
#g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
#g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/13:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'hour', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'hour', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/14:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
#for col in hourly_entsoe_generation_production.columns[3:]:
#    df[col] = df[col] / df['forecasted_load']

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/15:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    #df[col] = df[col] / df['forecasted_load']
    # Take log
    df[col] = np.log(df[col])

# Create new columns for quarter and month
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))



# Group the data by quarter, month, and country_code, and calculate median and std
# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['quarter', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='quarter', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

#g.map(plt.plot, 'hour', 'price_median', color='black')
#g.map(plt.fill_between, 'hour', 'price_q25', 'price_q75', alpha=0.3, color='black')
#g.map(plt.fill_between, 'hour', 'price_min', 'price_max', alpha=0.1, color='black')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
82/16:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
82/17:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly.copy()
# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

grouped_months = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_': 'country', 'date_': 'x_label'})
grouped_months['season'] = 'complete'

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')
82/18:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly.copy()
# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

grouped_months = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_': 'country', 'date_': 'x_label'})
grouped_months['season'] = 'complete'

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'temperature_2m_median', color='blue')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='orange')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='orange')
82/19:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly.copy()
# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

grouped_months = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_': 'country', 'date_': 'x_label'})
grouped_months['season'] = 'complete'

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'temperature_2m_median', color='blue')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='orange')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='orange')
82/20: g.fig
82/21:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2019-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

grouped_months = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#c5d3ff')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#c5d3ff')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#c5d3ff')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts
g.fig.legend(labels=['Temperature 2m (C)', 'Wind speed 10m (m/s)', 'Wind gusts 10m (m/s)'])
# Add x axis label
g.fig.text(0.5, 0.01, 'Hour of day', ha='center', va='center')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/22:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2019-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

grouped_months = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#c5d3ff')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#c5d3ff')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#c5d3ff')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts
g.fig.legend(labels=['Temperature 2m (C)', 'Wind speed 10m (m/s)', 'Wind gusts 10m (m/s)'])
# Add x axis label
g.fig.text(0.5, 0.01, 'Hour of day', ha='center', va='center')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])

g.fig
82/23:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2022-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts on the right side in every subplot
for ax in g.axes.flat:
    ax.legend(labels=['Temperature 2m (C)', '25\% - 75\%', 'Min - Max',
                      'Wind speed 10m (m/s)', '25\% - 75\%', 'Min - Max',
                      'Wind gusts 10m (m/s)', '25\% - 75\%', 'Min - Max'
                      ], loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=3)
    
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])

g.fig
82/24:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2022-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
                      'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
                      'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max'
                      ]
g.fig.legend(labels=labels, loc='center right', bbox_to_anchor=(1.3, 0.5), ncol=3)
# Add margin titles for every row and column
g.set_titles(row_template='{row_name}', col_template='{col_name}')
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])

g.fig
82/25:
# Add legend below
g.fig.legend(labels=labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3)
g.fig
82/26:
# Add legend below
g.fig.legend(labels=labels, loc='lower center', ncol=3)
g.fig
82/27:
# Add legend below
g.fig.legend(labels=labels, loc='upper center', ncol=3)
g.fig
82/28:
# Add legend upper center outside of plot area
g.fig.legend(labels=labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.1))
g.fig
82/29:
# Add legend upper center outside of plot area
g.fig.legend(labels=labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0))
g.fig
82/30:
# Add legend upper center outside of plot area
g.fig.legend(labels=labels)
g.fig
82/31:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2022-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'winter' if x.month in [12,1,2] else ('spring' if x.month in [3,4,5] else ('summer' if x.month in [6,7,8] else 'fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=True, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/32:
g.fig.legend(labels=labels, ncol = 3)
g.fig
82/33:
g.fig.legend(labels=labels, ncol = 3)
# Set plot title, axis labels, and legend
g.fig.title('Weather')
g.fig.tight_layout()
g.fig
82/34:
g.fig.legend(labels=labels, ncol = 3)
# Set plot title, axis labels, and legend
g.fig.suptitle('Weather Temporal', fontsize=16)
g.fig.tight_layout()
g.fig
82/35:
g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{row_key}-{col_key}")
g.fig
82/36:
g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{row_key}: {col_key}", fontsize=12)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.5)
g.fig
82/37:
g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{TSO_COUNTRIES[row_key]}: {col_key}", fontsize=12)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
82/38:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2022-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=False, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/39:
# Weather Temporal


# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
g = sns.catplot(grouped_quarters, col='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/40:
g.fig.legend(labels=labels, ncol = 3, bbox_to_anchor=(0.5, 0.05))
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{TSO_COUNTRIES[row_key]}: {col_key}", fontsize=12)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
82/41:
g.fig.legend(labels=labels, ncol = 3)
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{TSO_COUNTRIES[row_key]}: {col_key}", fontsize=12)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
82/42:
# Weather Temporal


# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
g = sns.catplot(grouped, col='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/43:
# Weather Temporal


# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/44: g.fig
82/45:
# Weather Temporal


# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/46: g.fig
82/47: print(grouped['date'].head())
82/48: print(df['date'].head())
82/49: print(grouped)
82/50:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
grouped['t'] = 't'
g = sns.FacetGrid(grouped, row='country',col = 't' sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/51:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
grouped['t'] = 't'
g = sns.FacetGrid(grouped, row='country',col = 't', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
82/52: g.fig
82/53:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
grouped['t'] = 't'
g = sns.FacetGrid(grouped, row='country',col = 't', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
#g.set(xticks=[0,4,8,12,16,20])
82/54:

g.fig
82/55:
# Extend the graphs
g.fig.set_size_inches(15, 10)
g.fig
82/56:
# Extend the graphs
g.fig.set_size_inches(20, 15)
g.fig
82/57:
# Extend the graphs
g.fig.set_size_inches(20, 30)
g.fig
82/58:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.fig
82/59:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Convert the timestamp to month e.g. January, February, March
g.set_xticklabels(rotation=45, horizontalalignment='right')
for (row_key, col_key),ax in g.axes_dict.items():
    # Convert the timestamp to month e.g. January, February, March
    ax.set_xticklabels([dt.fromtimestamp(x).strftime('%B') for x in ax.get_xticks()])
82/60:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Convert the timestamp to month e.g. January, February, March
g.set_xticklabels(rotation=45, horizontalalignment='right')
for (row_key, col_key),ax in g.axes_dict.items():
    # Convert the timestamp to month e.g. January, February, March
    ax.set_xticklabels([x.strftime('%B') for x in ax.get_xticks()])
82/61:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Convert the timestamp to month e.g. January, February, March
g.set_xticklabels(rotation=45, horizontalalignment='right')
for (row_key, col_key),ax in g.axes_dict.items():
    # Convert the timestamp to month e.g. January, February, March
    ax.set_xticklabels([x.strftime('%B') for x in ax.get_xticks()])
82/62:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Convert the timestamp to month e.g. January, February, March
g.set_xticklabels(rotation=45, horizontalalignment='right')
for (row_key, col_key),ax in g.axes_dict.items():
    # Convert the timestamp to month e.g. January, February, March
    ax.set_xticklabels([pd.to_datetime(x).strftime('%B') for x in ax.get_xticks()])
82/63:
# Extend the graphs
g.fig.set_size_inches(20, 40)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
82/64:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
82/65:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([pd.to_datetime(x).strftime('%B') for x in g.get_xticks()])
g.fig
82/66:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([dt.strftime('%B') for dt in g.axes_dict[0,0].get_xticks()])
g.fig
82/67:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([dt.strftime('%B') for dt in g.axes_dict[0].get_xticks()])
g.fig
82/68:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,13)])
g.fig
82/69:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,12)])
g.fig
82/70:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,7)])
g.fig
82/71:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# SEt tick labels to January, February, March, etc.
g.set_xticklabels([dt.datetime(2000, x, 1).strftime('%B') for x in range(1,8)])
g.fig
82/72:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2000,x,1).strftime('%B') for x in range(12)])
g.fig
82/73:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2000,x,1).strftime('%B') for x in range(1,13)])
g.fig
82/74:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2000,x,1) for x in range(1,13)])
g.fig
82/75:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
g.fig
82/76:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Convert the timestamp to month e.g. January, February, March
for (row_key, col_key),ax in g.axes_dict.items():
    # Convert the timestamp to month e.g. January, February, March
    ax.set_xticklabels([pd.to_datetime(x).strftime('%B') for x in ax.get_xticks()])
82/77:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
g.fig
82/78:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
g.fig
82/79:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1).strftime('%B') for x in range(1,13)])
g.fig
82/80:
# Weather Temporal
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
82/81:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
g.fig
82/82:
# Weather Temporal
df = w_hourly
# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
82/83:
# Weather Temporal
df = w_hourly
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
82/84:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
g.fig
82/85:
# Weather Temporal
df = w_hourly
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
82/86:
# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
g.fig
82/87:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
82/88:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')
# Add the standard deviation as a black line
g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
82/89:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')
# Add the standard deviation as a black line
g.map(plt.plot, 'x_label', 'windspeed_10m_std', color='black')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
82/90:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'windspeed_100m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_100m_q25', 'windspeed_100m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_100m_min', 'windspeed_100m_max', alpha=0.1, color='#7877E6')
# Add the standard deviation as a black line
g.map(plt.plot, 'x_label', 'windspeed_100m_std', color='black')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
82/91:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
82/92:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
83/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
83/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
84/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
84/2:
# --- Request Blends ----
data = client.base_request(endpoint="api/blends")
print(data.json())
#df = pd.DataFrame.from_records(data.json())
87/1:
# --- Request Crudes ----
df = client.get_crudes()
print(df)
#df = pd.DataFrame.from_records(data.json())
87/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
87/3:
# --- Request Crudes ----
df = client.get_crudes()
print(df)
#df = pd.DataFrame.from_records(data.json())
87/4: df
87/5: df.columns
87/6: df['ID']
87/7: df.columns
87/8: df['Name']
87/9: df.columns
87/10:
df.columns
df['intCrudeID']
87/11: crude = client.get_crude("A1RTP140")
87/12:
df.columns
df[['intCrudeID','Library']]
87/13: crude = client.get_crude(code="A1RTP140",library='CHEVRON')
87/14: crude = client.get_crude(code="A1RTP140",libary_name='CHEVRON')
88/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
88/2:
df.columns
df[['intCrudeID','Library']]
88/3: crude = client.get_crude(libary_name='CHEVRON',code="A1RTP140")
88/4: crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
88/5: crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
89/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
89/2: crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
89/3: crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
89/4:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
89/5: crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
90/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
90/2: crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
90/3:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crude
91/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
91/2:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crude
91/3:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crude
91/4:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
91/5:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crude
92/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
92/2:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crude
92/3:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude)
92/4:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude.attrs)
92/5:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude.attrs.cuts)
92/6:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude.attrs.Cuts)
92/7:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude.attrs.CutsProperties)
93/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
93/2:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude.attrs.CutsProperties)
93/3:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude)
94/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
94/2:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude)
95/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
95/2:
crude = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(crude)
95/3:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
print(summary)
95/4:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
summary
96/1:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
summary
96/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
96/3:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
summary
96/4:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
summary, cuts
96/5:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
 cuts
96/6:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
cuts
97/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
98/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
98/2: client.get_specifications()
99/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
99/2: client.get_specifications()
100/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
100/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
100/3: client.get_specifications()
101/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
101/2: client.get_default_specifications()
102/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
102/2: client.get_default_specifications()
102/3: df_spec = client.get_default_specifications()
102/4: df_spec['product_name'].unique()
102/5:
df_spec = client.get_default_specifications()
df_spec
102/6:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
102/7:
df_recut = client.get_slate_fraction(cut_id=35331, slate_id=25917, tabular = True)
# To tabular
df_recut
103/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
103/2:
df_recut = client.get_slate_fraction(cut_id=35331, slate_id=25917, tabular = True)
# To tabular
df_recut
103/3:
df_recut = client.get_slate_fraction(cutset_id=35331, slate_id=25917, tabular = True)
# To tabular
df_recut
105/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
105/2:
# How to put into your own custom specs? Especially how do you combine the product with the product defined in ur cut_id?
df_spec = client.get_default_specifications()
df_spec
105/3:
df_recut = client.get_slate_fraction(cutset_id=35331, slate_id=25917, tabular = True)
# To tabular
df_recut
106/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
106/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)
# To tabular
df_recut
107/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
107/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)
# To tabular
df_recut
108/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
108/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)
# To tabular
df_recut
109/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
109/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050)
# To tabular
df_recut
109/3:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
109/4: df_recut.reset_index()
110/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
110/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
111/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
111/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
112/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
112/2:
# How to put into your own custom specs? Especially how do you combine the product with the product defined in ur cut_id?
df_spec = client.get_default_specifications()
df_spec
112/3:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
113/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
113/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
114/1:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
114/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
114/3:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
115/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
115/2:
df_recut = client.get_blend_fraction(cutset_id=35331, blend_id=14050, tabular=True)
# To tabular
df_recut
115/3:
# Get RON on row level
df_recut[df_recut['property'] == 'RON']
115/4:
# Fetch the RON on the index
df_recut.loc['RON']
115/5:
# Fetch all possible properties from df_recut
df_recut['property'].unique()
115/6:
# Fetch all possible properties from df_recut. All unique indices are the properties
df_recut.index.unique()
115/7:
# Fetch all possible properties from df_recut. All unique indices are the properties
print(df_recut.index.unique())
115/8:
# Fetch all possible properties from df_recut. All unique indices are the properties
print(df_recut.index.unique())
# Fetch RONC
df_recut.loc['RONC']
115/9:
# Fetch all crude data
crudes = client.get_crudes()
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crudes
115/10:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
115/11:
# Fetch all crude data
crudes = client.get_crudes()
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
crudes
115/12:
crudes = client.get_crudes()
print(crudes.columns)
crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])
crudes
115/13:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
cuts
115/14:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
summary
115/15:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140")
cuts
115/16:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD
from hcomet.templates import *

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
115/17:
# Transform the crude into a blend with 100% of the crude
from hcomet import templates
crudes = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend = templates.blend_template(crudes)
116/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
116/2:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140", tabular = True)
cuts
117/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
117/2:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140", tabular = True)
cuts
118/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
118/2:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140", tabular = True)
cuts
119/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
119/2:
summary, cuts = client.get_crude(library_name='CHEVRON',code="A1RTP140", tabular = True)
cuts
119/3:
from hcomet import templates
crudes = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
# Transform the crude into a blend with 100% of the crude
blend_name = 'test_blend'
blend_template = templates.blend_template(crudes, blend_name)
# Get the new cuts
cutset_name = 'test_cutset'
cutset_template = templates.cutset_template(cutset, cutset_name)

# Create the blend
res = client.post_blend(blend_template, blend_name)
print(res)
# Create the cut
res = client.put_cutset(cutset_template, cutset_name)
print(res)
119/4:
from hcomet import templates
crudes = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
# Transform the crude into a blend with 100% of the crude
blend_name = 'test_blend'
blend_template = templates.blend_template(crudes, blend_name)
# Get the new cuts
cutset_name = 'test_cutset'
cutset_template = templates.cutset_template(cutset, cutset_name)

# Create the blend
res = client.post_blend(blend_template)
print(res)
# Create the cut
res = client.post_blend(cutset_template)
print(res)
119/5:
from hcomet import templates
crudes = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
# Transform the crude into a blend with 100% of the crude
blend_name = 'test_blend'
blend_template = templates.blend_template(crudes, blend_name)
# Get the new cuts
cutset_name = 'test_cutset'
cutset_template = templates.cutset_template(cutset, cutset_name)

# Create the blend
res = client.post_blend(blend_template)
print(res)
# Create the cut
res = client.post_blend(cutset_template)
print(res.text)
119/6:
from hcomet import templates
crudes = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
# Transform the crude into a blend with 100% of the crude
blend_name = 'test_blend'
blend_template = templates.blend_template(crudes, blend_name)
# Get the new cuts
cutset_name = 'test_cutset'
cutset_template = templates.cutset_template(cutset, cutset_name)

# Create the blend
res = client.post_blend(blend_template)
print(res.text)
# Create the cut
res = client.post_cutset(cutset_template)
print(res.text)
119/7:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
120/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
120/2:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
120/3:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
120/4:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends())
# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
120/5:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends())
# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
121/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
121/2:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends())
# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
122/2:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends())
# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/3:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends().head())
# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/4:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends().head())
print(client.get_cutsets().head())

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/5:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends().head())
print(client.get_cutsets().columns)

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/6:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends().head())
print(client.get_cutsets()['name','id'])

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/7:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends().head())
print(client.get_cutsets()[['name','id']])

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/8:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(client.get_blends()[['name','id']])
print(client.get_cutsets()[['name','id']])

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/9:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

print(blend_id, cutset_id)
122/10:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Existing blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Existing cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

# Recut blend based on cutset
df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)

# Delete blend and cutset
client.delete_blend(blend_id)
client.delete_cutset(cutset_id)
122/11:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Existing blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Existing cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

# Recut blend based on cutset
df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)

# Delete blend and cutset
client.delete_blend(blend_id)
client.delete_cutset(cutset_id)

df_recut
123/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
123/2:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Existing blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Existing cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

# Recut blend based on cutset
df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)

# Delete blend and cutset
client.delete_blend(blend_id)
client.delete_cutset(cutset_id)

df_recut
124/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
124/2:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Existing blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Existing cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

# Recut blend based on cutset
df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)

# Delete blend and cutset
client.delete_blend(blend_id)
client.delete_cutset(cutset_id)

df_recut
125/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
125/2:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Existing blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Existing cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

# Recut blend based on cutset
df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)

# Delete blend and cutset
client.delete_blend(blend_id)
client.delete_cutset(cutset_id)

df_recut
125/3:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
125/4:
from hcomet import templates
blend = [
    {'Percent': 100, 'Code': 'A1RTP140', 'Library': 'CHEVRON',  },
]
blend_name = 'test_blend'

cutset = [
        {'Name': 'GAS', 'IBP': -9999,'EBP' : 15},
        {"Name" : 'LN', "IBP" : 15, "EBP" : 85},
        {"Name" : 'HN', "IBP" : 85, "EBP" : 175},
        {"Name" : 'JET', "IBP" : 175, "EBP" : 272},
        {"Name" : 'GO', "IBP" : 272, "EBP" : 325},
        {"Name" : 'RES', "IBP" : 250, "EBP" : 9999},
    ]
cutset_name = 'test_cutset'

print(f"""Existing blends:\n{client.get_blends()[['name','id']]}""")
print(f"""Existing cutsets:\n{client.get_cutsets()[['name','id']]}""")

# Submit blend and cutset
blend_id = client.submit_blend(blend, blend_name)
cutset_id = client.submit_cutset(cutset, cutset_name)

# Recut blend based on cutset
df_recut = client.get_blend_fraction(blend_id = blend_id, cutset_id = cutset_id, tabular = True)

# Delete blend and cutset
client.delete_blend(blend_id)
client.delete_cutset(cutset_id)

df_recut
125/5:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
125/6:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
125/7:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
126/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
126/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
127/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
127/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
127/3:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
128/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
128/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
128/3:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
129/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
129/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
130/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
130/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
131/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
131/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
132/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
132/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
133/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
133/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
134/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
134/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
135/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
135/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
136/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
136/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
client.post_cru_files(LIBRARY,FILES)
136/3:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
137/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
137/2:
FILES = ["C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru","C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru"]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
138/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
138/2:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        "C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
138/3:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        "C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "FLASH_200980 "
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
138/4:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        "C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
138/5:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
138/6:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "flash_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
138/7:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
139/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
139/2:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "FLASH_200980"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
139/3:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
140/1:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
140/2:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
140/3:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
141/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
141/2:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
142/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
142/2:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        "C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        #"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
142/3:
FILES = [
        #"C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        #"C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        "C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        "C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
142/4:
FILES = [
        "C:/Users/Nicky/Downloads/EBO00_ASSAY_EBO1810A.cru",
        "C:/Users/Nicky/Downloads/DUD00_ASSAY_DUD1310A.cru",
        #"C:/Users/Nicky/Downloads/AUS00_FLASH_AUS2209A.cru",
        #"C:/Users/Nicky/Downloads/CLA02_FLASH_CLA2207U.cru",
        ]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
142/5:
FILES = ["C:/Users/Nicky/Downloads/AUS00 FLASH AUS2209A.cru"]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
143/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
143/2:
FILES = ["C:/Users/Nicky/Downloads/AUS00 FLASH AUS2209A.cru"]
# Our library name (FLASH_200980 doesn't work...)
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
143/3:
crudes = client.get_crudes()
print(crudes.columns)
crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])
crudes
143/4:
assay_names = [file.split("/")[-1] for file in FILES]
assay_names
143/5:
assay_names = [file.split("/")[-1] for file in FILES]
mask = crudes['name'].isin(assay_names)
crudes[mask]
143/6:
assay_names = [file.split("/")[-1] for file in FILES]
mask = crudes['id'].isin(assay_names)
crudes[mask]
143/7:
assay_names = [file.split("/")[-1] for file in FILES]
print(assay_names)
mask = crudes['id'].isin(assay_names)
crudes[mask]
143/8:
assay_names = [file.split("/")[-1].str.remove('.cru') for file in FILES]
print(assay_names)
mask = crudes['id'].isin(assay_names)
crudes[mask]
143/9:
assay_names = [file.split("/")[-1].remove('.cru') for file in FILES]
print(assay_names)
mask = crudes['id'].isin(assay_names)
crudes[mask]
143/10:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['id'].isin(assay_names)
crudes[mask]
143/11:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['name'].isin(assay_names)
crudes[mask]
143/12:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['library']==LIBRARY
crudes[mask]
143/13:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['library']==LIBRARY
print(crudes[mask])
143/14:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask])
144/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
144/2:
crudes = client.get_crudes()
print(crudes.columns)
crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])
crudes
145/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
145/2:
crudes = client.get_crudes()
print(crudes.columns)
crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])
crudes
146/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
146/2:
crudes = client.get_crudes()
print(crudes.columns)
crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])
crudes
146/3:
crudes = client.get_crudes()
print(crudes.columns)
#crudes = crudes.drop(columns = ['fixed_cut_filter_values','fixed_cut_filter_values_calc','dynamic_cut_filter_values','wc_properties'])
crudes
146/4:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask])
146/5:
FILES = ["C:/Users/Nicky/Downloads/AUS00 FLASH AUS2209A.cru"]
# TODO: Our library name (FLASH_200980 doesn't work...). Therefore, we need to request support from Haverly
LIBRARY = "VITOL"
res = client.post_cru_files(LIBRARY,FILES)
print(res.text)
146/6:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask])
146/7:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['int_crude_id'])
146/8:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['id'])
146/9:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['name'])
146/10:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['int_crude_id'])
146/11:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['code'])
146/12:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['cru_file_source'])
146/13:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
print(crudes[mask]['properties'])
146/14:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
crudes[mask]['properties'].values[0]
146/15:
assay_names = [file.split("/")[-1].split(".")[0] for file in FILES]
print(assay_names)
mask = crudes['int_crude_id'].isin(assay_names)
pd.DataFrame(crudes[mask]['properties'].values[0])
146/16: slates_df = client.get_slates()
147/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
147/2:
slates_df = client.get_slates()
slates_df
147/3:
slates_df = client.get_slates()
blends_df = client.get_blends()
blends_df
147/4:
slates_df = client.get_slates()
blends_df = client.get_blends()
print(f"""Blends: {blends_df.head()}""")

blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}: {blend}""")
147/5:
slates_df = client.get_slates()
blends_df = client.get_blends()
print(f"""Blends: {blends_df.head()}""")

blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}: {blend}""")
148/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
148/2:
slates_df = client.get_slates()
blends_df = client.get_blends()
print(f"""Blends: {blends_df.head()}""")

blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}: {blend}""")
148/3:
slates_df = client.get_slates()
blends_df = client.get_blends()
print(f"""Blends:\n{blends_df.head()}""")

blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend}""")
148/4:
slates_df = client.get_slates()
blends_df = client.get_blends()

# --- Blends ---
print(f"""Blends:\n{blends_df.head()}""")
blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend}""")
# --- Slates ---
print(f"""Blends:\n{slates_df.head()}""")
#blend_id = "46846" # test_vpr
#blend = client.get_blend(blend_id)
#print(f"""Blend {blend_id}:\n{blend}""")
148/5:
slates_df = client.get_slates()
blends_df = client.get_blends()

# --- Blends ---
print(f"""Blends:\n{blends_df.head()}""")
blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend}""")
# --- Slates ---
print(f"""Slates:\n{slates_df.head()}""")
slate_id = "25955" # China_Coking_XMB
slate = client.get_slate(slate_id)
print(f"""Slate {slate_id}:\n{slate}""")
148/6:
slates_df = client.get_slates()
blends_df = client.get_blends()

# --- Blends ---
print(f"""Blends:\n{blends_df.head()}""")
blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend}""")
# --- Slates ---
print(f"""Slates:\n{slates_df.head()}""")
slate_id = "26005" # China_Coking_XMB
slate = client.get_slate(slate_id)
print(f"""Slate {slate_id}:\n{slate}""")
149/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
149/2:
slates_df = client.get_slates()
blends_df = client.get_blends()

# --- Blends ---
print(f"""Blends:\n{blends_df.head()}""")
blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend}""")
# --- Slates ---
print(f"""Slates:\n{slates_df.head()}""")
slate_id = "26005" # China_Coking_XMB
slate = client.get_slate(slate_id)
print(f"""Slate {slate_id}:\n{slate}""")
149/3:
slates_df = client.get_slates()
blends_df = client.get_blends()

# --- Blends ---
print(f"""Blends:\n{blends_df.head()}""")
blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend.head()}""")
# --- Slates ---
print(f"""Slates:\n{slates_df.head()}""")
slate_id = "26005" # China_Coking_XMB
slate = client.get_slate(slate_id)
print(f"""Slate {slate_id}:\n{slate.head()}""")
150/1:
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import  HAVERLY_USERNAME, HAVERLY_PASSWORD

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
150/2:
slates_df = client.get_slates()
blends_df = client.get_blends()

# --- Blends ---
print(f"""Blends:\n{blends_df.head()}""")
blend_id = "46846" # test_vpr
blend = client.get_blend(blend_id)
print(f"""Blend {blend_id}:\n{blend.head()}""")
# --- Slates ---
print(f"""Slates:\n{slates_df.head()}""")
slate_id = "26005" # China_Coking_XMB
slate = client.get_slate(slate_id)
print(f"""Slate {slate_id}:\n{slate.head()}""")
150/3:
# --- Imports----
import sys
import pandas as pd
sys.path.append("..") # Adds higher directory to python modules path.
from hcomet.hcomet import HcometClient
from secret import *
import psycopg2

# --- Set up proxies ----
#proxies = {"http": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080", "https": f"http://{PROXY_USER}:{PROXY_PASS}@rdmproxy.vitol.com:8080"}
proxies = {}
# --- Set up Client ----
client = HcometClient(username = HAVERLY_USERNAME, password = HAVERLY_PASSWORD, proxies = proxies)
# --- Database retrieval functions ----
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
151/1:
g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{TSO_COUNTRIES[row_key]}: {col_key}", fontsize=12)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
151/2:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2022-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=False, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
151/3:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
151/4:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    [location_eindhoven],
    weather_variables=w_variables)

w_hourly
151/5:
# Create a graph of the weather data using matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20,3))
ax.plot(w_hourly['time'], w_hourly['temperature_2m'], label='Temperature')
ax.plot(w_hourly['time'], w_hourly['windspeed_10m'], label='Wind Speed')
ax.set_xlabel('Time')
ax.set_ylabel('Temperature (C), Wind Speed (m/s)')
ax.set_title('Weather Data for Eindhoven')
ax.legend()
plt.show()
151/6:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
151/7:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
151/8:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    #df[col] = df[col] / df['forecasted_load']
    # Take log
    df[col] = np.log(df[col])

# Create new columns for quarter and month
df['season'] = df['timestamp'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='season', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
151/9:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
151/10:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
151/11:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='windspeed_10m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Wind Speed (m/s)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
plt.show()
151/12:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
#df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.5)
plt.show()
151/13:
# Investigate the distribution of wind speed for each city
df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Wind Speed (m/s)')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
    # Xaxis max of 50 m/s
    axes[i].set_xlim(0, 50)
# Add legend outside the plot
fig.subplots_adjust(hspace=0.5)
plt.show()
151/14:
# Check whether the data is lognormally distributed, take the log of the data and plot the distribution
df = w_hourly.copy()
#df = df[df['country'] == 'NL']
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('D', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'
# Take log of wind speed (when wind speed is 0, take the log of 0.01)
df['log_windspeed_10m'] = np.log(df['windspeed_10m'].apply(lambda x: x if x > 0 else 0.01))

def move_legend(ax, new_loc, **kws):
    old_legend = ax.legend_
    handles = old_legend.legendHandles
    labels = [t.get_text() for t in old_legend.get_texts()]
    _title = old_legend.get_title().get_text()
    # Check if title in **kws
    if 'title' in kws:
        _title = kws['title']
        del kws['title']
    ax.legend(handles, labels, loc=new_loc, title=_title, **kws)
    
fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=True)
for i, region in enumerate(df['region'].unique()):
    
    sns.histplot(data=df[df['region'] == region], x='log_windspeed_10m', hue='label', ax=axes[i], bins=50, alpha = 0.5)
    axes[i].set_title(region)
    axes[i].set_ylabel('Frequency')
    # Show xlabel for all plots
    # Show the xticks for all plots
    axes[i].xaxis.set_tick_params(which='both', labelbottom=True)
    axes[i].set_xlabel('Logarithmic Wind Speed')
    # Show legend despite No artists with labels found to put in legend.  Manually add the legend values
    move_legend(axes[i], 2, ncol=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., title = '')
# Add legend outside the plot
fig.subplots_adjust(hspace=0.5)
plt.show()
151/15:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
plt.show()
151/16:
import seaborn as sns
from matplotlib import pyplot as plt

df = hourly_entsoe_day_ahead_prices.copy()
df = df.groupby(['country_code', 'price_area']).resample('M', on='timestamp').mean().reset_index()

fig, axes = plt.subplots(1, 1, figsize=(12, 5), sharex=False)
i=0
sns.lineplot(data=df, x='timestamp', y='price', hue='price_area', ax=axes)
# Smaller line width
for j in range(0, len(axes.lines)):
    axes.lines[j].set_linewidth(1.5)
    axes.lines[j].set_alpha(0.5)
axes.set_title('Day Ahead Prices')
# Set axis labels to '' to avoid overlapping
axes.set_ylabel('Price (EUR/MWh)')
axes.set_xlabel('')
# Rotate xticks
axes.tick_params(axis='x', rotation=45)
# Legends outside the plot
axes.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
# Create more space between the plots
fig.subplots_adjust(hspace=0.5)
151/17:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    #df[col] = df[col] / df['forecasted_load']
    # Take log
    df[col] = np.log(df[col])

# Create new columns for quarter and month
df['season'] = df['timestamp'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'quarter_': 'quarter', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='season', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
151/18:
import seaborn as sns
from matplotlib import pyplot as plt

# Merge the three dataframes
df = pd.merge(hourly_entsoe_generation_production, hourly_entsoe_day_ahead_prices, on=['timestamp', 'country_code', 'price_area'])
df = pd.merge(df, hourly_entsoe_day_ahead_forecast[['timestamp', 'price_area', 'country_code', 'forecasted_load']], on=['timestamp', 'country_code', 'price_area'])
# Select only NL and DE
df = df[df['country_code'].isin(['NL', 'DE', 'FR', 'BE', 'LU', 'ES', 'PT'])]

# Sum over price_area
df = df.groupby(['timestamp', 'country_code']).sum().reset_index()

# Divide the columns from hourly_entsoe_generation_production with the forecasted_load
for col in hourly_entsoe_generation_production.columns[3:]:
    #df[col] = df[col] / df['forecasted_load']
    # Take log
    df[col] = np.log(df[col])

# Create new columns for quarter and month
df['season'] = df['timestamp'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['quarter'] = pd.DatetimeIndex(df['timestamp']).quarter
df['hour'] = pd.DatetimeIndex(df['timestamp']).hour
# Group the data by date
df['date'] = df['timestamp'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

grouped_months = df.groupby(['date','country_code']).agg({
    'wind_onshore': ['median', 'std', 'min', 'max', q25, q75],
    'solar': ['median', 'std', 'min', 'max', q25, q75],
    'other_renewable': ['median', 'std', 'min', 'max', q25, q75],
    'fossil_gas': ['median', 'std', 'min', 'max', q25, q75],
    'price': ['median', 'std', 'min', 'max', q25, q75]
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_code_': 'country_code', 'season_': 'season', 'hour_': 'x_label'})

grouped_months = grouped_months.reset_index()
grouped_months.columns = ['_'.join(col) for col in grouped_months.columns]
# Rename country_code_ to country_code
grouped_months = grouped_months.rename(columns={'country_code_': 'country_code', 'date_': 'x_label'})
grouped_months['quarter'] = 5

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country_code', col='season', margin_titles=True, sharex=False, sharey=False)

# Plot wind, solar, renewable, fossil, and price
g.map(plt.plot, 'x_label', 'wind_onshore_median', color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_q25', 'wind_onshore_q75', alpha=0.3, color='blue')
g.map(plt.fill_between, 'x_label', 'wind_onshore_min', 'wind_onshore_max', alpha=0.1, color='blue')

g.map(plt.plot, 'x_label', 'solar_median', color='orange')
g.map(plt.fill_between, 'x_label', 'solar_q25', 'solar_q75', alpha=0.3, color='orange')
g.map(plt.fill_between, 'x_label', 'solar_min', 'solar_max', alpha=0.1, color='orange')

g.map(plt.plot, 'x_label', 'other_renewable_median', color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_q25', 'other_renewable_q75', alpha=0.3, color='green')
g.map(plt.fill_between, 'x_label', 'other_renewable_min', 'other_renewable_max', alpha=0.1, color='green')
 
g.map(plt.plot, 'x_label', 'fossil_gas_median', color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_q25', 'fossil_gas_q75', alpha=0.3, color='red')
g.map(plt.fill_between, 'x_label', 'fossil_gas_min', 'fossil_gas_max', alpha=0.1, color='red')

# Double axis for price
# Set plot title, axis labels, and legend
g.fig
151/19:
# Weather Temporal
import seaborn as sns
from matplotlib import pyplot as plt
df = w_hourly[w_hourly['time'] > '2022-01-01']

# Average over country
df = df.groupby(['time', 'country']).mean().reset_index()

# Create new columns for season and month (EU in same hemisphere)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# 25th Percentile
def q25(x):    
    return x.quantile(0.25)
# 75th Percentile
def q75(x):
    return x.quantile(0.75)

# Get mean, std, 25th percentile, 75th percentile, min, and max
grouped_quarters = df.groupby(['season', 'hour','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75],
})

# Reset index and rename columns
grouped_quarters = grouped_quarters.reset_index()
grouped_quarters.columns = ['_'.join(col) for col in grouped_quarters.columns]
# Rename country_code_ to country_code
grouped_quarters = grouped_quarters.rename(columns={'country_': 'country', 'season_': 'season', 'hour_': 'x_label'})

# Set up the plot
g = sns.FacetGrid(grouped_quarters, row='country', col='season', margin_titles=False, sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
# Show 0 to 24 in x axis in 0,4,8,12,16,20,24
g.set(xticks=[0,4,8,12,16,20])
151/20:
g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Set plot title, axis labels, and legend
g.fig.tight_layout()
for (row_key, col_key),ax in g.axes_dict.items():
    ax.set_title(f"{TSO_COUNTRIES[row_key]}: {col_key}", fontsize=12)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.25)
g.fig
151/21:
# Weather Temporal
df = w_hourly
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x, roll_length=24):
    return x.rolling(24).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/22:
# Weather Temporal
df = w_hourly
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x, roll_length=24):
    return x.rolling(24).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24* 7)],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x: rolling_std(x, 24 * 7)],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/23:
# Weather Temporal
df = w_hourly
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x, roll_length=24*7):
    return x.rolling(24).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/24:
# Weather Temporal 
df = w_hourly.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x, roll_length=24*7):
    return x.rolling(24).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/25:
# Weather Temporal 
df = w_hourly.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x):
    return x.rolling(24*7).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/26:
# Weather Temporal 
df = w_hourly.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x):
    return x.rolling(24*7).std().mean()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/27:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')

# Extend the graphs
g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/28:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')

# Extend the graphs
#g.fig.set_size_inches(20, 40)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/29:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')

# Extend the graphs
g.fig.set_size_inches(20)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/30:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')

# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/31:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
print(grouped)
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/32:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
print(grouped.tail())
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/33:
# Weather Temporal 
df = w_hourly.head(50000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x):
    return x.rolling(24*7).std().mean()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/34:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
print(grouped.tail())
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/35:
# Weather Temporal 
df = w_hourly.head(50000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x):
    return x.rolling(24*7).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/36:
# Weather Temporal 
df = w_hourly.head(50000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std
def rolling_std(x):
    return x.rolling(24).std()


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/37:
# Weather Temporal 
df = w_hourly.head(50000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std (not aggregate function)
def rolling_std(x):
    return x.rolling(24).std().iloc[-1]


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/38:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
print(grouped.tail())
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/39:
# Weather Temporal 
df = w_hourly.head(50000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std (not aggregate function)
def rolling_std(x):
    return x.rolling(24*7).std().iloc[-1] # -1 to get the last value


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/40:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
print(grouped.tail())
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/41:
# Weather Temporal 
df = w_hourly.head(500000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std (not aggregate function)
def rolling_std(x):
    return x.rolling(24*7).std().iloc[-1] # -1 to get the last value


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/42:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
print(grouped.tail())
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/43:
# Weather Temporal 
df = w_hourly.head(5000000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
df['hour'] = pd.DatetimeIndex(df['time']).hour
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))

# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std

# Rolling std (not aggregate function)
def rolling_std(x):
    return x.rolling(24*7).std().iloc[-1] # -1 to get the last value


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/44:
g = sns.FacetGrid(grouped, row='country', sharex=False, sharey=False)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/45:
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2 )

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])

# Remove temperature_2m_median 

g.fig
151/46:
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2 )

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.4)
# Convert to 12 ticks in x axis
g.set(xticks=[dt.datetime(2020,x,1) for x in range(1,13)])
151/47:
# Manipulate
# Add average temperature to daily, max + min / 2
w_daily['temperature_2m_avg'] = (w_daily['temperature_2m_max'] + w_daily['temperature_2m_min']) / 2
151/48:
# Weather Temporal 
df = w_daily.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))
# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std
# Rolling std (not aggregate function)
def rolling_std(x, window=7):
    return x.rolling(window).std().iloc[-1] # -1 to get the last value

grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/49:
# Weather Temporal 
df = w_daily.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))
# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std
# Rolling std (not aggregate function)
def rolling_std(x, window=7):
    return x.rolling(window).std().iloc[-1] # -1 to get the last value

grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/50:
# Weather Temporal 
df = w_hourly.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))
# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std
# Rolling std (not aggregate function)
def rolling_std(x, window=7):
    return x.rolling(window).std().iloc[-1] # -1 to get the last value

grouped = df.groupby(['date','country']).agg({
    'temperature_2m_avg': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/51:
# Weather Temporal 
df = w_hourly.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))
# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std
# Rolling std (not aggregate function)
def rolling_std(x, window=7):
    return x.rolling(window).std().iloc[-1] # -1 to get the last value

grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, lambda x : rolling_std(x, window=7), lambda x : rolling_std(x, window=30)],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/52: print(df.columns)
151/53: print(grouped.columns)
151/54:
# Weather Temporal 
df = w_hourly.head(5000)
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))
# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std
# Rolling std (not aggregate function)
def rolling_std(x, window=7):
    return x.rolling(window).std().iloc[-1] # -1 to get the last value

def rolling_week_std(x):
    return rolling_std(x, window=7)

def rolling_month_std(x):
    return rolling_std(x, window=30)


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75,  rolling_week_std, rolling_month_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/55:
# Weather Temporal 
df = w_hourly
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))
# Group the data by date
df['date'] = df['time'].dt.date
# Change year of date to 2020
df['date'] = df['date'].apply(lambda x: x.replace(year=2020))
# Get mean, std, ,25th percentile, 75th percentile, min, and max, rolling std
# Rolling std (not aggregate function)
def rolling_std(x, window=7):
    return x.rolling(window).std().iloc[-1] # -1 to get the last value

def rolling_week_std(x):
    return rolling_std(x, window=7)

def rolling_month_std(x):
    return rolling_std(x, window=30)


grouped = df.groupby(['date','country']).agg({
    'temperature_2m': ['median', 'std', 'min', 'max', q25, q75,  rolling_week_std, rolling_month_std],
    'windspeed_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],
    'windspeed_100m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],
    'windgusts_10m': ['median', 'std', 'min', 'max', q25, q75, rolling_week_std, rolling_month_std],
})
# Reset index and rename columns
grouped = grouped.reset_index()
grouped.columns = ['_'.join(col) for col in grouped.columns]
# Rename country_code_ to country_code
grouped = grouped.rename(columns={'country_': 'country', 'date_': 'x_label'})
# Convert date to datetime
#grouped['x_label'] = pd.to_datetime(grouped['x_label'])

# Set up the plot (for each country)
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap = 2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

# Wind gusts (#0D5060)
#g.map(plt.plot, 'x_label', 'windgusts_10m_median', color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_q25', 'windgusts_10m_q75', alpha=0.3, color='#0D5060')
#g.map(plt.fill_between, 'x_label', 'windgusts_10m_min', 'windgusts_10m_max', alpha=0.1, color='#0D5060')

# Add legend for temperature, wind speed, and wind gusts. Only one legend for all plots
labels=['Temperature 2m (C)', '25% - 75%', 'Min - Max',
        'Wind speed 10m (m/s)', '25% - 75%', 'Min - Max',
        'Wind gusts 10m (m/s)', '25% - 75%', 'Min - Max']
# Remove y and x names
g.set_axis_labels('', '')
151/56:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2 )

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_week_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(20, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.5)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%b")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"{TSO_COUNTRIES[row_key]}: {col_key}", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
151/57:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=3)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_week_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(vspace=0.35)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"{TSO_COUNTRIES[col_key]}", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Crease size between plots on same row
151/58:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=3)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_rolling_week_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"{TSO_COUNTRIES[col_key]}", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Crease size between plots on same row
151/59:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=3)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"{TSO_COUNTRIES[col_key]}", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Crease size between plots on same row
151/60:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.35)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"""{TSO_COUNTRIES[f"{row_key}{col_key}"]}""", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels, ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Crease size between plots on same row
151/61:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.6)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"""{TSO_COUNTRIES[f"{row_key}{col_key}"]}""", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels[0:3], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Decrease the space between the plots horizontally
g.fig.subplots_adjust(wspace=0.1)
151/62:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=2)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=0.6)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"""{TSO_COUNTRIES[f"{row_key}{col_key}"]}""", fontsize=12)
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels[0:3], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Decrease the space between the plots horizontally
g.fig.subplots_adjust(wspace=0.1)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=1)
151/63:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=4)

g.map(plt.plot, 'x_label', 'temperature_2m_median', color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_q25', 'temperature_2m_q75', alpha=0.3, color='#E28C1F')
g.map(plt.fill_between, 'x_label', 'temperature_2m_min', 'temperature_2m_max', alpha=0.1, color='#E28C1F')

#g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"""{TSO_COUNTRIES[f"{row_key}{col_key}"]}""")
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels[0:3], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Decrease the space between the plots horizontally
g.fig.subplots_adjust(wspace=0.1)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=1)
151/64:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=4)

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10mq75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

#g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"""{TSO_COUNTRIES[f"{row_key}{col_key}"]}""")
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels[3:6], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Decrease the space between the plots horizontally
g.fig.subplots_adjust(wspace=0.1)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=1)
151/65:
from matplotlib.dates import DateFormatter
g = sns.FacetGrid(grouped, col='country', sharex=False, sharey=False, col_wrap=4)

g.map(plt.plot, 'x_label', 'windspeed_10m_median', color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_q25', 'windspeed_10m_q75', alpha=0.3, color='#7877E6')
g.map(plt.fill_between, 'x_label', 'windspeed_10m_min', 'windspeed_10m_max', alpha=0.1, color='#7877E6')

#g.map(plt.plot, 'x_label', 'temperature_2m_std', color='black')
# Extend the graphs horizontally
g.fig.set_size_inches(30, 10)
g.set_xticklabels(rotation=0)
# Add custom formatter (January, February, etc.)
formatter = DateFormatter("%B")
for (row_key, col_key),ax in g.axes_dict.items():
    ax.xaxis.set_major_formatter(formatter)
    ax.set_title(f"""{TSO_COUNTRIES[f"{row_key}{col_key}"]}""")
    
# Drop xaxis label
g.set_axis_labels('', '')
# Add legend
g.fig.legend(labels=labels[3:6], ncol = 3,loc='upper center', bbox_to_anchor=(0.5, -0.05),)
# Decrease the space between the plots horizontally
g.fig.subplots_adjust(wspace=0.1)
# Increase the space between the plots vertically
g.fig.subplots_adjust(hspace=1)
151/66:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median'],
    'windspeed_10m_max': ['median'],
    'windgusts_10m_max': ['median'],
})

# Rename columns
summary_df.columns = ['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Reset index
summary_df = summary_df.reset_index()
# Rename columns
summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City'])
# Pivot
pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])
pivot_summary
151/67:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median'],
    'windspeed_10m_max': ['median'],
    'windgusts_10m_max': ['median'],
})

# Rename columns
summary_df.columns = ['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Reset index
summary_df = summary_df.reset_index()
# Rename columns
summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])
# Pivot
pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])
pivot_summary
151/68:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median'],
    'windspeed_10m_max': ['median'],
    'windgusts_10m_max': ['median'],
})

# Rename columns
summary_df.columns = ['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Reset index
summary_df = summary_df.reset_index()
# Rename columns
summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])
# Pivot
pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])
pivot_summary
151/69:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median'],
    'windspeed_10m_max': ['median'],
    'windgusts_10m_max': ['median'],
})

# Rename columns
summary_df.columns = [' Median: Average Temperature (2m)', 'Median: Max Windspeed (10m)', 'Median: Max Windgust (10m)']
# Reset index
summary_df = summary_df.reset_index()
# Rename columns
summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])
# Pivot
pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])
pivot_summary
151/70:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median'],
    'windspeed_10m_max': ['median'],
    'windgusts_10m_max': ['median'],
})

# Reset index
summary_df = summary_df.reset_index()
# Rename columns
summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])
# Pivot
pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
pivot_summary.to_latex()
151/71:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median'],
    'windspeed_10m_max': ['median'],
    'windgusts_10m_max': ['median'],
})

# Reset index
summary_df = summary_df.reset_index()
# Rename columns
summary_df.columns = ['Country', 'City', 'Season', 'Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)']
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])
# Pivot
pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
print(pivot_summary.to_latex())
153/1:
import requests
import pandas as pd

companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')
companies
153/2:
def google_search(company):
    url = 'https://www.google.com/search?q=' + company
    response = requests.get(url)
    return response

def get_results(company):
    # Extract results from Google search
    response = google_search(company)
    soup = BeautifulSoup(response.content, 'html.parser')

    search_results = soup.find('div', {'class': 'g'})
    print(search_results)
    # Extract the telephone number (if any)
    telephone_number = None
    for tag in search_results.find_all('a', href=True):
        if re.search(r'^tel:', tag['href']):
            telephone_number = tag['href'][4:]
            break

    # Extract the company social media links (if any)
    social_media_links = {}
    for tag in search_results.find_all('a', href=True):
        if re.search(r'twitter.com', tag['href']):
            social_media_links['twitter'] = tag['href']
        elif re.search(r'linkedin.com', tag['href']):
            social_media_links['linkedin'] = tag['href']
        elif re.search(r'facebook.com', tag['href']):
            social_media_links['facebook'] = tag['href']

    # Extract all links from the search result page
    all_links = []
    for tag in search_results.find_all('a', href=True):
        all_links.append(tag['href'])
    
     # Return as a dictionary
    time.sleep(random.randint(1, 3))
    return {
        'company': company,
        'telephone_number': telephone_number,
        'social_media_links': social_media_links,
        'all_links': all_links
    }
153/3:
# Get results for all companies
results = []
for idx, row in companies.iterrows():
    results.append(get_results(row['company']))
153/4:
# Get results for all companies
results = []
for idx, row in companies.iterrows():
    results.append(get_results(row['name']))
153/5:
import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
import time, random
companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')
153/6:
def google_search(company):
    url = 'https://www.google.com/search?q=' + company
    response = requests.get(url)
    return response

def get_results(company):
    # Extract results from Google search
    response = google_search(company)
    soup = BeautifulSoup(response.content, 'html.parser')

    search_results = soup.find('div', {'class': 'g'})
    print(search_results)
    # Extract the telephone number (if any)
    telephone_number = None
    for tag in search_results.find_all('a', href=True):
        if re.search(r'^tel:', tag['href']):
            telephone_number = tag['href'][4:]
            break

    # Extract the company social media links (if any)
    social_media_links = {}
    for tag in search_results.find_all('a', href=True):
        if re.search(r'twitter.com', tag['href']):
            social_media_links['twitter'] = tag['href']
        elif re.search(r'linkedin.com', tag['href']):
            social_media_links['linkedin'] = tag['href']
        elif re.search(r'facebook.com', tag['href']):
            social_media_links['facebook'] = tag['href']

    # Extract all links from the search result page
    all_links = []
    for tag in search_results.find_all('a', href=True):
        all_links.append(tag['href'])
    
     # Return as a dictionary
    time.sleep(random.randint(1, 3))
    return {
        'company': company,
        'telephone_number': telephone_number,
        'social_media_links': social_media_links,
        'all_links': all_links
    }
153/7:
# Get results for all companies
results = []
for idx, row in companies.iterrows():
    results.append(get_results(row['name']))
153/8:
def google_search(company):
    url = 'https://www.google.com/search?q=' + company
    response = requests.get(url)
    return response

def get_results(company):
    # Extract results from Google search
    response = google_search(company)
    print(response.text)
    soup = BeautifulSoup(response.content, 'html.parser')

    search_results = soup.find('div', {'class': 'g'})
    print(search_results)
    # Extract the telephone number (if any)
    telephone_number = None
    for tag in search_results.find_all('a', href=True):
        if re.search(r'^tel:', tag['href']):
            telephone_number = tag['href'][4:]
            break

    # Extract the company social media links (if any)
    social_media_links = {}
    for tag in search_results.find_all('a', href=True):
        if re.search(r'twitter.com', tag['href']):
            social_media_links['twitter'] = tag['href']
        elif re.search(r'linkedin.com', tag['href']):
            social_media_links['linkedin'] = tag['href']
        elif re.search(r'facebook.com', tag['href']):
            social_media_links['facebook'] = tag['href']

    # Extract all links from the search result page
    all_links = []
    for tag in search_results.find_all('a', href=True):
        all_links.append(tag['href'])
    
     # Return as a dictionary
    time.sleep(random.randint(1, 3))
    return {
        'company': company,
        'telephone_number': telephone_number,
        'social_media_links': social_media_links,
        'all_links': all_links
    }
153/9:
# Get results for all companies
results = []
for idx, row in companies.iterrows():
    print(row['name'])
    results.append(get_results(row['name']))
153/10:
def google_search(company):
    url = 'https://www.google.com/search?q=' + company
    cookies = {
        'CONSENT': 'YES+cb.20220415-07-p0.en+FX+135',
    }
    response = requests.get(url, cookies=cookies)    
    return response

def get_results(company):
    # Extract results from Google search
    response = google_search(company)
    print(response.text)
    soup = BeautifulSoup(response.content, 'html.parser')

    search_results = soup.find('div', {'class': 'g'})
    print(search_results)
    # Extract the telephone number (if any)
    telephone_number = None
    for tag in search_results.find_all('a', href=True):
        if re.search(r'^tel:', tag['href']):
            telephone_number = tag['href'][4:]
            break

    # Extract the company social media links (if any)
    social_media_links = {}
    for tag in search_results.find_all('a', href=True):
        if re.search(r'twitter.com', tag['href']):
            social_media_links['twitter'] = tag['href']
        elif re.search(r'linkedin.com', tag['href']):
            social_media_links['linkedin'] = tag['href']
        elif re.search(r'facebook.com', tag['href']):
            social_media_links['facebook'] = tag['href']

    # Extract all links from the search result page
    all_links = []
    for tag in search_results.find_all('a', href=True):
        all_links.append(tag['href'])
    
     # Return as a dictionary
    time.sleep(random.randint(1, 3))
    return {
        'company': company,
        'telephone_number': telephone_number,
        'social_media_links': social_media_links,
        'all_links': all_links
    }
153/11:
# Get results for all companies
results = []
for idx, row in companies.iterrows():
    print(row['name'])
    results.append(get_results(row['name']))
153/12:
import requests
import pandas as pd
from googlesearch import search
companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')
154/1:
import requests
import pandas as pd
from googlesearch import search
companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')
155/1:
import requests
import pandas as pd
from googlesearch import search
companies = pd.read_csv('C:/Users/Nicky/Downloads/data-1681569476291.csv')
155/2:
# Get results for all companies
companies
155/3:
results = search("Club Noah B.V.", num_results = 10)
results
155/4:
results = search("Club Noah B.V.", num_results = 10, advanced = True)
results
155/5:
for result in results:
    print(result)
155/6:
results = search("Club Noah B.V.", num_results = 100, advanced = True)
results
155/7:
for result in results:
    print(result)
155/8:
results = search("Puur Bouwadvies", num_results = 15, advanced = True)
results
155/9:
for result in results:
    print(result)
155/10:
results = search("Puur Bouwadvies", num_results = 15, advanced = True)
results
155/11:
for result in results:
    print(result)
155/12:
results = search("Sonnem Holding B.V.", num_results = 15, advanced = True)
results
155/13:
for result in results:
    print(result)
155/14:
results = search("Sonnem Holding", num_results = 15, advanced = True)
results
155/15:
for result in results:
    print(result)
155/16:
results = search("Val The Label", num_results = 15, advanced = True)
results
155/17:
for result in results:
    print(result)
155/18:
results = search("Kyos Energy", num_results = 15, advanced = True)
results
155/19:
for result in results:
    print(result)
152/1:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])
152/2: w_daily
152/3:
# Dump the IPYNB environment for reproducibility
!jupyter nbconvert --to notebook --inplace --execute --ExecutePreprocessor.timeout=-1 --output-dir=../output/ ../notebooks/01-eda.ipynb
152/4:
# Dump the IPYNB environment for reproducibility
import dill as pickle
pickle.dump_session('notebook_env.db')
152/5:
# Load the IPYNB environment for reproducibility
import dill as pickle
pickle.load_session('notebook_env.db')
152/6:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/7:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
152/8:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
152/9:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/10:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m']
}

START_DATE = dt.date(2000,1,1)
END_DATE = dt.date(2023,1,1)
152/11:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/12:
# Manipulate
# Add average temperature to daily, max + min / 2
w_daily['temperature_2m_avg'] = (w_daily['temperature_2m_max'] + w_daily['temperature_2m_min']) / 2
152/13:
# Energy

# Connect to remote database
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

# Create data retrieval functions
def get_hourly_entsoe_generation_production(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(biomass) AS biomass,
    AVG(fossil_brown_coal_lignite) AS fossil_brown_coal_lignite,
    AVG(fossil_coal_derived_gas) AS fossil_coal_derived_gas,
    AVG(fossil_gas) AS fossil_gas,
    AVG(fossil_hard_coal) AS fossil_hard_coal,
    AVG(fossil_oil) AS fossil_oil,
    AVG(geothermal) AS geothermal,
    AVG(hydro_pumped_storage) AS hydro_pumped_storage,
    AVG(hydro_run_of_river_and_poundage) AS hydro_run_of_river_and_poundage,
    AVG(hydro_water_reservoir) AS hydro_water_reservoir,
    AVG(nuclear) AS nuclear,
    AVG(other) AS other,
    AVG(other_renewable) AS other_renewable,
    AVG(solar) AS solar,
    AVG(waste) AS waste, 
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore
    FROM scraper.entsoe_generation_production
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

def get_hourly_entsoe_day_ahead_forecast(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(solar) AS solar,
    AVG(wind_offshore) AS wind_offshore,
    AVG(wind_onshore) AS wind_onshore,
    AVG(forecasted_load) AS forecasted_load,
    AVG(actual_aggregated) AS actual_aggregated
    FROM scraper.entsoe_da_forecast
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df


def get_hourly_entsoe_day_ahead_prices(start_date, end_date,country_list):
    query = f"""
    SELECT 
    date_trunc('hour', datetime_start_utc) AS "timestamp", 
    price_area, 
    country_code,
    AVG(price) AS price
    FROM scraper.entsoe_da_prices
    WHERE price_area IN ('{"','".join(country_list)}')
    AND "datetime_start_utc" BETWEEN '{start_date}' AND '{end_date}'
    GROUP BY "timestamp","price_area", "country_code"
    ORDER BY "timestamp"
    """
    server.start()
    connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
    df = load_query(connection, query)
    server.stop()
    return df

hourly_entsoe_day_ahead_forecast = get_hourly_entsoe_day_ahead_forecast(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_generation_production = get_hourly_entsoe_generation_production(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
hourly_entsoe_day_ahead_prices = get_hourly_entsoe_day_ahead_prices(START_DATE, END_DATE, list(TSO_COUNTRIES.keys()))
152/14:
# Dump the IPYNB environment for reproducibility
import dill as pickle
pickle.dump_session('notebook_env.db')
152/15:
lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall'))
# Resample the weather data (hourly, daily) to the desired frequency
del server
152/16:
# Dump the IPYNB environment for reproducibility
import dill as pickle
pickle.dump_session('notebook_env.db')
152/17:
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])
152/18:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])
152/19:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    plot_rolling_mean_and_variance(temps_decomposition[temps_decomposition['city'] == city])
152/20:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
    break
152/21:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(temps_decomposition['temperature_2m_avg'], label='Original')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(temps_decomposition['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
    break
df
152/22:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
for city in w_daily['city'].unique():
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
    break
df
152/23:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
cities = ['Amsterdam','Rotterdam','Berlin','London','Brussels']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/24:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
cities = ['Amsterdam','Rotterdam','Berlin','Brussels']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/25:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/26:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')

    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/27:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.5)
    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/28:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.25)
    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/29:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.25)
    plt.legend(loc='bottom right')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/30:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.25)
    plt.legend(loc='bottom right') 
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/31:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.25)
    plt.legend(loc='bottom right')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/32:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.25)
    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Rotterdam','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/33:
import matplotlib.pyplot as plt
from statsmodels.graphics.api import qqplot
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AutoRegResults

# Let the index be the date
temps_decomposition = w_daily.set_index('time')
temps_decomposition.sort_index(inplace=True)
# Plot both the rolling mean and the rolling variance (2 subplots for every country and city)

def plot_rolling_mean_and_variance(df):
    plt.figure(figsize=(12,6))
    plt.subplot(211)
    plt.plot(df['temperature_2m_avg'], label='Original')
    plt.plot(df['temperature_2m_avg'].rolling(window=365).mean(), label='Rolling mean (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling mean (1M)')

    plt.legend(loc='best')
    plt.title(f'{df["city"].unique()[0]}: Temperature with rolling mean ')
    plt.subplot(212)
    plt.plot(df['temperature_2m_avg'].rolling(window=365).var(), label = 'Rolling variance (1Y)')
    plt.plot(df['temperature_2m_avg'].rolling(window=30).mean(), label='Rolling variance (1M)')
    # Increase size between subplots
    plt.subplots_adjust(hspace=0.25)
    plt.legend(loc='best')
    plt.title(f'Rolling Variance')
    plt.show()

# Plot for all cities
print(temps_decomposition['city'].unique())
cities = ['Amsterdam','Debrecen','Berlin','Brussels', 'Paris', 'Vienna']
for city in cities:
    df = temps_decomposition[temps_decomposition['city'] == city].copy(deep=True)
    plot_rolling_mean_and_variance(df)
152/34:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['Country', 'City', 'Season'])
# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var
summary_df = summary_df.pivot_table(index=['Country', 'City', 'Season'], columns=['variable', 'statistic'], values='value').reset_index()
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/35:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var
summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable', 'statistic'], values='value').reset_index()
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/36:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var
summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])
summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))

#summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable', 'statistic'], values='value').reset_index()
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/37:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var
summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable'], values='value').reset_index()
summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])
summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))

#
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/38:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
print(summary_df.head())
# Pivot temperature_2m_avg_median to variable: temperature_2m_avg and statistic: median, var
summary_df = summary_df.pivot_table(index=['country', 'city', 'season'], columns=['variable'], values='value').reset_index()
summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])
summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))

#
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/39:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name='variable', value_name='value')
print(summary_df.head())
summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])
summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))

#
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/40:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'])
print(summary_df.head())
summary_df['statistic'] = summary_df['variable'].apply(lambda x: x.split('_')[-1])
summary_df['variable'] = summary_df['variable'].apply(lambda x: '_'.join(x.split('_')[:-1]))

#
summary_df
# Pivot
#pivot_summary = summary_df.pivot(index=['Country', 'City'], columns='Season', values=['Average Temperature (2m)', 'Max Windspeed (10m)', 'Max Windgust (10m)'])

# To latex
#print(pivot_summary.to_latex())
152/41:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# To latex
print(pivot_summary.to_latex())
152/42:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# To latex
print(summary_df.to_latex())
152/43:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
pivot_summary
# To latex
#print(pivot_summary.to_latex())
152/44:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Pivot
print(summary_df.head())
pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
#print(pivot_summary.to_latex())
152/45:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Pivot the multiindex columns to single index long format columns using variable, statistic
#summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Pivot
#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
print(summary_df.to_latex())
152/46:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
print(pivot_summary.to_latex())
152/47:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
print(pivot_summary)
152/48:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'season'])
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
pivot_summary
152/49:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['season', 'variable', 'statistic'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
pivot_summary
152/50:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'season', 'statistic'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
pivot_summary
152/51:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'season', 'statistic'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
print(pivot_summary.to_latex())
152/52:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# To latex
print(pivot_summary.to_latex())
152/53:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        pivot_summary[variable, statistic].style.format("{:.3f}")

pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/54:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")

pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/55:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', axis = 1 , subset = [variable,statistic])

pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/56:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', axis = 1 , subset = [variable,statistic])

#pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/57:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic])

pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/58:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic])

pivot_summary.style
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/59:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic])

# Plot the styled dataframe
pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/60:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', subset = [variable,statistic][0])

# Plot the styled dataframe
pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/61:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        pivot_summary.style.background_gradient(cmap='Blues', subset = [(variable,statistic)])

# Plot the styled dataframe
pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/62:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
        pivot_summary.style.background_gradient(cmap='Blues', subset = subset)

# Plot the styled dataframe
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/63:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

#print(summary_df.head())
#pivot_summary = summary_df.pivot(index=['country', 'city'], columns=['Season', 'statistic'], values=['value'])
# Style the dataframe based on each variable and statistic (e.g. the first two levels of the multiindex)
# Important to differ it for each variable
for variable in pivot_summary.columns.levels[0]:
    for statistic in pivot_summary.columns.levels[1]:
        # Apply background gradient
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.background_gradient(cmap='Blues', axis=1)
        #pivot_summary[variable,statistic] = pivot_summary[variable, statistic].style.format("{:.3f}")
        subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
        print(subset)
        pivot_summary.style.background_gradient(cmap='Blues', subset = subset)

# Plot the styled dataframe
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/64:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}

# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
for variable, cmap in gradients.items():
    idx = pd.IndexSlice
    subset = idx[:, :, idx[variable, :, :]]
    df_subset = pivot_summary.loc[:, subset]
    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)
    pivot_summary.loc[:, subset] = df_style
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/65:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}

# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
for variable, cmap in gradients.items():
    idx = pd.IndexSlice
    subset = idx[:, :, idx[variable, :, :]]
    df_subset = pivot_summary.loc[:, subset]
    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)
    pivot_summary.loc[:, subset] = df_style
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/66:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    df_subset = pivot_summary.loc[:, subset]
    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)
    pivot_summary.loc[:, subset] = df_style
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/67:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_dfs = []
for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    df_subset = pivot_summary.loc[:, subset]
    df_style = df_subset.style.background_gradient(cmap=cmap, axis=1)
    styled_dfs.append(df_style)
    #pivot_summary.loc[:, subset] = df_style
styled_df = pd.concat(styled_dfs, axis=1)

#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_df
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/68:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame

for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    pivot_summary.loc[:, subset].style.background_gradient(cmap=cmap, axis=1)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/69:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame

for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    print(pivot_summary.loc[:, subset].style.background_gradient(cmap=cmap, axis=1))
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/70:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame

for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    pivot_summary.style.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/71:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame

for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/72:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame

for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    pivot_summary = pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/73:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_df = pivot_summary.style
for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    styled_df = pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_df
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/74:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    styled_pivot_summary = styled_pivot_summary.style.background_gradient(cmap=cmap, axis=None, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/75:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/76:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    statistics = pivot_summary.columns.levels[1]
    seasons = pivot_summary.columns.levels[2]
    subset = [(variable, statistic, season) for statistic in statistics for season in seasons]
    #df_subset = pivot_summary.loc[:, subset]
    styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/77:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'Blues'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/78:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'Blues',
    'windgusts_10m_max': 'ocean'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/79:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'ocean'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/80:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# To latex



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/81:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# Styler to latex
print(styled_pivot_summary.to_latex(float_format="{{:0.3f}}".format))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/82:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format
        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): "{:0.3f}".format for season in seasons})
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# Styler to latex
print(styled_pivot_summary.to_latex(float_format="{{:0.3f}}".format))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/83:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format
        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): "{:0.3f}".format for season in seasons})
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# Styler to latex
print(styled_pivot_summary.to_latex()



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/84:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format
        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): "{:0.3f}".format for season in seasons})
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# Styler to latex
print(styled_pivot_summary.to_latex())



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/85:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format
        styled_pivot_summary = styled_pivot_summary.format({(variable, statistic, season): "{:0.3f}".format for season in seasons})
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/86:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;index"))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/87:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data"))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/88:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrule = True))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/89:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 0 if x.month in [12,1,2] else (1 if x.month in [3,4,5] else (2 if x.month in [6,7,8] else 3)))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/90:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone',
    'windgusts_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/91:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'YlOrRd',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/92:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'berlin',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/93:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'magma',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/94:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'PuRd',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/95:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'summer',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/96:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'bone',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/97:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'bone',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=-cmap, axis=1, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/98:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'bone',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/99:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))



#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/100:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below column headers
pivot_summary.style.set_table_styles([{'selector': 'th', 'props': [('border-bottom', '2px solid black')]}])
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/101:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below each multiiindex
styled_pivot_summary = styled_pivot_summary.set_properties(**{'border-bottom': '1px solid black'})
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/102:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below each multiiindex (e.g. below median, var, winter)
styled_pivot_summary = styled_pivot_summary.set_properties(**{'border-bottom': '1px solid black'})
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "all;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/103:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add lines below each multiiindex (e.g. below median, var, winter)     {'selector': 'th.level0', 'props': 'border-bottom: 1px solid black;'},
styled_pivot_summary = styled_pivot_summary.set_table_styles([{'selector': 'th.level0', 'props': 'border-bottom: 1px solid black;'}, {'selector': 'th.level1', 'props': 'border-bottom: 1px solid black;'}, {'selector': 'th.level2', 'props': 'border-bottom: 1px solid black;'}])
 
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/104:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add lines below each multiiindex (e.g. below median, var, winter)     {'selector': 'th.level', 'props': 'border-bottom: 1px solid black;'}
styled_pivot_summary = styled_pivot_summary.set_table_styles([{'selector': 'th.level', 'props': 'border-bottom: 1px solid black;'}])
 
 
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/105:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below each multiiindex

pivot_summary.style.set_table_styles([{'selector': 'th', 'props': [('border-bottom', '2px solid black')]}])
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/106:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below each multiiindex

pivot_summary.style.set_table_styles([{'selector': 'th.level0', 'props': [('border-bottom', '2px solid black')]}])
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/107:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below each multiiindex

pivot_summary.style.set_table_styles([{'selector': 'th.level1', 'props': [('border-bottom', '2px solid black')]}])
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = True, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/108:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Add clines below each multiiindex

styled_pivot_summary = pivot_summary.style.set_table_styles([{'selector': 'th.level1', 'props': [('border-bottom', '2px solid black')]}])
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/109:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/110:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Function to apply custom styles to column headers
def style_column_headers(column, level):
    if column.name[level] in column.name:
        return ['border-bottom: 1px solid black;' for _ in column]
    return ['' for _ in column]

# Apply custom styles to column headers
for level in range(pivot_summary.columns.nlevels):
    styled_pivot_summary = styled_pivot_summary.apply(style_column_headers, axis=0, level=level)


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/111:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")
# Function to apply custom styles to column headers
def style_column_headers(column, level):
    if column.name[level] in column.name:
        return ['border-bottom: 5px solid black;' for _ in column]
    return ['' for _ in column]

# Apply custom styles to column headers
for level in range(pivot_summary.columns.nlevels):
    styled_pivot_summary = styled_pivot_summary.apply(style_column_headers, axis=0, level=level)


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/112:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")

def custom_to_latex(df):
    latex_output = df.to_latex(multirow=True, escape=False, convert_css=True, clines = "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c')
    latex_lines = latex_output.splitlines()
    new_lines = []
    found_header = False

    for line in latex_lines:
        new_lines.append(line)

        # Find the header line and add the \cline commands
        if not found_header and '\\toprule' in line:
            found_header = True
            for i in range(1, df.columns.nlevels + 1):
                new_lines.append(f"\\cline{{{i + 1}-{i + df.shape[1]}}}")

    return '\n'.join(new_lines)


# Styler to latex
print(custom_to_latex(styled_pivot_summary))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/113:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")

def custom_to_latex(df):
    latex_output = df.to_latex(escape=False, convert_css=True, clines = "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c')
    latex_lines = latex_output.splitlines()
    new_lines = []
    found_header = False

    for line in latex_lines:
        new_lines.append(line)

        # Find the header line and add the \cline commands
        if not found_header and '\\toprule' in line:
            found_header = True
            for i in range(1, df.columns.nlevels + 1):
                new_lines.append(f"\\cline{{{i + 1}-{i + df.shape[1]}}}")

    return '\n'.join(new_lines)


# Styler to latex
print(custom_to_latex(styled_pivot_summary))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/114:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")

def custom_to_latex(df):
    latex_output = df.to_latex(convert_css=True, clines = "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c')
    latex_lines = latex_output.splitlines()
    new_lines = []
    found_header = False

    for line in latex_lines:
        new_lines.append(line)

        # Find the header line and add the \cline commands
        if not found_header and '\\toprule' in line:
            found_header = True
            for i in range(1, df.columns.nlevels + 1):
                new_lines.append(f"\\cline{{{i + 1}-{i + df.shape[1]}}}")

    return '\n'.join(new_lines)


# Styler to latex
print(custom_to_latex(styled_pivot_summary))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/115:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/116:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
# Define a custom formatting function
def custom_formatting(value, median, variance):
    return f"{median:.3f}\n({variance:.3f})"

# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        
        # Apply background gradient
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        
        # Apply custom formatting
        if statistic == 'median':
            for season in seasons:
                median_subset = (variable, 'median', season)
                var_subset = (variable, 'var', season)
                styled_pivot_summary = styled_pivot_summary.applymap(
                    lambda x: custom_formatting(x, pivot_summary.loc[:, median_subset], pivot_summary.loc[:, var_subset]),
                    subset=[median_subset]
                )

# Hide the variance values
styled_pivot_summary = styled_pivot_summary.hide_columns([(var[0], 'var', var[2]) for var in styled_pivot_summary.columns if var[1] == 'var'])

# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/117:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
def custom_formatting(median, variance):
    return f"{median:.3f}\n({variance:.3f})"

formatted_pivot = pivot_summary.apply(lambda x: custom_formatting(x[(x.name[0], 'median', x.name[2])], x[(x.name[0], 'var', x.name[2])]), axis=1)
formatted_pivot.columns = formatted_pivot.columns.droplevel(1)

styled_formatted_pivot = formatted_pivot.style

for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, season) for season in seasons]
        styled_formatted_pivot = styled_formatted_pivot.background_gradient(cmap=cmap, axis=None, subset=subset)

print(styled_formatted_pivot.to_latex(convert_css=True, clines="skip-last;data", hrules=False, multicol_align='c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/118:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/119:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Rename the count column
summary_df.rename(columns={'count': 'temperature_2m_avg_count'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/120:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/121:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
print(summary_df.columns)
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/122:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
summary_df.columns = ['_'.join(col).strip() for col in summary_df.columns.values]
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/123:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
summary_df.columns = ['_'.join(col).strip() for col in summary_df.columns.values]
print(summary_df.columns)
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/124:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
summary_df.columns = ['_'.join(col).strip('_') for col in summary_df.columns.values]
# Remove leading and trailing underscores
print(summary_df.columns)
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'observations'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/125:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var', 'count'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations
     
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
summary_df.columns = ['_'.join(col).strip('_') for col in summary_df.columns.values]
# Remove leading and trailing underscores
print(summary_df.columns)
# Rename the count column
summary_df.rename(columns={'temperature_2m_avg_count': 'count'}, inplace=True)

# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season', 'count'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city', 'count'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/126:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations that ends up in the first level of the index
    'time': 'count',
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
print(summary_df.columns)

# Remove leading and trailing underscores
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/127:
# Provide a summary table:
# First column index is the variable, e.g. temperature_2m
# Second column index is the season
# First row index is the country, e.g. AT
# Second row index is the city, e.g. Vienna
df = w_daily 
df['date'] = df['time'].dt.date
df['season'] = df['time'].apply(lambda x: 'Winter' if x.month in [12,1,2] else ('Spring' if x.month in [3,4,5] else ('Summer' if x.month in [6,7,8] else 'Fall')))

summary_df = df.groupby(['country', 'city', 'season']).agg({
    'temperature_2m_avg': ['median', 'var'],
    'windspeed_10m_max': ['median', 'var'],
    # Add a count of the number of observations that ends up in the first level of the index
    'time': 'count',
    #'windgusts_10m_max': ['median', 'var'],
})

# Reset index
summary_df = summary_df.reset_index()
# Last column becomes count
summary_df.columns[-1] = 'count'
print(summary_df.head(10))
# Remove leading and trailing underscores
# Melt to normal format again
summary_df = summary_df.melt(id_vars=['country', 'city', 'season'], var_name = ['variable', 'statistic'], value_name='value')
# Sort by country and city
summary_df = summary_df.sort_values(by=['country', 'city', 'variable','statistic','season'])
# Pivot
pivot_summary = summary_df.pivot(index=['country', 'city'], columns = ['variable', 'statistic', 'season'], values='value')

# Define the background gradients for each variable
gradients = {
    'temperature_2m_avg': 'coolwarm',
    'windspeed_10m_max': 'bone'
}
#subset = [(variable, statistic, season) for season in pivot_summary.columns.levels[2]]
# Loop over the gradients and apply them to the appropriate subsets of the DataFrame
styled_pivot_summary = pivot_summary.style
for variable, cmap in gradients.items():
    for statistic in pivot_summary.columns.levels[1]:
        seasons = pivot_summary.columns.levels[2]
        subset = [(variable, statistic, season) for season in seasons]
        #df_subset = pivot_summary.loc[:, subset]
        styled_pivot_summary = styled_pivot_summary.background_gradient(cmap=cmap, axis=None, subset=subset)
        # Float format 3 decimals
        
    
#pivot_summary.style.background_gradient(cmap='Blues', subset = [('temperature_2m_avg', 'median', 0),('temperature_2m_avg', 'median', 1),('temperature_2m_avg', 'median', 2),('temperature_2m_avg', 'median', 3)])
styled_pivot_summary = styled_pivot_summary.format("{:.3f}")


# Styler to latex
print(styled_pivot_summary.to_latex(convert_css=True, clines= "skip-last;data", hrules = False, multicol_align = 'c', multirow_align='c'))

#print(pivot_summary.to_latex(float_format="{{:0.3f}}".format))
152/128:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
152/129:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    [location_eindhoven],
    weather_variables=w_variables)

w_hourly
152/130:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    #'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1960,1,1)
END_DATE = dt.date(2023,1,1)
152/131:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/132:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : [],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1960,1,1)
END_DATE = dt.date(2023,1,1)
152/133:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/134:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M')
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = pd.DataFrame()
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = pd.to_datetime(hourly['time'], format='%Y-%m-%dT%H:%M')
            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = pd.DataFrame()      
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
152/135:
server = SSHTunnelForwarder(
    (REMOTE_HOST, 22),
    ssh_username=REMOTE_USER,
    ssh_password=REMOTE_PASS,
    remote_bind_address=('localhost', DB_PORT)
)

query = """
SELECT datetime_start_utc AS "time", price_area, biomass, fossil_brown_coal_lignite, fossil_coal_derived_gas, fossil_gas, fossil_hard_coal, fossil_oil, geothermal, hydro_pumped_storage, hydro_run_of_river_and_poundage, hydro_water_reservoir, nuclear, other, other_renewable, solar, waste, wind_offshore, wind_onshore
FROM scraper.entsoe_generation_production
WHERE price_area IN ('NL') AND "datetime_start_utc" BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY "time","price_area"
ORDER BY "time"
"""
server.start()
connection = load_connection(user=DB_USER, password=DB_PASS, host='localhost', database = 'raw', port = server.local_bind_port)
df = load_query(connection, query)
server.stop()

#print(df.head(10))

location_eindhoven = {'coordinates' : (51.441642, 5.469722), 'city' : 'Eindhoven', 'country' : 'NL'}
w_variables = {
    'daily' : ['temperature_2m_max', 'temperature_2m_min', 'windspeed_10m_max', 'windgusts_10m_max','shortwave_radiation_sum'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m', 'windgusts_10m', 'shortwave_radiation']
}

print(w_variables)
w_daily, w_hourly = obtain_openmeteo_historical_array(
    dt.date(2020,1,1), 
    dt.date(2022,12,31), 
    [location_eindhoven],
    weather_variables=w_variables)

w_hourly
152/136:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : [],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1960,1,1)
END_DATE = dt.date(2023,1,1)
152/137:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/138:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max','temperature_2m_min'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1960,1,1)
END_DATE = dt.date(2023,1,1)
152/139:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/140:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max','temperature_2m_min'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1960,1,1)
END_DATE = dt.date(2023,1,1)
152/141:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/142:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max','temperature_2m_min'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1945,1,1)
END_DATE = dt.date(2023,1,1)
152/143:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/144:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---
def convert_time(time_str):
    try:
        return pd.to_datetime(time_str, format='%Y-%m-%dT%H:%M')
    except ValueError:
        return pd.to_datetime(time_str.replace("-", ""), format='%Y-%m-%dT%H:%M')

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            #daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M') 
            daily['time'] = daily['time'].apply(convert_time)
            
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = hourly['time'].apply(convert_time)

            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None 
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
    Failed to retrieve weather data from OpenMeteo API for (48.148598-17.107748) between 1945-01-01-2023-01-01: time data "1945-01-01T-23:00" at position 1 doesn't match format specified
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
152/145:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---
def convert_time(time_str):
    try:
        return pd.to_datetime(time_str, format='%Y-%m-%dT%H:%M')
    except ValueError:
        return pd.to_datetime(time_str.replace("-", ""), format='%Y-%m-%dT%H:%M')

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            #daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M') 
            daily['time'] = daily['time'].apply(convert_time)
            
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = hourly['time'].apply(convert_time)

            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None 
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
    
    Failed to retrieve weather data from OpenMeteo API for (48.148598-17.107748) between 1945-01-01-2023-01-01: time data "1945-01-01T-23:00" at position 1 doesn't match format specified
    
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
152/146:
# --- Imports ---
import pandas as pd
import numpy as np
import urllib.parse
import datetime as dt
import requests
import psycopg2
import sys

from secret import *
from sshtunnel import SSHTunnelForwarder

# --- Constants ---
# Weather Variables in parsed format (See https://open-meteo.com/en/docs#api-documentation)
hourly_weather_variables = {
    'temperature' : ['temperature_2m', 'dewpoint_2m', 'apparent_temperature'],
    'humidity' : ['relativehumidity_2m'],
    'pressure' : ['pressure_msl', 'surface_pressure'],
    'clouds' : ['cloudcover', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high'],
    'wind' : ['windspeed_10m', 'windspeed_80m', 'windspeed_120m', 'windspeed_180m', 'winddirection_10m', 'winddirection_80m', 'winddirection_120m', 'winddirection_180m', 'windgusts_10m'],
    'radiation' : ['shortwave_radiation', 'direct_radiation', 'direct_normal_irradiance', 'diffuse_radiation'],
    'energy' : ['vapor_pressure_deficit', 'cape', 'evapotranspiration', 'et0_fao_evapotranspiration'],
    'precipitation' : ['precipitation', 'snowfall', 'precipitation_probability', 'rain', 'showers'],
    'weather' : ['weathercode'],
    'snow' : ['snow_depth', 'freezinglevel_height'],
    'visibility' : ['visibility'],
    'soil' : ['soil_temperature_0cm', 'soil_temperature_6cm', 'soil_temperature_18cm', 'soil_temperature_54cm', 'soil_moisture_0_1cm', 'soil_moisture_1_3cm', 'soil_moisture_3_9cm', 'soil_moisture_9_27cm', 'soil_moisture_27_81cm']
}
daily_weather_variables = {
    'temperature' : ['temperature_2m_max', 'temperature_2m_min', 'apparent_temperature_max', 'apparent_temperature_min'],
    'precipitation' : ['precipitation_sum', 'rain_sum', 'showers_sum', 'snowfall_sum', 'precipitation_hours', 'precipitation_probability_max', 'precipitation_probability_min', 'precipitation_probability_mean'],
    'weather' : ['weathercode', 'sunrise', 'sunset'],
    'wind' : ['windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant'],
    'radiation' : ['shortwave_radiation_sum'],
    'energy' : ['et0_fao_evapotranspiration'],
    'uv' : ['uv_index_max', 'uv_index_clear_sky_max']
}
# --- Weather Retrieval Functions ---
def convert_time(time_str):
    try:
        return pd.to_datetime(time_str, format='%Y-%m-%dT%H:%M')
    except ValueError:
        return pd.to_datetime(time_str.replace("-", ""), format='%Y-%m-%dT%H:%M')

# Base OpenMeteo API call
def obtain_openmeteo_historical(start : dt.date, end : dt.date, lat : float, lon : float, weather_variables : dict) -> pd.DataFrame:
    url = "https://archive-api.open-meteo.com/v1/archive"
    hourly_params = weather_variables.get('hourly', [])
    daily_params = weather_variables.get('daily', [])
    params = {'latitude' : lat, 
              'longitude' : lat, 
              'start_date' : start.strftime('%Y-%m-%d'), 
              'end_date' : end.strftime('%Y-%m-%d'), 
              'hourly' : ','.join(hourly_params),
              'daily' : ','.join(daily_params),
              'timezone' : 'UTC'
              }
    try:
        response = requests.get(url, params=urllib.parse.urlencode(params, safe = ','))
        data = response.json()
        # Daily Data
        if 'daily' in data:
            daily = pd.DataFrame(data['daily'])
            #daily['time'] = pd.to_datetime(daily['time'], format='%Y-%m-%dT%H:%M') 
            daily['time'] = daily['time'].apply(convert_time)
            
            daily['latitude'] = lat
            daily['longitude'] = lon
        else:
            daily = None
        # Hourly Data
        if 'hourly' in data:
            hourly = pd.DataFrame(data['hourly'])
            hourly['time'] = hourly['time'].apply(convert_time)

            hourly['latitude'] = lat
            hourly['longitude'] = lon
        else:
            hourly = None 
        
        return daily, hourly
    except Exception as e:
        print(f"Failed to retrieve weather data from OpenMeteo API for ({lat}-{lon}) between {start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}: {e}")
        
# Request an array of locations using OpenMeteo API
def obtain_openmeteo_historical_array(start : dt.date, end : dt.date, locations_array : list, weather_variables : dict) -> pd.DataFrame:
    daily, hourly = [], []
    for location in locations_array:
        lat, lon = location['coordinates']
        # Get rest of location data (e.g. name, elevation, etc.)
        location_metadata = {k:v for k,v in location.items() if k != 'coordinates'}
        daily_data, hourly_data = obtain_openmeteo_historical(start, end, lat, lon, weather_variables)
        if daily_data is not None:
            daily_data = daily_data.assign(**location_metadata)
            daily.append(daily_data)
        if hourly_data is not None:
            hourly_data = hourly_data.assign(**location_metadata)
            hourly.append(hourly_data)
    return pd.concat(daily), pd.concat(hourly)

# --- Database Retrieval Functions (SSH) ---
def load_connection(user: str,password: str,host: str,database: str, port):
    pg_connection_dict = {
    'dbname': database,
    'user': user,
    'password': password,
    'port': port,
    'host': host
    }
    keepalive_kwargs = {"keepalives": 1,"keepalives_idle": 250,"keepalives_interval": 5,"keepalives_count": 5}
    try:
        connection = psycopg2.connect(**pg_connection_dict, **keepalive_kwargs)
        return connection
    except(Exception, EnvironmentError) as e:
        raise Exception ("Error in connection, }".format(e))
        
def load_query(connection, query) -> pd.DataFrame:
    try:
        cursor = connection.cursor()
        cursor.execute(query) 
        record = cursor.fetchall()
        # Convert to pandas
        df_record = pd.DataFrame.from_records(record, columns =[x[0] for x in cursor.description])
        return df_record

    except(Exception,EnvironmentError) as e:
        raise Exception ("Error while fetching data from postgres, }".format(e))
    finally:
        # Close connection
        if(connection):
            cursor.close()
152/147:
TSO_COUNTRIES = {
"SK" : "Slovakia",
"DK" : "Denmark",
"LT" : "Lithuania",
"SI" : "Slovenia",
"CZ" : "Czech Republic",
"FR" : "France",
"LV" : "Latvia",
"RO" : "Romania",
"NL" : "Netherlands",
"EE" : "Estonia",
"HU" : "Hungary",
"AT" : "Austria",
"FI" : "Finland",
"PL" : "Poland",
"ES" : "Spain",
"PT" : "Portugal",
"NO" : "Norway",
"BG" : "Bulgaria",
"IT" : "Italy",
"HR" : "Croatia",
"SE" : "Sweden",
"GR" : "Greece",
"CH" : "Switzerland",
"BE" : "Belgium",
"DE" : "Germany"
}

# Regions are North Western Europe, mediterranean, eastern, western, baltic, central, southern, scandinavia
LOCATIONS = [
    {"city" : "Bratislava", "coordinates": (48.148598, 17.107748), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Koice", "coordinates": (48.720595, 21.257698), "country": "SK", "region": "Eastern Europe"},
    {"city" : "Copenhagen", "coordinates": (55.676097, 12.568337), "country": "DK", "region": "North Western Europe"},
    {"city" : "Aarhus", "coordinates": (56.162939, 10.203921), "country": "DK", "region": "North Western Europe"},
    {"city" : "Vilnius", "coordinates": (54.687157, 25.279652), "country": "LT", "region": "Baltic"},
    {"city" : "Kaunas", "coordinates": (54.898521, 23.903597), "country": "LT", "region": "Baltic"},
    {"city" : "Ljubljana", "coordinates": (46.056946, 14.505751), "country": "SI", "region": "Central Europe"},
    {"city" : "Maribor", "coordinates": (46.554650, 15.646049), "country": "SI", "region": "Central Europe"},
    {"city" : "Prague", "coordinates": (50.075539, 14.437800), "country": "CZ", "region": "Central Europe"},
    {"city" : "Brno", "coordinates": (49.195060, 16.606837), "country": "CZ", "region": "Central Europe"},
    {"city" : "Paris", "coordinates": (48.856614, 2.352222), "country": "FR", "region": "Mediterranean"},
    {"city" : "Marseille", "coordinates": (43.296482, 5.369780), "country": "FR", "region": "Mediterranean"},
    {"city" : "Riga", "coordinates": (56.949649, 24.105186), "country": "LV", "region": "Baltic"},
    {"city" : "Daugavpils", "coordinates": (55.874296, 26.536963), "country": "LV", "region": "Baltic"},
    {"city" : "Bucharest", "coordinates": (44.426765, 26.102537), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Cluj-Napoca", "coordinates": (46.771210, 23.623635), "country": "RO", "region": "Eastern Europe"},
    {"city" : "Amsterdam", "coordinates": (52.370216, 4.895168), "country": "NL", "region": "North Western Europe"},
    {"city" : "Rotterdam", "coordinates": (51.920179, 4.481774), "country": "NL", "region": "North Western Europe"},
    {"city" : "Tallinn", "coordinates": (59.436962, 24.753574), "country": "EE", "region": "Baltic"},
    {"city" : "Tartu", "coordinates": (58.380624, 26.725056), "country": "EE", "region": "Baltic"},
    {"city" : "Budapest", "coordinates": (47.497913, 19.040236), "country": "HU", "region": "Central Europe"},
    {"city" : "Debrecen", "coordinates": (47.531604, 21.627312), "country": "HU", "region": "Central Europe"},
    {"city" : "Vienna", "coordinates": (48.208176, 16.373819), "country": "AT", "region": "Central Europe"},
    {"city" : "Graz", "coordinates": (47.070714, 15.439504), "country": "AT", "region": "Central Europe"},
    {"city" : "Helsinki", "coordinates": (60.169856, 24.938379), "country": "FI", "region": "Scandinavia"},
    {"city" : "Turku", "coordinates": (60.451810, 22.266630), "country": "FI", "region": "Scandinavia"},
    {"city" : "Warsaw", "coordinates": (52.229676, 21.012229), "country": "PL", "region": "Central Europe"},
    {"city" : "Lodz", "coordinates": (51.759250, 19.455983), "country": "PL", "region": "Central Europe"},
    {"city" : "Madrid", "coordinates": (40.416775, -3.703790), "country": "ES", "region": "Mediterranean"},
    {"city" : "Barcelona", "coordinates": (41.385064, 2.173403), "country": "ES", "region": "Mediterranean"},
    {"city" : "Lisbon", "coordinates": (38.722252, -9.139337), "country": "PT", "region": "Mediterranean"},
    {"city" : "Porto", "coordinates": (41.157944, -8.629105), "country": "PT", "region": "Mediterranean"},
    {"city" : "Oslo", "coordinates": (59.913869, 10.752245), "country": "NO", "region": "Scandinavia"},
    {"city" : "Bergen", "coordinates": (60.392050, 5.322050), "country": "NO", "region": "Scandinavia"},
    {"city" : "Sofia", "coordinates": (42.697708, 23.321868), "country": "BG", "region": "Balkans"},
    {"city" : "Plovdiv", "coordinates": (42.135407, 24.745290), "country": "BG", "region": "Balkans"},
    {"city" : "Rome", "coordinates": (41.902782, 12.496366), "country": "IT", "region": "Mediterranean"},
    {"city" : "Milan", "coordinates": (45.464204, 9.189982), "country": "IT", "region": "Mediterranean"},
    {"city" : "Zagreb", "coordinates": (45.815011, 15.981919), "country": "HR", "region": "Balkans"},
    {"city" : "Split", "coordinates": (43.508132, 16.440193), "country": "HR", "region": "Balkans"},
    {"city" : "Stockholm", "coordinates": (59.329323, 18.068581), "country": "SE", "region": "Scandinavia"},
    {"city" : "Gothenburg", "coordinates": (57.708870, 11.974560), "country": "SE", "region": "Scandinavia"},
    {"city" : "Athens", "coordinates": (37.983810, 23.727539), "country": "GR", "region": "Balkans"},
    {"city" : "Thessaloniki", "coordinates": (40.640060, 22.944420), "country": "GR", "region": "Balkans"},
    {"city" : "Zrich", "coordinates": (47.376887, 8.541694), "country": "CH", "region": "Central Europe"},
    {"city" : "Geneva", "coordinates": (46.204391, 6.143158), "country": "CH", "region": "Central Europe"},
    {"city" : "Brussels", "coordinates": (50.850340, 4.351710), "country": "BE", "region": "North Western Europe"},
    {"city" : "Antwerp", "coordinates": (51.219448, 4.402464), "country": "BE", "region": "North Western Europe"},
    {"city" : "Berlin", "coordinates": (52.520007, 13.404954), "country": "DE", "region": "North Western Europe"},
    {"city" : "Hamburg", "coordinates": (53.551086, 9.993682), "country": "DE", "region": "North Western Europe"},
]

W_VARIABLES = {
    'daily' : ['temperature_2m_max','temperature_2m_min'],
    'hourly' : ['temperature_2m', 'windspeed_10m','windspeed_100m']
}

START_DATE = dt.date(1950,1,1)
END_DATE = dt.date(2023,1,1)
152/148:
# Weather
w_daily, w_hourly = obtain_openmeteo_historical_array(
    START_DATE, 
    END_DATE, 
    LOCATIONS,
    weather_variables=W_VARIABLES)

w_daily
152/149:
# Dump the IPYNB environment for reproducibility
import dill as pickle
pickle.dump_session('notebook_env.db')
152/150: del server
152/151: del server
152/152:
# Dump the IPYNB environment for reproducibility
import dill as pickle
pickle.dump_session('notebook_env.db')
152/153:
del connection
del server
152/154:
import seaborn as sns

df = w_hourly.copy()
# groupby region, city, and country and resample to weekly
df = df.groupby(['region', 'city', 'country']).resample('M', on='time').mean().reset_index()
df['label'] = df['city'] + ' (' + df['country'] + ')'

fig, axes = plt.subplots(7, 1, figsize=(10, 20), sharex=False)
for i, region in enumerate(df['region'].unique()):
    sns.lineplot(data=df[df['region'] == region], x='time', y='temperature_2m', hue='label', ax=axes[i])
    # Smaller line width
    for j in range(0, len(axes[i].lines)):
        axes[i].lines[j].set_linewidth(1.5)
        axes[i].lines[j].set_alpha(0.5)
    axes[i].set_title(region)
    # Set axis labels to '' to avoid overlapping
    axes[i].set_ylabel('Temperature (C)')
    axes[i].set_xlabel('')
    # Rotate xticks
    axes[i].tick_params(axis='x', rotation=45)
    # Legends outside the plot
    axes[i].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., ncol = 2)
    # Create more space between the plots
fig.subplots_adjust(hspace=0.5)
plt.show()
   1: _ih[-15:]
   2: %history -g
